{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chương này bao gồm những kiến thức chính:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Đếm từ và tần số xuất hiện để phân tích ngữ nghĩa\n",
    "* Dự đoán xác suất xuất hiện từ với Zip'f Law\n",
    "* Biểu diễn vecto từ và làm thế nào để bắt đầu sử dụng chúng\n",
    "* Tìm document liên quan từ một corpus sử dụng tần số document nghịch đảo\n",
    "* Ước tính sự giống nhau của document với cosine và Okapi BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong chương này, chúng ta sẽ xem xét 3 cách mạnh mẽ để biểu diễn từ và tầm quan trọng của chúng trong 1 document:\n",
    "* <i>Bags of words</i>: Vector hóa số từ hoặc tần suất\n",
    "* <i>Bags of n-grams</i>: Đếm số cặp từ (bigram), triplets (trigrams), ...\n",
    "* <i>TF-IDF vectors</i>: Chấm điểm từ, đại diện cho mức độ quan trọng của chúng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUAN TRỌNG: TF-IDF là term frequenct times inverse document frequency.<br/>\n",
    "Team frequencies là đếm mỗi từ trong document, cái mà bạn đã học ở chương trước. Inverse document frequency nghĩa là bạn sẽ chia mỗi số từ đó cho số từ trong tài liệu.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mỗi một kỹ thuật có thể được áp dụng riêng rẽ hoặc như một phần của pipeline NLP. Đây là mô hình thống kê dựa trên frequency (tần số) xuất hiện. Phần sau của quyển sách này, bạn sẽ có cách khác để tìm hiểu sâu hơn về mối quan hệ giữa các từ và mô hình giữa chúng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhưng máy học NLP \"shallow\" (nông cạn) là mạnh mẽ và hữu ích cho nhiều ứng dụng thực tiễn như lọc spam và sentimen analysis (phân tích cảm xúc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở chương trước, bạn đã tạo một mô hình không gian vecto đầu tiên của một văn bản. Bạn sử dụng one-hot encoding cho mỗi từ và rồi tổ hợp tất cả các vector với một phép toán OR (hoặc sum) để tạo một vector biểu diễn cho 1 text. Và vecto bag-of-words nhị phân này cần một lượng index lớn để biểu diễn document khi load vào trong 1 cấu trúc như là Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ về đếm số lần xuất hiện của từ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"\"\"The faster Harry got to the store, the faster Harry,\n",
    "... the faster, would get home.\"\"\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " '...',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với list đơn của bạn, bạn muốn lấy unique words từ document và đếm chúng. Dictionary trong python phục vụ tốt cho mục đích này, và bởi vì bạn muốn đếm số từ, bạn có thể sử dụng <b>Counter</b>, như bạn đã làm ở chương trước:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         '...': 1,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bag_of_words = Counter(tokens)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như dictionary Python, thứ tự các key bị xáo trộn. Sự sắp xếp mới này tối ưu cho lưu trữ, cập nhật và phục hồi, không thích hợp hiển thị. Thông tin về thứ tự ban đầu của các từ bị loại bỏ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Một <b>collections.Counter</b> là một đối tượng collection không thứ tự. Hơn nữa, gọi là một bag hoặc multiset. Phụ thuộc vào nền tảng của bạn và phiên bản Python, bạn có thể thấy rằng một <b>Counter</b> được hiển thị trong một thứ tự hợp lý, giống như thứ tự từ vựng hoặc thứ tự xuất hiện trong tokens của bạn. Nhưng cũng giống như dict chuẩn của Python, bạn không thể tin vào sắp xếp của tokens (keys) trong một <b>Counter</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đối với document ngắn như thế này, bag of words không thứ tự cũng bao gồm rất nhiều thông tin quan trọng về mục đích và ngữ nghĩa. Và thông tin trong bag of word là đủ để thực hiện 1 số thứ hữu ích như detect sapm, phân tích cảm xúc. Nó có thể như 1 cái túi, nhưng nó chứa đầy ý nghĩa và thông tin. Vì vậy, sắp xếp những từ này theo một trật tự dễ dàng hơn để hiểu. Đối tượng <b>Counter</b> có một phương thức thực hiện, <b>most_common</b> cho mục đích này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc định, most_common() liệt kê tất cả các tokens phổ biến nhất, nhưng bạn có thể giới hạn số lượng list ra bằng cách thêm số vào như số 4 bên trên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đặc biệt, số lần một từ xuất hiện trong 1 document được gọi là <b>tearm frequency</b> (tần số), viết tắt là TF. <br/>\n",
    "Vì vậy 4 term hoặc token đầu của bạn là  “the,” “,”, “harry,” and “faster.”. Tuy nhiên từ \"the\" và ký hiệu \",\" là không quan trọng trong ý nghĩa của document. Và những token uninfomative (không thông tin) xuất hiện nhiều. Vì vậy, trong ví dụ này, chúng ta sẽ loại bỏ chúng, cùng với một danh sách stop word và dấu chấm câu. Điều này không luôn luôn loại ở được mọi trường hợp, nhưng cho đến thời điểm này thì nó là ví dụ đơn giản. Bạn thấy rằng các từ \"harry\" và \"faster\" là các token hàng đầu ở trong vector TF của bạn (bag of word.<br/>\n",
    "Nào, hãy tính toán term frequency của \"harry\" từ đối tượng <b>Counter</b> bạn định nghĩa ở trên:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "12\n",
      "0.1667\n"
     ]
    }
   ],
   "source": [
    "times_harry_appears=bag_of_words[\"harry\"] # số lần xuất hiện của từ harry\n",
    "print(times_harry_appears)\n",
    "num_unique_words = len(bag_of_words) # số unique token nguồn\n",
    "print(num_unique_words)\n",
    "tf = times_harry_appears/num_unique_words\n",
    "print(round(tf,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy dừng lại vài giây và tìm hiểu sâu hơn một chút về team frequency, một từ mà chúng ta sử dụng thường xuyên ở cuốn sách này.<br/>\n",
    "Khi nói bạn đã tìm được từ \"dog\" 3 lần ở document A và 100 lần ở document B. Rõ ràng \"dog\" là quan trọng hơn trong document B. Nhưng khoan. Hãy nói bạn tìm ở document A là một email 30 từ và document B là tiểu thuyết Chiến tranh và hòa bình (War and Peace) (xấp xỉ 580,000 từ). Phân tích như sau:<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$TF(\"dog\",document_A)=3/30=.1$$\n",
    "$$TF(\"dog\",document_B)=100/580000=0.0017$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ bạn có thể thấy rằng các phân tích mô tả một cái gì đó về 2 văn bản và mối quan hệ giữa chúng với từ \"dog\". Vì vậy, thay vì đếm số từ thô để mô tả document của bạn trong một corpus, bạn có thể chuẩn hóa với term frequencties. Cũng như thế, bạn có thể tính toán mỗi từ và lấy được độ quan trọng tương đối của từ đó với document.Bạn đã thực hiện một số tiến bộ lớn trong việc số hóa văn bản, vượt ra ngoài chỉ sự hiện diện hay vắng mặt của một từ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loại bỏ stop word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " '...',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         '...': 1,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_counts = Counter(tokens)\n",
    "kite_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn đã có thể chuyển đổi văn bản của bạn thành số ở mức cơ bản. Nhưng bạn vẫn chỉ lưu trữ chúng ở dictionary, tiếp theo chúng ta sẽ tạo một vector của số từ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'store',\n",
       " ',',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " '...',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key, value in kite_counts.most_common():\n",
    "    document_vector.append(value/doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2,\n",
       " 0.2,\n",
       " 0.13333333333333333,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có thể tăng tốc xử lý bằng numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colections của các từ (bộ sưu tập các từ) trong vocabulary được gọi là <i>lexicon</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The faster Harry got to the store, the faster and faster Harry would get home.',\n",
       " 'Harry is hairy and faster than Jill.',\n",
       " 'Jill is not as hairy as Harry.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[',',\n",
       "  '.',\n",
       "  'and',\n",
       "  'faster',\n",
       "  'faster',\n",
       "  'faster',\n",
       "  'get',\n",
       "  'got',\n",
       "  'harry',\n",
       "  'harry',\n",
       "  'home',\n",
       "  'store',\n",
       "  'the',\n",
       "  'the',\n",
       "  'the',\n",
       "  'to',\n",
       "  'would'],\n",
       " ['.', 'and', 'faster', 'hairy', 'harry', 'is', 'jill', 'than'],\n",
       " ['.', 'as', 'as', 'hairy', 'harry', 'is', 'jill', 'not']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_tokens = sum(doc_tokens, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'faster',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'harry',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'store',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would',\n",
       " '.',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'than',\n",
       " '.',\n",
       " 'as',\n",
       " 'as',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
