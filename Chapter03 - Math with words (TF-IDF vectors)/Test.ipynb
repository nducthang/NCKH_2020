{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chương này bao gồm những kiến thức chính:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Đếm từ và tần số xuất hiện để phân tích ngữ nghĩa\n",
    "* Dự đoán xác suất xuất hiện từ với Zip'f Law\n",
    "* Biểu diễn vecto từ và làm thế nào để bắt đầu sử dụng chúng\n",
    "* Tìm document liên quan từ một corpus sử dụng tần số document nghịch đảo\n",
    "* Ước tính sự giống nhau của document với cosine và Okapi BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong chương này, chúng ta sẽ xem xét 3 cách mạnh mẽ để biểu diễn từ và tầm quan trọng của chúng trong 1 document:\n",
    "* <i>Bags of words</i>: Vector hóa số từ hoặc tần suất\n",
    "* <i>Bags of n-grams</i>: Đếm số cặp từ (bigram), triplets (trigrams), ...\n",
    "* <i>TF-IDF vectors</i>: Chấm điểm từ, đại diện cho mức độ quan trọng của chúng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUAN TRỌNG: TF-IDF là term frequenct times inverse document frequency.<br/>\n",
    "Team frequencies là đếm mỗi từ trong document, cái mà bạn đã học ở chương trước. Inverse document frequency nghĩa là bạn sẽ chia mỗi số từ đó cho số từ trong tài liệu.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mỗi một kỹ thuật có thể được áp dụng riêng rẽ hoặc như một phần của pipeline NLP. Đây là mô hình thống kê dựa trên frequency (tần số) xuất hiện. Phần sau của quyển sách này, bạn sẽ có cách khác để tìm hiểu sâu hơn về mối quan hệ giữa các từ và mô hình giữa chúng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhưng máy học NLP \"shallow\" (nông cạn) là mạnh mẽ và hữu ích cho nhiều ứng dụng thực tiễn như lọc spam và sentimen analysis (phân tích cảm xúc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở chương trước, bạn đã tạo một mô hình không gian vecto đầu tiên của một văn bản. Bạn sử dụng one-hot encoding cho mỗi từ và rồi tổ hợp tất cả các vector với một phép toán OR (hoặc sum) để tạo một vector biểu diễn cho 1 text. Và vecto bag-of-words nhị phân này cần một lượng index lớn để biểu diễn document khi load vào trong 1 cấu trúc như là Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ về đếm số lần xuất hiện của từ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"\"\"The faster Harry got to the store, the faster Harry,\n",
    "... the faster, would get home.\"\"\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " '...',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với list đơn của bạn, bạn muốn lấy unique words từ document và đếm chúng. Dictionary trong python phục vụ tốt cho mục đích này, và bởi vì bạn muốn đếm số từ, bạn có thể sử dụng <b>Counter</b>, như bạn đã làm ở chương trước:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         '...': 1,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bag_of_words = Counter(tokens)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như dictionary Python, thứ tự các key bị xáo trộn. Sự sắp xếp mới này tối ưu cho lưu trữ, cập nhật và phục hồi, không thích hợp hiển thị. Thông tin về thứ tự ban đầu của các từ bị loại bỏ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Một <b>collections.Counter</b> là một đối tượng collection không thứ tự. Hơn nữa, gọi là một bag hoặc multiset. Phụ thuộc vào nền tảng của bạn và phiên bản Python, bạn có thể thấy rằng một <b>Counter</b> được hiển thị trong một thứ tự hợp lý, giống như thứ tự từ vựng hoặc thứ tự xuất hiện trong tokens của bạn. Nhưng cũng giống như dict chuẩn của Python, bạn không thể tin vào sắp xếp của tokens (keys) trong một <b>Counter</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đối với document ngắn như thế này, bag of words không thứ tự cũng bao gồm rất nhiều thông tin quan trọng về mục đích và ngữ nghĩa. Và thông tin trong bag of word là đủ để thực hiện 1 số thứ hữu ích như detect sapm, phân tích cảm xúc. Nó có thể như 1 cái túi, nhưng nó chứa đầy ý nghĩa và thông tin. Vì vậy, sắp xếp những từ này theo một trật tự dễ dàng hơn để hiểu. Đối tượng <b>Counter</b> có một phương thức thực hiện, <b>most_common</b> cho mục đích này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc định, most_common() liệt kê tất cả các tokens phổ biến nhất, nhưng bạn có thể giới hạn số lượng list ra bằng cách thêm số vào như số 4 bên trên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đặc biệt, số lần một từ xuất hiện trong 1 document được gọi là <b>tearm frequency</b> (tần số), viết tắt là TF. <br/>\n",
    "Vì vậy 4 term hoặc token đầu của bạn là  “the,” “,”, “harry,” and “faster.”. Tuy nhiên từ \"the\" và ký hiệu \",\" là không quan trọng trong ý nghĩa của document. Và những token uninfomative (không thông tin) xuất hiện nhiều. Vì vậy, trong ví dụ này, chúng ta sẽ loại bỏ chúng, cùng với một danh sách stop word và dấu chấm câu. Điều này không luôn luôn loại ở được mọi trường hợp, nhưng cho đến thời điểm này thì nó là ví dụ đơn giản. Bạn thấy rằng các từ \"harry\" và \"faster\" là các token hàng đầu ở trong vector TF của bạn (bag of word.<br/>\n",
    "Nào, hãy tính toán term frequency của \"harry\" từ đối tượng <b>Counter</b> bạn định nghĩa ở trên:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "12\n",
      "0.1667\n"
     ]
    }
   ],
   "source": [
    "times_harry_appears=bag_of_words[\"harry\"] # số lần xuất hiện của từ harry\n",
    "print(times_harry_appears)\n",
    "num_unique_words = len(bag_of_words) # số unique token nguồn\n",
    "print(num_unique_words)\n",
    "tf = times_harry_appears/num_unique_words\n",
    "print(round(tf,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy dừng lại vài giây và tìm hiểu sâu hơn một chút về team frequency, một từ mà chúng ta sử dụng thường xuyên ở cuốn sách này.<br/>\n",
    "Khi nói bạn đã tìm được từ \"dog\" 3 lần ở document A và 100 lần ở document B. Rõ ràng \"dog\" là quan trọng hơn trong document B. Nhưng khoan. Hãy nói bạn tìm ở document A là một email 30 từ và document B là tiểu thuyết Chiến tranh và hòa bình (War and Peace) (xấp xỉ 580,000 từ). Phân tích như sau:<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$TF(\"dog\",document_A)=3/30=.1$$\n",
    "$$TF(\"dog\",document_B)=100/580000=0.0017$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ bạn có thể thấy rằng các phân tích mô tả một cái gì đó về 2 văn bản và mối quan hệ giữa chúng với từ \"dog\". Vì vậy, thay vì đếm số từ thô để mô tả document của bạn trong một corpus, bạn có thể chuẩn hóa với term frequencties. Cũng như thế, bạn có thể tính toán mỗi từ và lấy được độ quan trọng tương đối của từ đó với document.Bạn đã thực hiện một số tiến bộ lớn trong việc số hóa văn bản, vượt ra ngoài chỉ sự hiện diện hay vắng mặt của một từ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loại bỏ stop word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " '...',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         '...': 1,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_counts = Counter(tokens)\n",
    "kite_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn đã có thể chuyển đổi văn bản của bạn thành số ở mức cơ bản. Nhưng bạn vẫn chỉ lưu trữ chúng ở dictionary, tiếp theo chúng ta sẽ tạo một vector của số từ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'store',\n",
       " ',',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " '...',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key, value in kite_counts.most_common():\n",
    "    document_vector.append(value/doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2,\n",
       " 0.2,\n",
       " 0.13333333333333333,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có thể tăng tốc xử lý bằng numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colections của các từ (bộ sưu tập các từ) trong vocabulary được gọi là <i>lexicon</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The faster Harry got to the store, the faster and faster Harry would get home.',\n",
       " 'Harry is hairy and faster than Jill.',\n",
       " 'Jill is not as hairy as Harry.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[',',\n",
       "  '.',\n",
       "  'and',\n",
       "  'faster',\n",
       "  'faster',\n",
       "  'faster',\n",
       "  'get',\n",
       "  'got',\n",
       "  'harry',\n",
       "  'harry',\n",
       "  'home',\n",
       "  'store',\n",
       "  'the',\n",
       "  'the',\n",
       "  'the',\n",
       "  'to',\n",
       "  'would'],\n",
       " ['.', 'and', 'faster', 'hairy', 'harry', 'is', 'jill', 'than'],\n",
       " ['.', 'as', 'as', 'hairy', 'harry', 'is', 'jill', 'not']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_tokens = sum(doc_tokens, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'faster',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'harry',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'store',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would',\n",
       " '.',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'than',\n",
       " '.',\n",
       " 'as',\n",
       " 'as',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'faster',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'harry',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'store',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would',\n",
       " '.',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'than',\n",
       " '.',\n",
       " 'as',\n",
       " 'as',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mỗi vector trong 3 document của bạn sẽ cần có 18 giá trị."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta sẽ copy giá trị của các vecto cơ sở, cập nhật giá trị của mỗi vecto của document, và lưu chúng trong 1 mảng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.05555555555555555),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.16666666666666666),\n",
       "              ('get', 0.05555555555555555),\n",
       "              ('got', 0.05555555555555555),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.1111111111111111),\n",
       "              ('home', 0.05555555555555555),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.05555555555555555),\n",
       "              ('than', 0),\n",
       "              ('the', 0.16666666666666666),\n",
       "              ('to', 0.05555555555555555),\n",
       "              ('would', 0.05555555555555555)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.05555555555555555),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.05555555555555555),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0.05555555555555555),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value/len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Không gian vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def consine_sim(vec1, vec2):\n",
    "    \"\"\" Let's convert our dictionaries to lists for easier matching \"\"\"\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "    \n",
    "    dot_prod = 0\n",
    "    for i,v in enumerate(vec1):\n",
    "        dot_prod += v*vec2[i]\n",
    "    \n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    \n",
    "    return dot_prod/(mag_1*mag_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nói về consine semilarity giữa 2 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipf’s law states that given some corpus of natural language utterances, the\n",
    "frequency of any word is inversely proportional to its rank in the frequency table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with cities and social networks, so with words. Let’s first download the Brown Corpus from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Thang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "brown.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chúng ta có hơn 1 triệu tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " ('of', 36412),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('that', 10594),\n",
       " ('is', 10109),\n",
       " ('was', 9815),\n",
       " ('he', 9548),\n",
       " ('for', 9489),\n",
       " ('it', 8760),\n",
       " ('with', 7289),\n",
       " ('as', 7253),\n",
       " ('his', 6996),\n",
       " ('on', 6741),\n",
       " ('be', 6377),\n",
       " ('at', 5372),\n",
       " ('by', 5306),\n",
       " ('i', 5164)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "puncs = set((',', '.', '--', '-', '!', '?',\\\n",
    "             ':', ';', '``', \"''\", '(', ')', '[', ']'))\n",
    "word_list = (x.lower() for x in brown.words() if x not in puncs)\n",
    "token_counts = Counter(word_list)\n",
    "token_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick glance shows that the word frequencies in the Brown corpus follow the logarithmic relationship Zipf predicted. “The” (rank 1 in term frequency) occurs roughly\n",
    "twice as often as “of” (rank 2 in term frequency), and roughly three times as often as\n",
    "“and” (rank 3 in term frequency). If you don’t believe us, use the example code (https://github.com/totalgood/nlpia/blob/master/src/nlpia/book/examples/ch03\n",
    "_zipf.py) in the nlpia package to see this yourself.\n",
    " In short, if you rank the words of a corpus by the number of occurrences and list\n",
    "them in descending order, you’ll find that, for a sufficiently large sample, the first\n",
    "word in that ranked list is twice as likely to occur in the corpus as the second word in\n",
    "the list. And it is four times as likely to appear as the fourth word in the list. So given a\n",
    "large corpus, you can use this breakdown to say statistically how likely a given word is\n",
    "to appear in any given document of that corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlpia'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-e65df77c265b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnlpia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloaders\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkite_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkite_history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nlpia'"
     ]
    }
   ],
   "source": [
    "from nlpia.data.loaders import kite_text, kite_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return of Zipf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relavance ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ có rất nhiều packet hỗ trợ chúng ta tự động hoá. Như là scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install scipy <br/>\n",
    "pip intall sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tại đây, bạn có thể sử dụng sklearn để xây dựng một ma trận TF-IDF. Lớp TF-IDF của sklearn là một <i>model</i> và phương thức <i>.transform()</i> cái mà tuân thủ tất cả các nguyên tắc API cho mô hình học máy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The faster Harry got to the store, the faster and faster Harry would get home.',\n",
       " 'Harry is hairy and faster than Jill.',\n",
       " 'Jill is not as hairy as Harry.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16 0.   0.48 0.21 0.21 0.   0.25 0.21 0.   0.   0.   0.21 0.   0.64\n",
      "  0.21 0.21]\n",
      " [0.37 0.   0.37 0.   0.   0.37 0.29 0.   0.37 0.37 0.   0.   0.49 0.\n",
      "  0.   0.  ]\n",
      " [0.   0.75 0.   0.   0.   0.29 0.22 0.   0.29 0.29 0.38 0.   0.   0.\n",
      "  0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = docs\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "print(model.todense().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model TFIDFVectorizer là một ma trận thưa, bởi vì ma trận TF-IDF hầu hết là số 0, hầu hết số documents là nhhor trong số tổng số từ của vocabulary.<br/>\n",
    ".todense() chuyển đổi một ma trận thưa thành ma trận chính quy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vẽ bảng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okapi BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Những người thông minh tại Đại học thành phố London đã đưa ra một cách tốt hơn để rank\n",
    "kết quả tìm kiếm. Thay vì chỉ đơn thuần là tính toán TF-IDF cosin tương đồng, họ bình thường hóa và mịn tương đồng. Họ cũng bỏ qua thuật ngữ trùng lặp trong tài liệu truy vấn, cắt hiệu quả tần số hạn cho các vector truy vấn ở 1. Và dấu chấm\n",
    "sản phẩm cho tương cosin không bình thường bởi các chỉ tiêu vector TF-IDF (số từ ngữ trong tài liệu và truy vấn), nhưng thay bằng một hàm phi tuyến của\n",
    "độ dài tài liệu riêng của mình:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vì vậy, bạn chỉ cần các vectơ cơ bản nhất TF-IDF để thức ăn vào đường ống của bạn để có được\n",
    "nhà nước-of-the-nghệ thuật trình diễn cho tìm kiếm ngữ nghĩa, phân loại tài liệu, hệ thống thoại, và hầu hết các ứng dụng khác mà chúng tôi đề cập trong chương 1. TF-IDFs là\n",
    "Giai đoạn đầu tiên trong đường ống của bạn, thiết lập cơ bản nhất của đặc biệt bạn sẽ trích từ văn bản. Trong\n",
    "chương tiếp theo, chúng tôi tính toán vector chủ đề từ vectơ TF-IDF của bạn. vectơ Topic\n",
    "là một đại diện tốt hơn về ý nghĩa của nội dung của một túi lời\n",
    "hơn bất kỳ các vectơ TF-IDF bình thường và làm nhẵn cẩn thận. Và điều duy nhất\n",
    "trở nên tốt hơn từ đó khi chúng ta chuyển sang vector từ Word2vec trong chương 6 và embeddings ròng thần kinh về ý nghĩa của từ và tài liệu trong chương sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
