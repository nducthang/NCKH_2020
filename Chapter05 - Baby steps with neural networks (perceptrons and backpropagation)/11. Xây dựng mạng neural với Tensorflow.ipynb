{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nội dung chính:\n",
    "1. Xây dựng mạng Neural với Raw Tensorflow.\n",
    "2. Bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Xây dựng mạng neural với Raw Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta sẽ xây dựng một mạng neural gồm 2 lớp ẩn với Tensorflow để xử lý bài toàn quen thuộc là MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tập dữ liệu là bộ chữ số viết tay MNIST. Dataset bao gồm 60000 mẫu train và 10000 mẫu test. Các hình ảnh có kích thước cố định là 28x28 và chuẩn hoá giá trị từ 0 đến 1. Để đơn giản, mỗi hình ảnh đã được làm phẳng (flatten) và chuyển đổi thành một mảng numpy 1-D gồm 784 đặc trưng (28 * 28)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quá trình xây dựng mạng nơ ron sẽ trải qua 4 bước:\n",
    "\n",
    "1. Chuẩn bị dữ liệu đầu vào.\n",
    "2. Xây dựng mạng nơ ron.\n",
    "3. Thiết lập hàm loss function và phương pháp tối ưu gradient descent.\n",
    "4. Fitting mô hình và đánh giá kết quả.\n",
    "\n",
    "Các bước lần lượt như bên dưới:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Chuẩn bị dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import struct\n",
    "import urllib\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "def parse_data(path, dataset, flatten):\n",
    "    if dataset != 'train' and dataset != 't10k':\n",
    "        raise NameError('dataset must be train or t10k')\n",
    "\n",
    "    label_file = os.path.join(path, dataset + '-labels-idx1-ubyte')\n",
    "    with open(label_file, 'rb') as file:\n",
    "        _, num = struct.unpack(\">II\", file.read(8))\n",
    "        labels = np.fromfile(file, dtype=np.int8)  # int8\n",
    "        new_labels = np.zeros((num, 10))\n",
    "        new_labels[np.arange(num), labels] = 1\n",
    "\n",
    "    img_file = os.path.join(path, dataset + '-images-idx3-ubyte')\n",
    "    with open(img_file, 'rb') as file:\n",
    "        _, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        imgs = np.fromfile(file, dtype=np.uint8).reshape(num, rows, cols)  # uint8\n",
    "        imgs = imgs.astype(np.float32) / 255.0\n",
    "        if flatten:\n",
    "            imgs = imgs.reshape([num, -1])\n",
    "\n",
    "    return imgs, new_labels\n",
    "\n",
    "def read_mnist(path, flatten=True, num_train=55000):\n",
    "    \"\"\"\n",
    "    Read in the mnist dataset, given that the data is stored in path\n",
    "    Return two tuples of numpy arrays\n",
    "    ((train_imgs, train_labels), (test_imgs, test_labels))\n",
    "    \"\"\"\n",
    "    imgs, labels = parse_data(path, 'train', flatten)\n",
    "    indices = np.random.permutation(labels.shape[0])\n",
    "    train_idx, val_idx = indices[:num_train], indices[num_train:]\n",
    "    train_img, train_labels = imgs[train_idx, :], labels[train_idx, :]\n",
    "    val_img, val_labels = imgs[val_idx, :], labels[val_idx, :]\n",
    "    test = parse_data(path, 't10k', flatten)\n",
    "    return [train_img, train_labels], [val_img, val_labels], test\n",
    "\n",
    "mnist_folder = './data/mnist'\n",
    "train, val, test = read_mnist(mnist_folder, flatten=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Xây dựng mạng neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tham số mạng\n",
    "n_hidden_1 = 256 # số feature layer 1\n",
    "n_hidden_2 = 256\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X và Y lần lượt là 2 placeholder được sử dụng để chứa biến dự báo và và biến được dự báo. Kích thước của nó có height bằng None ngụ ý rằng ta có thể đưa vào bao nhiêu quan sát tùy ý. Điều này thuận lợi cho fitting model theo các batch_size khác nhau ở bước sau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta nhận thấy các giá trị cần điều chỉnh (tuning) trong mô hình là các weight và bias. Do đó trong mạng neural ta sẽ xây dựng các layer dựa trên ma trận hệ số và vector bias. Lưu ý chúng ta phải khai báo các ma trận hệ số và vector bias dưới dạng Variable để thuật toán tối ưu có thể hiểu được đây là các giá trị cần cập nhật. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tạo model\n",
    "def neural_network(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Thiết lập hàm mất mát và optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-e60de6feec1f>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "# Kiến trúc mô hình\n",
    "logits = neural_network(X)\n",
    "\n",
    "# định nghịa loss và optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# khởi tạo tất cả các biến\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm `tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)` sẽ tính ra hàm ra giá trị cross entropy giữa 2 phân phối được dự báo từ mạng nơ ron (logits) và giá trị thực tế (ground truth). Giá trị này càng nhỏ thì giữa 2 phân phối càng sát nhau tức mô hình đưa ra dự báo càng chuẩn xác. Thuật toán gradient descent mà chúng ta sử dụng là AdamOptimizer với learning_rate = 0.1. Lưu ý khi ta kích hoạt quá trình train_op thì thuật toán AdamOptimizer sẽ tự động tìm đến các variable và cập nhật lại variable theo phương gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do đó các weights và bias của mỗi layer đều được cập nhật sau mỗi lượt huấn luyện. Tuy nhiên có một số trường hợp thuật toán sẽ dừng lại khi gradient quá nhỏ và accuracy và loss function dường như không thay đổi. Những trường hợp này ta cần thay đổi tốc độ học, thay đổi hàm loss function hoặc điều chỉnh lại cấu trúc mạng nơ ron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm `tf.argmax(x, 1)` sẽ tìm ra class có xác xuất lớn nhất trong lớp các class trả về. Đây chính là class được dự báo. Hàm `tf.equal()` sẽ so sánh class dự báo có trùng với class thực tế (ground truth) hay không. acc là tỷ lệ phần trăm dự báo chính xác được tính ra từ trung bình của toàn bộ các kết quả so sánh ở toàn bộ các quan sát."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 50\n",
    "\n",
    "# tạo hàm lấy batch tiếp theo. Khi lấy hết batch của mẫu thì shuffle lại mẫu.\n",
    "def next_batch(X, batch_size, index=0):\n",
    "    start = index\n",
    "    index += batch_size\n",
    "    if index > len(X[0]):\n",
    "        perm = np.arange(len(X[0]))\n",
    "        np.random.shuffle(perm)\n",
    "        X[0] = X[0][perm]\n",
    "        X[1] = X[1][perm]\n",
    "        start = 0\n",
    "        index = batch_size\n",
    "    end = index\n",
    "    return X[0][start:end], X[1][start:end], index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss= 11289.8096, Training Accuracy= 0.250\n",
      "Step 50, Loss= 621.6454, Training Accuracy= 0.836\n",
      "Step 100, Loss= 245.6350, Training Accuracy= 0.906\n",
      "Step 150, Loss= 239.2593, Training Accuracy= 0.820\n",
      "Step 200, Loss= 135.5943, Training Accuracy= 0.867\n",
      "Step 250, Loss= 131.0241, Training Accuracy= 0.852\n",
      "Step 300, Loss= 79.8466, Training Accuracy= 0.852\n",
      "Step 350, Loss= 46.4432, Training Accuracy= 0.867\n",
      "Step 400, Loss= 62.9294, Training Accuracy= 0.836\n",
      "Step 450, Loss= 54.5932, Training Accuracy= 0.820\n",
      "Step 500, Loss= 26.1929, Training Accuracy= 0.891\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8634\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    idx = 0\n",
    "    for step in range(1, num_steps + 1):\n",
    "        batch_x, batch_y, idx = next_batch(train, batch_size = batch_size, index = idx)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\",sess.run(accuracy, feed_dict={X: test[0], Y: test[1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Câu 1:</b> <br/>\n",
    "<b>Iris flower dataset</b> là một bộ dữ liệu nhỏ (nhỏ hơn rất nhiều so với MNIST. Bộ dữ liệu này bao gồm thông tin của ba loại hoa Iris (một loài hoa lan) khác nhau: <i>Iris setosa, Iris virginica</i> và <i>Iris versicolor</i>. Mỗi loại có 50 bông hoa được đo với dữ liệu là 4 thông tin: chiều dài, chiều rộng đài hoa (sepal), và chiều dài, chiều rộng cánh hoa (petal). Dưới đây là ví dụ về hình ảnh của ba loại hoa. (Chú ý, đây không phải là bộ cơ sở dữ liệu ảnh như MNIST, mỗi điểm dữ liệu trong tập này chỉ là một vector 4 chiều).<br/>\n",
    "<b>Iris flower dataset</b> có sẵn trong thư viện <b>scikit-learn</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 3.5, 1.4, 0.2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tách train data và test data:<br/>\n",
    "Giả sử chúng ta muốn dùng 50 điểm dữ liệu cho test set, 100 điểm còn lại cho training set. Scikit-learn có một hàm số cho phép chúng ta ngẫu nhiên lựa chọn các điểm này, như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=50)\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thiết lập mạng neural thích hợp để phân loại hoa Iris trên."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
