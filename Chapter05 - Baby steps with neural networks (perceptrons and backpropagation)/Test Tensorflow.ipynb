{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow là một thư viện mã nguồn mở dùng để xử lí cách phép tính toán số học bằng cách mô tả một mô hình biểu đồ thể hiện sự thay đổi về giá trị của dữ liệu. Tiền thân của Tensorflow là DistBelief - dự án về hệ thống học máy của Google Brain được phát triển vào năm 2011. Tensorflow là dự án thứ 2, được phát hành dưới dạng mã nguồn mở vào 09/11/2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow = tensor + flow = data + flow => luồng của các dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Một số project nổi tiếng sử dụng tensorlow\n",
    "* Phân loại ung thư da – Dermatologist-level classification of skin cancer with deep neural networks (Esteva et al., Nature 2017)\n",
    "* WaveNet: Text to speech – Wavenet: A generative model for raw audio (Oord et al., 2016)\n",
    "* Vẽ hình – Draw Together with a Neural Network (Ha et al., 2017)\n",
    "* Image Style Transfer Using Convolutional Neural Networks (Gatys et al., 2016) Tensorflow adaptation by Cameroon Smith (cysmith@github)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đơn vị dữ liệu chính của Tensorflow là những Tensor và Tensorflow là dòng chảy của những Tensor.\n",
    "Một Tensor bao gồm một tập hợp các giá trị nguyên thủy (integer, float, string, ..) cấu thành nên một tập hợp không giới hạn số chiều."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rank của một Tensor là số chiều của nó\n",
    "* Shape là một bộ (tuple) của các số biểu diễn số lượng phần tử có trong mỗi chiều."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý: Tensorflow sử dụng mảng numpy để biểu diễn các giá trị của Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ:\n",
    "* 3.0 : Tensor với rank là 0, nói cách khác là 1 Scalar (scalar)\n",
    "* [1., 2., 3.]: Tensor với rank là 1, là một vector có shape là [3] (vector)\n",
    "* [[1., 2., 3.], [4., 5., 6.]]: Tensor với rank là 2, là một ma trận có shape là [2, 3] (matrix)\n",
    "* [[[1., 2., 3.]], [[7., 8., 9.]]]: Tensor với rank là 3 với shape là [2, 1, 3] (N-tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Các thành phần chính của Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow phân biệt rạch ròi việc định nghĩa và tính toán trong quá trình thực thi. Bao gồm:\n",
    "* Xây dựng, định nghĩa đồ thị(Graph)\n",
    "* Sử dụng một Session để thực thi các tính toán trong đồ thị"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhìn chung, các thành phần chính của một chương trình TensorFlow bao gồm:\n",
    "\n",
    "* tf.Graph: Cấu thành nên đồ thị tính toán.\n",
    "* tf.Session: Khởi chạy đồ thị tính toán. Những thành phần khác sẽ có trong các viết tiếp theo, trong phạm vi bài viết này mình sẽ chỉ nhắc tới một số thành phần chính :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/tensors_flowing.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph (Đồ thị) hay một đồ thị tính toán là một tập hợp các phép toán, thao tác của Tensorflow được đặt vào trong một đồ thị. Một graph được cấu thành bởi 2 loại đối tượng chính:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.Operation: Là các node của Graph. Operation mô tả sự tính toán để tạo ra các tensor. Các toán tử này có thể là Const Hằng số, Variable Biến số, Add Phép cộng, Mul Phép nhân...\n",
    "* tf.Tensor: Là các cạnh của Graph. Chúng biểu diễn các giá trị dữ liệu xuyên suốt đồ thị. Hầu hết các hàm của Tensorflow đều trả lại tf.Tensors. Trong thế giới của Tensorflow, mọi kiểu dữ liệu đều được quy về một mối được gọi là Tensor hay trong Tensorflow, tất cả các loại dữ liệu đều là Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tại Sao TF Sử Dụng Graph?\n",
    "* Tối ưu trong tính toán. Cho phép chỉ tính toán các node cần để có được giá trị bạn muốn\n",
    "* Chia bài toán thành các module nhỏ, giúp graph nhận biết module nào cần cho bài toán\n",
    "* Tính toán phân tán, song song trên nhiều CPU, TPU, GPU hoặc trên nhiều máy\n",
    "* Nhiều mô hình máy học sử dụng graph để học và biểu diễn(visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = tf.get_default_graph()\n",
    "g2 = tf.Graph()\n",
    "# add ops to the default graph\n",
    "with g1.as_default():\n",
    "    a = tf.constant(3)\n",
    "# add ops to the user created graph\n",
    "with g2.as_default():\n",
    "    b = tf.constant(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Subgraph</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhìn hình phía trên, bởi vì chúng ta chỉ cần giá trị của pow_op mà pow_op lại không phụ thuộc gì vào useless . Do đó, session trong trường hợp này sẽ không tính giá trị của useless -> Tiết kiệm cho việc tính toán\n",
    "Nhưng nếu bạn muốn lấy cả giá trị của pow_op và uselessthì phải làm sao? Đây là giải pháp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "y = 3\n",
    "add_op = tf.add(x, y)\n",
    "mul_op = tf.multiply(x, y)\n",
    "useless = tf.multiply(x, add_op)\n",
    "pow_op = tf.pow(add_op, mul_op)\n",
    "with tf.Session() as sess:\n",
    "    z, not_useless = sess.run([pow_op, useless])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn chỉ cần truyền vào list các tensor mà bạn cần tính toán giá trị của nó. Chính là tham số fetches trong cú pháp của Session.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fetches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e727356ba333>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'fetches' is not defined"
     ]
    }
   ],
   "source": [
    "tf.Session.run(fetches,feed_dict=None,options=None,run_metadata=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vậy nên có thể hiểu được phần nào cái tên Tensorflow là một thư viện mô tả, điều chỉnh dòng chảy của các Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "tf.Tensor(7.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant(3.0, dtype=tf.float32)\n",
    "b = tf.constant(4.0)\n",
    "total = a + b \n",
    "print(a)\n",
    "print(b)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Tensor(\"Const:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Const_1:0\", shape=(), dtype=float32)\n",
      "Tensor(\"add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "a = tf.constant(3.0, dtype=tf.float32)\n",
    "b = tf.constant(4.0)\n",
    "total = a + b \n",
    "print(a)\n",
    "print(b)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir .  # câu lệnh trên Terminal\n",
    "# tensorboard --logdir=\"./graphs\" --port 6006 # Hoặc bất kỳ port nào khác"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph sẽ được biểu diễn tại page với url được cung cấp dưới dạng localhost:6060/#graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tensorboard/graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>constant</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>placeholder</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: 3, b:4.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: [1,3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Variable</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable giống placeholder yêu cầu lập trình viên phải truyền vào DType của Node đồng thờitruyền vào một giá trị khởi tạo được gọi là Seed Mầm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có 1 lưu ý nữa: Các biến này bạn đã cung cấp giá trị khởi tạo, nhưng vẫn mới chỉ là định nghĩa. Bạn cần khởi tạo chúng trước khi chạy trong session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong quá trình chạy thuật toán, giá trị mới cho các Variable sẽ được tính toán và thay đổi để tạo nên một Model mới. Các giá trị này không được truyền vào sẵn như placeholder mà thay đổi thông qua quá trình chạy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các Variable cần phải được đưa về trạng thái seed trước khi thực hiện các tính toán đi qua Node đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.3        0.6        0.90000004]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, {x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.660002\n"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.train của Tensorflow một OptimizerTrình tối ưu hóa mong muốn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [1,2,3,4]\n",
    "y_train = [0,-1,-2,-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.459616] b: [-0.4965184] loss: 1.5448234\n",
      "W: [-0.48454273] b: [-0.48487374] loss: 1.4825068\n",
      "W: [-0.49684232] b: [-0.4691753] loss: 1.444397\n",
      "W: [-0.5049019] b: [-0.45227283] loss: 1.409699\n",
      "W: [-0.5115062] b: [-0.43511063] loss: 1.376103\n",
      "W: [-0.51758033] b: [-0.41800055] loss: 1.3433373\n",
      "W: [-0.523432] b: [-0.40104443] loss: 1.3113549\n",
      "W: [-0.52916396] b: [-0.38427448] loss: 1.2801343\n",
      "W: [-0.53481066] b: [-0.36769974] loss: 1.2496573\n",
      "W: [-0.54038435] b: [-0.35132164] loss: 1.2199055\n",
      "W: [-0.5458894] b: [-0.33513904] loss: 1.1908623\n",
      "W: [-0.55132794] b: [-0.31915003] loss: 1.1625105\n",
      "W: [-0.5567012] b: [-0.30335245] loss: 1.1348338\n",
      "W: [-0.56201] b: [-0.28774402] loss: 1.1078157\n",
      "W: [-0.5672552] b: [-0.2723225] loss: 1.0814412\n",
      "W: [-0.5724376] b: [-0.25708568] loss: 1.0556946\n",
      "W: [-0.5775579] b: [-0.2420313] loss: 1.0305606\n",
      "W: [-0.5826169] b: [-0.22715722] loss: 1.0060253\n",
      "W: [-0.5876153] b: [-0.21246126] loss: 0.982074\n",
      "W: [-0.59255385] b: [-0.1979413] loss: 0.95869297\n",
      "W: [-0.59743327] b: [-0.18359523] loss: 0.9358686\n",
      "W: [-0.6022543] b: [-0.16942096] loss: 0.91358757\n",
      "W: [-0.6070175] b: [-0.15541643] loss: 0.89183706\n",
      "W: [-0.6117237] b: [-0.14157961] loss: 0.87060446\n",
      "W: [-0.61637354] b: [-0.1279085] loss: 0.84987724\n",
      "W: [-0.62096775] b: [-0.11440111] loss: 0.82964355\n",
      "W: [-0.6255069] b: [-0.10105547] loss: 0.80989164\n",
      "W: [-0.62999165] b: [-0.08786966] loss: 0.7906098\n",
      "W: [-0.6344227] b: [-0.07484176] loss: 0.7717871\n",
      "W: [-0.63880074] b: [-0.06196988] loss: 0.7534125\n",
      "W: [-0.6431263] b: [-0.04925214] loss: 0.73547554\n",
      "W: [-0.6474001] b: [-0.03668671] loss: 0.7179655\n",
      "W: [-0.6516227] b: [-0.02427176] loss: 0.7008723\n",
      "W: [-0.65579474] b: [-0.01200548] loss: 0.68418586\n",
      "W: [-0.6599168] b: [0.00011391] loss: 0.6678971\n",
      "W: [-0.66398954] b: [0.01208815] loss: 0.6519957\n",
      "W: [-0.66801345] b: [0.02391901] loss: 0.63647336\n",
      "W: [-0.6719892] b: [0.03560818] loss: 0.6213203\n",
      "W: [-0.6759173] b: [0.04715736] loss: 0.6065279\n",
      "W: [-0.6797984] b: [0.05856823] loss: 0.59208786\n",
      "W: [-0.683633] b: [0.06984246] loss: 0.5779916\n",
      "W: [-0.68742174] b: [0.08098166] loss: 0.5642309\n",
      "W: [-0.69116503] b: [0.09198748] loss: 0.55079764\n",
      "W: [-0.6948635] b: [0.10286149] loss: 0.5376844\n",
      "W: [-0.6985177] b: [0.11360527] loss: 0.52488333\n",
      "W: [-0.7021281] b: [0.12422038] loss: 0.51238704\n",
      "W: [-0.70569533] b: [0.13470837] loss: 0.5001883\n",
      "W: [-0.7092198] b: [0.14507076] loss: 0.48827982\n",
      "W: [-0.7127021] b: [0.15530905] loss: 0.47665495\n",
      "W: [-0.71614265] b: [0.16542475] loss: 0.4653069\n",
      "W: [-0.719542] b: [0.1754193] loss: 0.4542289\n",
      "W: [-0.7229007] b: [0.18529417] loss: 0.44341472\n",
      "W: [-0.7262191] b: [0.19505078] loss: 0.4328579\n",
      "W: [-0.7294978] b: [0.20469053] loss: 0.42255253\n",
      "W: [-0.73273724] b: [0.21421485] loss: 0.4124925\n",
      "W: [-0.73593783] b: [0.22362511] loss: 0.40267205\n",
      "W: [-0.73910016] b: [0.23292267] loss: 0.3930853\n",
      "W: [-0.74222463] b: [0.24210888] loss: 0.3837267\n",
      "W: [-0.7453116] b: [0.2511851] loss: 0.37459117\n",
      "W: [-0.74836165] b: [0.2601526] loss: 0.3656729\n",
      "W: [-0.7513752] b: [0.26901272] loss: 0.35696703\n",
      "W: [-0.7543526] b: [0.27776673] loss: 0.34846842\n",
      "W: [-0.7572944] b: [0.28641593] loss: 0.34017223\n",
      "W: [-0.760201] b: [0.29496154] loss: 0.33207333\n",
      "W: [-0.76307267] b: [0.3034048] loss: 0.32416746\n",
      "W: [-0.76591] b: [0.31174695] loss: 0.3164498\n",
      "W: [-0.7687134] b: [0.3199892] loss: 0.30891576\n",
      "W: [-0.7714832] b: [0.32813275] loss: 0.30156112\n",
      "W: [-0.7742198] b: [0.33617878] loss: 0.29438165\n",
      "W: [-0.77692366] b: [0.34412843] loss: 0.28737307\n",
      "W: [-0.77959514] b: [0.3519829] loss: 0.28053135\n",
      "W: [-0.7822346] b: [0.3597433] loss: 0.27385256\n",
      "W: [-0.7848425] b: [0.36741075] loss: 0.26733264\n",
      "W: [-0.78741914] b: [0.37498638] loss: 0.2609681\n",
      "W: [-0.7899649] b: [0.3824713] loss: 0.25475502\n",
      "W: [-0.79248023] b: [0.38986656] loss: 0.24868989\n",
      "W: [-0.7949654] b: [0.3971733] loss: 0.24276918\n",
      "W: [-0.7974208] b: [0.4043925] loss: 0.23698932\n",
      "W: [-0.7998468] b: [0.41152528] loss: 0.23134711\n",
      "W: [-0.80224377] b: [0.41857263] loss: 0.22583924\n",
      "W: [-0.80461204] b: [0.4255356] loss: 0.22046256\n",
      "W: [-0.80695194] b: [0.43241516] loss: 0.2152138\n",
      "W: [-0.8092638] b: [0.43921232] loss: 0.21008998\n",
      "W: [-0.811548] b: [0.4459281] loss: 0.20508823\n",
      "W: [-0.8138048] b: [0.45256343] loss: 0.20020556\n",
      "W: [-0.8160346] b: [0.45911932] loss: 0.19543907\n",
      "W: [-0.8182377] b: [0.4655967] loss: 0.1907861\n",
      "W: [-0.8204144] b: [0.47199652] loss: 0.18624389\n",
      "W: [-0.8225651] b: [0.47831967] loss: 0.18180983\n",
      "W: [-0.82469] b: [0.4845671] loss: 0.17748134\n",
      "W: [-0.82678944] b: [0.49073973] loss: 0.17325595\n",
      "W: [-0.82886374] b: [0.49683845] loss: 0.16913109\n",
      "W: [-0.8309132] b: [0.5028641] loss: 0.16510442\n",
      "W: [-0.8329381] b: [0.5088176] loss: 0.16117366\n",
      "W: [-0.83493876] b: [0.5146998] loss: 0.1573365\n",
      "W: [-0.8369155] b: [0.52051157] loss: 0.15359063\n",
      "W: [-0.8388685] b: [0.52625376] loss: 0.14993396\n",
      "W: [-0.84079814] b: [0.53192717] loss: 0.14636436\n",
      "W: [-0.84270465] b: [0.5375326] loss: 0.14287974\n",
      "W: [-0.8445884] b: [0.543071] loss: 0.13947809\n",
      "W: [-0.84644955] b: [0.548543] loss: 0.13615741\n",
      "W: [-0.8482884] b: [0.5539495] loss: 0.13291575\n",
      "W: [-0.8501053] b: [0.5592912] loss: 0.12975137\n",
      "W: [-0.85190034] b: [0.56456894] loss: 0.12666227\n",
      "W: [-0.85367393] b: [0.5697835] loss: 0.12364671\n",
      "W: [-0.85542625] b: [0.5749356] loss: 0.12070299\n",
      "W: [-0.85715765] b: [0.58002603] loss: 0.11782924\n",
      "W: [-0.85886824] b: [0.5850555] loss: 0.115023986\n",
      "W: [-0.8605584] b: [0.5900247] loss: 0.11228554\n",
      "W: [-0.86222833] b: [0.5949344] loss: 0.1096123\n",
      "W: [-0.86387825] b: [0.5997853] loss: 0.10700263\n",
      "W: [-0.8655084] b: [0.60457814] loss: 0.10445517\n",
      "W: [-0.86711895] b: [0.60931355] loss: 0.101968296\n",
      "W: [-0.8687103] b: [0.6139923] loss: 0.09954065\n",
      "W: [-0.87028253] b: [0.618615] loss: 0.0971708\n",
      "W: [-0.871836] b: [0.6231823] loss: 0.09485737\n",
      "W: [-0.8733709] b: [0.6276949] loss: 0.09259902\n",
      "W: [-0.87488735] b: [0.6321535] loss: 0.09039444\n",
      "W: [-0.8763856] b: [0.6365587] loss: 0.08824237\n",
      "W: [-0.877866] b: [0.64091116] loss: 0.08614146\n",
      "W: [-0.8793286] b: [0.64521146] loss: 0.08409064\n",
      "W: [-0.8807737] b: [0.64946026] loss: 0.08208866\n",
      "W: [-0.88220155] b: [0.65365815] loss: 0.080134295\n",
      "W: [-0.8836123] b: [0.6578058] loss: 0.0782265\n",
      "W: [-0.88500607] b: [0.6619038] loss: 0.07636407\n",
      "W: [-0.8863832] b: [0.6659527] loss: 0.07454602\n",
      "W: [-0.8877438] b: [0.6699531] loss: 0.07277124\n",
      "W: [-0.88908815] b: [0.6739056] loss: 0.071038716\n",
      "W: [-0.8904164] b: [0.6778108] loss: 0.06934741\n",
      "W: [-0.8917287] b: [0.6816692] loss: 0.06769647\n",
      "W: [-0.89302534] b: [0.68548137] loss: 0.06608471\n",
      "W: [-0.8943064] b: [0.6892479] loss: 0.06451141\n",
      "W: [-0.8955721] b: [0.6929694] loss: 0.06297551\n",
      "W: [-0.89682275] b: [0.6966463] loss: 0.06147619\n",
      "W: [-0.89805835] b: [0.7002791] loss: 0.06001258\n",
      "W: [-0.8992792] b: [0.70386845] loss: 0.05858381\n",
      "W: [-0.90048534] b: [0.7074148] loss: 0.057189114\n",
      "W: [-0.9016771] b: [0.71091866] loss: 0.055827554\n",
      "W: [-0.90285456] b: [0.71438056] loss: 0.054498427\n",
      "W: [-0.9040179] b: [0.71780103] loss: 0.05320094\n",
      "W: [-0.9051674] b: [0.72118056] loss: 0.051934354\n",
      "W: [-0.90630305] b: [0.7245196] loss: 0.050697908\n",
      "W: [-0.90742517] b: [0.72781867] loss: 0.049490876\n",
      "W: [-0.9085338] b: [0.7310782] loss: 0.0483126\n",
      "W: [-0.90962917] b: [0.7342987] loss: 0.04716239\n",
      "W: [-0.9107114] b: [0.73748064] loss: 0.046039555\n",
      "W: [-0.91178066] b: [0.7406245] loss: 0.044943433\n",
      "W: [-0.91283715] b: [0.74373066] loss: 0.04387343\n",
      "W: [-0.913881] b: [0.74679965] loss: 0.04282894\n",
      "W: [-0.91491234] b: [0.74983186] loss: 0.04180926\n",
      "W: [-0.9159313] b: [0.75282776] loss: 0.0408139\n",
      "W: [-0.91693807] b: [0.7557878] loss: 0.039842192\n",
      "W: [-0.9179328] b: [0.75871235] loss: 0.03889362\n",
      "W: [-0.91891557] b: [0.7616019] loss: 0.03796767\n",
      "W: [-0.9198866] b: [0.76445687] loss: 0.03706373\n",
      "W: [-0.920846] b: [0.76727766] loss: 0.036181346\n",
      "W: [-0.92179394] b: [0.77006465] loss: 0.03531994\n",
      "W: [-0.9227305] b: [0.77281827] loss: 0.034479067\n",
      "W: [-0.92365587] b: [0.7755389] loss: 0.033658177\n",
      "W: [-0.92457014] b: [0.778227] loss: 0.032856867\n",
      "W: [-0.92547345] b: [0.78088284] loss: 0.0320746\n",
      "W: [-0.926366] b: [0.7835069] loss: 0.03131098\n",
      "W: [-0.92724776] b: [0.7860995] loss: 0.030565538\n",
      "W: [-0.928119] b: [0.78866106] loss: 0.029837828\n",
      "W: [-0.9289798] b: [0.791192] loss: 0.029127488\n",
      "W: [-0.9298303] b: [0.7936926] loss: 0.02843401\n",
      "W: [-0.9306706] b: [0.79616326] loss: 0.02775706\n",
      "W: [-0.9315009] b: [0.7986043] loss: 0.027096223\n",
      "W: [-0.9323212] b: [0.80101615] loss: 0.026451124\n",
      "W: [-0.9331317] b: [0.8033991] loss: 0.025821388\n",
      "W: [-0.9339325] b: [0.80575347] loss: 0.025206644\n",
      "W: [-0.9347237] b: [0.80807966] loss: 0.024606552\n",
      "W: [-0.9355054] b: [0.810378] loss: 0.024020713\n",
      "W: [-0.93627775] b: [0.81264883] loss: 0.023448836\n",
      "W: [-0.93704087] b: [0.8148925] loss: 0.022890586\n",
      "W: [-0.93779486] b: [0.8171092] loss: 0.022345616\n",
      "W: [-0.9385398] b: [0.81929946] loss: 0.0218136\n",
      "W: [-0.9392758] b: [0.82146347] loss: 0.021294251\n",
      "W: [-0.940003] b: [0.82360154] loss: 0.020787284\n",
      "W: [-0.9407215] b: [0.825714] loss: 0.02029242\n",
      "W: [-0.9414314] b: [0.82780117] loss: 0.019809289\n",
      "W: [-0.94213283] b: [0.82986337] loss: 0.019337665\n",
      "W: [-0.9428258] b: [0.8319009] loss: 0.018877272\n",
      "W: [-0.9435105] b: [0.833914] loss: 0.018427864\n",
      "W: [-0.944187] b: [0.8359029] loss: 0.017989129\n",
      "W: [-0.9448554] b: [0.8378681] loss: 0.017560832\n",
      "W: [-0.94551575] b: [0.8398097] loss: 0.017142761\n",
      "W: [-0.94616824] b: [0.8417281] loss: 0.016734619\n",
      "W: [-0.9468129] b: [0.8436235] loss: 0.016336197\n",
      "W: [-0.94744986] b: [0.84549624] loss: 0.015947267\n",
      "W: [-0.94807917] b: [0.8473465] loss: 0.015567628\n",
      "W: [-0.94870096] b: [0.8491746] loss: 0.015196995\n",
      "W: [-0.9493153] b: [0.8509808] loss: 0.014835178\n",
      "W: [-0.94992226] b: [0.85276544] loss: 0.014481971\n",
      "W: [-0.950522] b: [0.85452867] loss: 0.014137197\n",
      "W: [-0.95111454] b: [0.8562708] loss: 0.013800618\n",
      "W: [-0.9517] b: [0.85799205] loss: 0.013472065\n",
      "W: [-0.95227844] b: [0.8596927] loss: 0.01315132\n",
      "W: [-0.9528499] b: [0.86137295] loss: 0.012838208\n",
      "W: [-0.95341456] b: [0.8630331] loss: 0.012532554\n",
      "W: [-0.9539724] b: [0.8646734] loss: 0.012234184\n",
      "W: [-0.9545236] b: [0.86629397] loss: 0.011942908\n",
      "W: [-0.95506823] b: [0.8678952] loss: 0.011658582\n",
      "W: [-0.9556063] b: [0.8694772] loss: 0.011381033\n",
      "W: [-0.95613796] b: [0.8710403] loss: 0.011110076\n",
      "W: [-0.95666325] b: [0.87258464] loss: 0.01084556\n",
      "W: [-0.9571822] b: [0.8741105] loss: 0.01058734\n",
      "W: [-0.957695] b: [0.8756181] loss: 0.0103352815\n",
      "W: [-0.9582016] b: [0.8771077] loss: 0.0100892205\n",
      "W: [-0.95870215] b: [0.8785794] loss: 0.00984903\n",
      "W: [-0.95919675] b: [0.88003343] loss: 0.009614559\n",
      "W: [-0.9596854] b: [0.8814701] loss: 0.009385641\n",
      "W: [-0.9601682] b: [0.88288957] loss: 0.009162186\n",
      "W: [-0.9606452] b: [0.88429207] loss: 0.008944061\n",
      "W: [-0.9611165] b: [0.88567775] loss: 0.00873113\n",
      "W: [-0.9615822] b: [0.8870468] loss: 0.008523252\n",
      "W: [-0.9620422] b: [0.8883995] loss: 0.0083203465\n",
      "W: [-0.9624968] b: [0.88973594] loss: 0.0081222495\n",
      "W: [-0.96294594] b: [0.8910564] loss: 0.007928886\n",
      "W: [-0.9633897] b: [0.8923611] loss: 0.007740113\n",
      "W: [-0.9638281] b: [0.8936502] loss: 0.007555818\n",
      "W: [-0.96426123] b: [0.8949238] loss: 0.007375947\n",
      "W: [-0.96468925] b: [0.8961821] loss: 0.0072003356\n",
      "W: [-0.9651121] b: [0.8974254] loss: 0.0070289127\n",
      "W: [-0.9655299] b: [0.8986538] loss: 0.0068615675\n",
      "W: [-0.96594274] b: [0.8998675] loss: 0.006698211\n",
      "W: [-0.9663506] b: [0.9010666] loss: 0.006538734\n",
      "W: [-0.96675354] b: [0.9022514] loss: 0.006383075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9671517] b: [0.903422] loss: 0.0062311105\n",
      "W: [-0.9675451] b: [0.90457857] loss: 0.006082746\n",
      "W: [-0.9679337] b: [0.9057213] loss: 0.0059379456\n",
      "W: [-0.96831775] b: [0.90685034] loss: 0.00579657\n",
      "W: [-0.9686972] b: [0.90796584] loss: 0.005658568\n",
      "W: [-0.96907204] b: [0.909068] loss: 0.005523844\n",
      "W: [-0.9694424] b: [0.91015697] loss: 0.0053923395\n",
      "W: [-0.96980834] b: [0.9112329] loss: 0.005263962\n",
      "W: [-0.9701699] b: [0.91229594] loss: 0.00513863\n",
      "W: [-0.9705272] b: [0.91334623] loss: 0.005016298\n",
      "W: [-0.97088015] b: [0.91438395] loss: 0.004896871\n",
      "W: [-0.97122884] b: [0.91540927] loss: 0.0047802883\n",
      "W: [-0.9715734] b: [0.9164223] loss: 0.004666484\n",
      "W: [-0.9719138] b: [0.9174232] loss: 0.0045553814\n",
      "W: [-0.97225016] b: [0.9184121] loss: 0.004446921\n",
      "W: [-0.97258246] b: [0.9193891] loss: 0.004341066\n",
      "W: [-0.9729108] b: [0.9203545] loss: 0.0042377156\n",
      "W: [-0.97323525] b: [0.9213083] loss: 0.004136818\n",
      "W: [-0.97355574] b: [0.9222507] loss: 0.004038335\n",
      "W: [-0.9738724] b: [0.9231818] loss: 0.0039421855\n",
      "W: [-0.97418535] b: [0.9241017] loss: 0.0038483376\n",
      "W: [-0.9744945] b: [0.9250106] loss: 0.0037567173\n",
      "W: [-0.97479993] b: [0.9259087] loss: 0.0036672773\n",
      "W: [-0.9751017] b: [0.92679596] loss: 0.003579963\n",
      "W: [-0.97539985] b: [0.9276726] loss: 0.003494733\n",
      "W: [-0.9756945] b: [0.9285388] loss: 0.003411529\n",
      "W: [-0.9759855] b: [0.9293946] loss: 0.0033303138\n",
      "W: [-0.9762731] b: [0.93024015] loss: 0.0032510194\n",
      "W: [-0.97655725] b: [0.9310756] loss: 0.00317362\n",
      "W: [-0.976838] b: [0.931901] loss: 0.003098064\n",
      "W: [-0.9771154] b: [0.9327165] loss: 0.0030243008\n",
      "W: [-0.97738945] b: [0.9335222] loss: 0.002952306\n",
      "W: [-0.97766024] b: [0.93431836] loss: 0.0028820103\n",
      "W: [-0.97792774] b: [0.93510497] loss: 0.0028134054\n",
      "W: [-0.9781921] b: [0.9358821] loss: 0.0027464214\n",
      "W: [-0.9784533] b: [0.9366499] loss: 0.0026810304\n",
      "W: [-0.9787113] b: [0.93740857] loss: 0.002617213\n",
      "W: [-0.97896624] b: [0.93815815] loss: 0.0025548914\n",
      "W: [-0.9792181] b: [0.93889874] loss: 0.0024940707\n",
      "W: [-0.979467] b: [0.93963045] loss: 0.002434699\n",
      "W: [-0.9797129] b: [0.9403534] loss: 0.00237673\n",
      "W: [-0.97995585] b: [0.9410677] loss: 0.0023201455\n",
      "W: [-0.9801959] b: [0.9417735] loss: 0.002264906\n",
      "W: [-0.98043305] b: [0.9424708] loss: 0.0022109824\n",
      "W: [-0.98066735] b: [0.94315976] loss: 0.0021583433\n",
      "W: [-0.9808989] b: [0.94384044] loss: 0.0021069618\n",
      "W: [-0.9811277] b: [0.94451296] loss: 0.0020568015\n",
      "W: [-0.9813537] b: [0.94517744] loss: 0.002007825\n",
      "W: [-0.981577] b: [0.945834] loss: 0.0019600228\n",
      "W: [-0.9817976] b: [0.94648266] loss: 0.0019133639\n",
      "W: [-0.98201555] b: [0.9471236] loss: 0.0018678051\n",
      "W: [-0.98223096] b: [0.9477568] loss: 0.001823335\n",
      "W: [-0.98244375] b: [0.9483825] loss: 0.0017799326\n",
      "W: [-0.98265404] b: [0.94900066] loss: 0.0017375464\n",
      "W: [-0.98286176] b: [0.9496114] loss: 0.0016961822\n",
      "W: [-0.983067] b: [0.95021486] loss: 0.0016558003\n",
      "W: [-0.98326975] b: [0.9508111] loss: 0.0016163823\n",
      "W: [-0.98347014] b: [0.95140016] loss: 0.0015778908\n",
      "W: [-0.9836681] b: [0.9519822] loss: 0.0015403276\n",
      "W: [-0.9838637] b: [0.95255727] loss: 0.0015036539\n",
      "W: [-0.98405695] b: [0.9531254] loss: 0.0014678554\n",
      "W: [-0.98424786] b: [0.9536868] loss: 0.0014329049\n",
      "W: [-0.9844365] b: [0.9542414] loss: 0.0013987906\n",
      "W: [-0.9846229] b: [0.9547894] loss: 0.0013654913\n",
      "W: [-0.984807] b: [0.95533085] loss: 0.0013329812\n",
      "W: [-0.984989] b: [0.9558658] loss: 0.0013012483\n",
      "W: [-0.98516876] b: [0.9563943] loss: 0.0012702696\n",
      "W: [-0.9853464] b: [0.9569165] loss: 0.0012400275\n",
      "W: [-0.98552185] b: [0.95743245] loss: 0.0012105061\n",
      "W: [-0.98569524] b: [0.95794225] loss: 0.001181682\n",
      "W: [-0.98586655] b: [0.9584459] loss: 0.0011535526\n",
      "W: [-0.9860358] b: [0.95894355] loss: 0.0011260853\n",
      "W: [-0.9862031] b: [0.9594352] loss: 0.0010992752\n",
      "W: [-0.9863683] b: [0.959921] loss: 0.0010731005\n",
      "W: [-0.9865315] b: [0.960401] loss: 0.0010475557\n",
      "W: [-0.9866928] b: [0.9608752] loss: 0.0010226164\n",
      "W: [-0.98685217] b: [0.96134377] loss: 0.0009982688\n",
      "W: [-0.98700964] b: [0.9618067] loss: 0.0009745013\n",
      "W: [-0.9871652] b: [0.9622641] loss: 0.0009512993\n",
      "W: [-0.9873189] b: [0.96271604] loss: 0.0009286518\n",
      "W: [-0.98747075] b: [0.96316254] loss: 0.00090653874\n",
      "W: [-0.9876208] b: [0.9636037] loss: 0.00088496343\n",
      "W: [-0.98776907] b: [0.9640395] loss: 0.00086389546\n",
      "W: [-0.9879155] b: [0.96447015] loss: 0.0008433256\n",
      "W: [-0.98806024] b: [0.96489567] loss: 0.000823247\n",
      "W: [-0.9882032] b: [0.96531606] loss: 0.0008036472\n",
      "W: [-0.9883445] b: [0.96573144] loss: 0.00078451034\n",
      "W: [-0.9884841] b: [0.9661418] loss: 0.0007658389\n",
      "W: [-0.988622] b: [0.9665473] loss: 0.0007475993\n",
      "W: [-0.98875827] b: [0.9669479] loss: 0.00072980113\n",
      "W: [-0.98889285] b: [0.96734375] loss: 0.00071243144\n",
      "W: [-0.9890259] b: [0.9677348] loss: 0.0006954675\n",
      "W: [-0.9891573] b: [0.96812123] loss: 0.00067890924\n",
      "W: [-0.9892872] b: [0.968503] loss: 0.0006627453\n",
      "W: [-0.98941547] b: [0.9688802] loss: 0.0006469678\n",
      "W: [-0.98954225] b: [0.9692529] loss: 0.00063156337\n",
      "W: [-0.9896675] b: [0.9696211] loss: 0.0006165292\n",
      "W: [-0.9897912] b: [0.96998495] loss: 0.0006018503\n",
      "W: [-0.98991346] b: [0.9703444] loss: 0.00058752083\n",
      "W: [-0.9900343] b: [0.97069955] loss: 0.0005735312\n",
      "W: [-0.9901536] b: [0.97105044] loss: 0.0005598791\n",
      "W: [-0.9902715] b: [0.9713971] loss: 0.00054655096\n",
      "W: [-0.99038804] b: [0.97173965] loss: 0.00053353806\n",
      "W: [-0.99050313] b: [0.9720781] loss: 0.0005208335\n",
      "W: [-0.99061686] b: [0.97241247] loss: 0.0005084313\n",
      "W: [-0.9907292] b: [0.97274286] loss: 0.00049632875\n",
      "W: [-0.99084026] b: [0.97306925] loss: 0.00048451062\n",
      "W: [-0.9909499] b: [0.9733918] loss: 0.00047297764\n",
      "W: [-0.99105835] b: [0.9737104] loss: 0.00046171912\n",
      "W: [-0.99116546] b: [0.97402525] loss: 0.00045072407\n",
      "W: [-0.99127126] b: [0.9743363] loss: 0.0004399932\n",
      "W: [-0.9913758] b: [0.97464365] loss: 0.00042951838\n",
      "W: [-0.99147904] b: [0.97494733] loss: 0.00041929382\n",
      "W: [-0.9915811] b: [0.9752474] loss: 0.0004093084\n",
      "W: [-0.9916819] b: [0.9755438] loss: 0.0003995624\n",
      "W: [-0.9917815] b: [0.9758367] loss: 0.00039005151\n",
      "W: [-0.99187994] b: [0.9761261] loss: 0.0003807664\n",
      "W: [-0.9919772] b: [0.976412] loss: 0.00037169852\n",
      "W: [-0.9920733] b: [0.97669446] loss: 0.00036285137\n",
      "W: [-0.99216825] b: [0.9769736] loss: 0.0003542077\n",
      "W: [-0.992262] b: [0.9772494] loss: 0.00034577594\n",
      "W: [-0.9923547] b: [0.97752184] loss: 0.00033754553\n",
      "W: [-0.99244624] b: [0.977791] loss: 0.00032950976\n",
      "W: [-0.9925367] b: [0.97805697] loss: 0.00032166144\n",
      "W: [-0.9926261] b: [0.97831976] loss: 0.00031400323\n",
      "W: [-0.9927144] b: [0.9785794] loss: 0.0003065307\n",
      "W: [-0.99280167] b: [0.97883594] loss: 0.0002992325\n",
      "W: [-0.99288785] b: [0.9790894] loss: 0.0002921054\n",
      "W: [-0.992973] b: [0.9793398] loss: 0.0002851549\n",
      "W: [-0.9930572] b: [0.9795872] loss: 0.00027836597\n",
      "W: [-0.99314034] b: [0.97983164] loss: 0.00027173734\n",
      "W: [-0.9932225] b: [0.98007315] loss: 0.00026526835\n",
      "W: [-0.99330366] b: [0.9803118] loss: 0.00025895235\n",
      "W: [-0.9933838] b: [0.9805476] loss: 0.00025278848\n",
      "W: [-0.99346304] b: [0.98078054] loss: 0.00024676922\n",
      "W: [-0.99354136] b: [0.9810107] loss: 0.00024089438\n",
      "W: [-0.99361867] b: [0.98123807] loss: 0.0002351616\n",
      "W: [-0.9936951] b: [0.9814628] loss: 0.00022955929\n",
      "W: [-0.9937706] b: [0.98168474] loss: 0.00022409589\n",
      "W: [-0.99384516] b: [0.9819041] loss: 0.0002187594\n",
      "W: [-0.9939189] b: [0.9821208] loss: 0.00021355208\n",
      "W: [-0.99399173] b: [0.9823349] loss: 0.00020846816\n",
      "W: [-0.9940637] b: [0.98254645] loss: 0.00020350685\n",
      "W: [-0.9941348] b: [0.9827555] loss: 0.00019865987\n",
      "W: [-0.994205] b: [0.982962] loss: 0.00019393156\n",
      "W: [-0.99427444] b: [0.98316604] loss: 0.00018931158\n",
      "W: [-0.994343] b: [0.9833676] loss: 0.00018480777\n",
      "W: [-0.99441075] b: [0.9835668] loss: 0.00018040591\n",
      "W: [-0.9944776] b: [0.98376364] loss: 0.00017611119\n",
      "W: [-0.9945438] b: [0.98395807] loss: 0.00017191809\n",
      "W: [-0.9946091] b: [0.9841502] loss: 0.00016782542\n",
      "W: [-0.99467367] b: [0.98433995] loss: 0.00016383051\n",
      "W: [-0.99473745] b: [0.98452747] loss: 0.00015992913\n",
      "W: [-0.99480045] b: [0.9847128] loss: 0.0001561229\n",
      "W: [-0.99486274] b: [0.9848958] loss: 0.00015240749\n",
      "W: [-0.99492425] b: [0.9850767] loss: 0.00014877622\n",
      "W: [-0.99498504] b: [0.9852554] loss: 0.00014523693\n",
      "W: [-0.9950451] b: [0.98543197] loss: 0.00014177707\n",
      "W: [-0.99510443] b: [0.98560643] loss: 0.00013840289\n",
      "W: [-0.995163] b: [0.9857788] loss: 0.00013510798\n",
      "W: [-0.99522096] b: [0.9859491] loss: 0.00013189041\n",
      "W: [-0.9952782] b: [0.98611736] loss: 0.00012875168\n",
      "W: [-0.99533474] b: [0.9862836] loss: 0.00012568639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9953906] b: [0.9864479] loss: 0.00012269312\n",
      "W: [-0.9954458] b: [0.9866102] loss: 0.00011977301\n",
      "W: [-0.9955003] b: [0.9867705] loss: 0.00011692288\n",
      "W: [-0.99555427] b: [0.98692894] loss: 0.0001141378\n",
      "W: [-0.9956075] b: [0.98708546] loss: 0.00011141955\n",
      "W: [-0.99566007] b: [0.98724014] loss: 0.00010876788\n",
      "W: [-0.99571204] b: [0.98739296] loss: 0.000106178064\n",
      "W: [-0.9957634] b: [0.98754394] loss: 0.00010365113\n",
      "W: [-0.99581414] b: [0.98769313] loss: 0.000101181126\n",
      "W: [-0.9958643] b: [0.98784053] loss: 9.877408e-05\n",
      "W: [-0.9959138] b: [0.98798615] loss: 9.6420394e-05\n",
      "W: [-0.99596274] b: [0.98813003] loss: 9.412569e-05\n",
      "W: [-0.9960111] b: [0.9882722] loss: 9.188465e-05\n",
      "W: [-0.9960589] b: [0.9884126] loss: 8.969724e-05\n",
      "W: [-0.9961061] b: [0.9885514] loss: 8.75618e-05\n",
      "W: [-0.9961527] b: [0.98868847] loss: 8.547736e-05\n",
      "W: [-0.9961988] b: [0.98882395] loss: 8.344197e-05\n",
      "W: [-0.99624425] b: [0.9889578] loss: 8.145621e-05\n",
      "W: [-0.99628925] b: [0.98909] loss: 7.951629e-05\n",
      "W: [-0.9963337] b: [0.9892207] loss: 7.7623015e-05\n",
      "W: [-0.9963776] b: [0.9893498] loss: 7.577511e-05\n",
      "W: [-0.996421] b: [0.98947734] loss: 7.397088e-05\n",
      "W: [-0.99646384] b: [0.98960334] loss: 7.220903e-05\n",
      "W: [-0.9965062] b: [0.98972785] loss: 7.049143e-05\n",
      "W: [-0.99654806] b: [0.9898509] loss: 6.8813126e-05\n",
      "W: [-0.9965894] b: [0.9899724] loss: 6.717385e-05\n",
      "W: [-0.99663025] b: [0.9900925] loss: 6.5574495e-05\n",
      "W: [-0.9966706] b: [0.9902112] loss: 6.401318e-05\n",
      "W: [-0.9967105] b: [0.99032843] loss: 6.248971e-05\n",
      "W: [-0.9967499] b: [0.99044424] loss: 6.1001167e-05\n",
      "W: [-0.9967888] b: [0.9905587] loss: 5.954807e-05\n",
      "W: [-0.99682724] b: [0.99067175] loss: 5.8130296e-05\n",
      "W: [-0.9968652] b: [0.99078345] loss: 5.6747427e-05\n",
      "W: [-0.99690276] b: [0.99089384] loss: 5.5396165e-05\n",
      "W: [-0.9969399] b: [0.99100286] loss: 5.4077478e-05\n",
      "W: [-0.9969765] b: [0.9911106] loss: 5.2789946e-05\n",
      "W: [-0.9970127] b: [0.9912171] loss: 5.1534073e-05\n",
      "W: [-0.9970485] b: [0.9913222] loss: 5.0306713e-05\n",
      "W: [-0.99708384] b: [0.9914261] loss: 4.9110076e-05\n",
      "W: [-0.9971188] b: [0.9915288] loss: 4.7939175e-05\n",
      "W: [-0.9971533] b: [0.99163026] loss: 4.6798603e-05\n",
      "W: [-0.9971874] b: [0.9917305] loss: 4.568408e-05\n",
      "W: [-0.99722105] b: [0.9918296] loss: 4.4596884e-05\n",
      "W: [-0.9972543] b: [0.99192744] loss: 4.353365e-05\n",
      "W: [-0.9972872] b: [0.9920241] loss: 4.2498425e-05\n",
      "W: [-0.9973197] b: [0.9921196] loss: 4.1486186e-05\n",
      "W: [-0.99735177] b: [0.99221396] loss: 4.049878e-05\n",
      "W: [-0.9973835] b: [0.9923072] loss: 3.9534872e-05\n",
      "W: [-0.9974148] b: [0.9923993] loss: 3.8593826e-05\n",
      "W: [-0.99744576] b: [0.9924903] loss: 3.76752e-05\n",
      "W: [-0.99747634] b: [0.99258024] loss: 3.677811e-05\n",
      "W: [-0.99750656] b: [0.9926691] loss: 3.5902456e-05\n",
      "W: [-0.9975364] b: [0.9927569] loss: 3.5047575e-05\n",
      "W: [-0.9975659] b: [0.9928436] loss: 3.4213368e-05\n",
      "W: [-0.9975951] b: [0.99292934] loss: 3.339934e-05\n",
      "W: [-0.9976239] b: [0.993014] loss: 3.2603155e-05\n",
      "W: [-0.99765235] b: [0.99309766] loss: 3.1826796e-05\n",
      "W: [-0.9976805] b: [0.99318033] loss: 3.106893e-05\n",
      "W: [-0.99770826] b: [0.993262] loss: 3.0330495e-05\n",
      "W: [-0.99773574] b: [0.9933427] loss: 2.9607585e-05\n",
      "W: [-0.9977628] b: [0.99342245] loss: 2.8902703e-05\n",
      "W: [-0.9977896] b: [0.9935012] loss: 2.8214748e-05\n",
      "W: [-0.9978161] b: [0.99357903] loss: 2.7542355e-05\n",
      "W: [-0.99784225] b: [0.9936559] loss: 2.6887192e-05\n",
      "W: [-0.99786806] b: [0.9937319] loss: 2.6247017e-05\n",
      "W: [-0.99789363] b: [0.99380696] loss: 2.5622669e-05\n",
      "W: [-0.99791884] b: [0.9938811] loss: 2.5012829e-05\n",
      "W: [-0.99794376] b: [0.99395436] loss: 2.4417077e-05\n",
      "W: [-0.9979684] b: [0.9940268] loss: 2.3836097e-05\n",
      "W: [-0.9979927] b: [0.9940983] loss: 2.3268696e-05\n",
      "W: [-0.9980168] b: [0.994169] loss: 2.2714023e-05\n",
      "W: [-0.9980405] b: [0.99423885] loss: 2.2173104e-05\n",
      "W: [-0.998064] b: [0.9943078] loss: 2.1644932e-05\n",
      "W: [-0.99808717] b: [0.994376] loss: 2.1129883e-05\n",
      "W: [-0.99811006] b: [0.99444336] loss: 2.0626934e-05\n",
      "W: [-0.9981327] b: [0.9945099] loss: 2.0135982e-05\n",
      "W: [-0.99815506] b: [0.9945756] loss: 1.9657076e-05\n",
      "W: [-0.9981772] b: [0.9946406] loss: 1.9188492e-05\n",
      "W: [-0.998199] b: [0.9947048] loss: 1.8731598e-05\n",
      "W: [-0.99822056] b: [0.9947682] loss: 1.8285558e-05\n",
      "W: [-0.99824184] b: [0.99483085] loss: 1.7850418e-05\n",
      "W: [-0.9982629] b: [0.9948927] loss: 1.7425227e-05\n",
      "W: [-0.9982837] b: [0.9949539] loss: 1.701084e-05\n",
      "W: [-0.99830425] b: [0.9950143] loss: 1.6605385e-05\n",
      "W: [-0.9983246] b: [0.99507403] loss: 1.6210317e-05\n",
      "W: [-0.99834466] b: [0.99513304] loss: 1.582381e-05\n",
      "W: [-0.99836445] b: [0.99519134] loss: 1.544751e-05\n",
      "W: [-0.99838406] b: [0.9952489] loss: 1.5080138e-05\n",
      "W: [-0.99840343] b: [0.99530584] loss: 1.4720535e-05\n",
      "W: [-0.99842256] b: [0.99536204] loss: 1.43701145e-05\n",
      "W: [-0.99844146] b: [0.9954176] loss: 1.4027925e-05\n",
      "W: [-0.9984601] b: [0.9954725] loss: 1.3693476e-05\n",
      "W: [-0.99847853] b: [0.99552673] loss: 1.3367787e-05\n",
      "W: [-0.9984968] b: [0.9955803] loss: 1.3049029e-05\n",
      "W: [-0.9985148] b: [0.99563324] loss: 1.2738334e-05\n",
      "W: [-0.99853253] b: [0.9956855] loss: 1.2435634e-05\n",
      "W: [-0.9985501] b: [0.9957372] loss: 1.2139853e-05\n",
      "W: [-0.9985675] b: [0.9957882] loss: 1.1850905e-05\n",
      "W: [-0.9985847] b: [0.99583864] loss: 1.1568455e-05\n",
      "W: [-0.9986016] b: [0.9958885] loss: 1.12930975e-05\n",
      "W: [-0.99861836] b: [0.9959377] loss: 1.1024592e-05\n",
      "W: [-0.9986349] b: [0.99598634] loss: 1.0761537e-05\n",
      "W: [-0.9986512] b: [0.9960344] loss: 1.0505828e-05\n",
      "W: [-0.99866736] b: [0.9960819] loss: 1.0255582e-05\n",
      "W: [-0.9986833] b: [0.9961288] loss: 1.0011587e-05\n",
      "W: [-0.99869907] b: [0.99617517] loss: 9.773492e-06\n",
      "W: [-0.9987147] b: [0.99622095] loss: 9.540863e-06\n",
      "W: [-0.99873006] b: [0.9962662] loss: 9.313486e-06\n",
      "W: [-0.99874526] b: [0.9963109] loss: 9.092123e-06\n",
      "W: [-0.9987603] b: [0.99635506] loss: 8.875421e-06\n",
      "W: [-0.9987751] b: [0.9963987] loss: 8.6642e-06\n",
      "W: [-0.9987898] b: [0.99644184] loss: 8.4578405e-06\n",
      "W: [-0.9988043] b: [0.99648446] loss: 8.256767e-06\n",
      "W: [-0.99881864] b: [0.99652654] loss: 8.05989e-06\n",
      "W: [-0.99883276] b: [0.99656814] loss: 7.868304e-06\n",
      "W: [-0.9988467] b: [0.9966093] loss: 7.680357e-06\n",
      "W: [-0.99886054] b: [0.99664986] loss: 7.498108e-06\n",
      "W: [-0.9988742] b: [0.99669] loss: 7.3193482e-06\n",
      "W: [-0.99888766] b: [0.9967296] loss: 7.145077e-06\n",
      "W: [-0.99890095] b: [0.9967688] loss: 6.974762e-06\n",
      "W: [-0.9989141] b: [0.99680746] loss: 6.8093027e-06\n",
      "W: [-0.9989272] b: [0.99684566] loss: 6.647196e-06\n",
      "W: [-0.99894] b: [0.99688345] loss: 6.488657e-06\n",
      "W: [-0.9989527] b: [0.99692076] loss: 6.3344833e-06\n",
      "W: [-0.9989652] b: [0.99695766] loss: 6.183135e-06\n",
      "W: [-0.9989776] b: [0.9969941] loss: 6.0359816e-06\n",
      "W: [-0.9989899] b: [0.9970301] loss: 5.892207e-06\n",
      "W: [-0.999002] b: [0.99706566] loss: 5.751859e-06\n",
      "W: [-0.9990139] b: [0.99710083] loss: 5.615111e-06\n",
      "W: [-0.9990257] b: [0.9971355] loss: 5.4817974e-06\n",
      "W: [-0.9990374] b: [0.9971698] loss: 5.3510885e-06\n",
      "W: [-0.9990489] b: [0.9972037] loss: 5.2234464e-06\n",
      "W: [-0.9990603] b: [0.9972372] loss: 5.099274e-06\n",
      "W: [-0.99907154] b: [0.9972703] loss: 4.978124e-06\n",
      "W: [-0.9990827] b: [0.99730295] loss: 4.8594397e-06\n",
      "W: [-0.99909365] b: [0.99733526] loss: 4.743706e-06\n",
      "W: [-0.9991045] b: [0.99736714] loss: 4.6308805e-06\n",
      "W: [-0.9991152] b: [0.9973987] loss: 4.5206034e-06\n",
      "W: [-0.99912584] b: [0.99742985] loss: 4.41293e-06\n",
      "W: [-0.9991363] b: [0.9974606] loss: 4.307916e-06\n",
      "W: [-0.99914664] b: [0.997491] loss: 4.205633e-06\n",
      "W: [-0.9991569] b: [0.99752104] loss: 4.1054936e-06\n",
      "W: [-0.99916697] b: [0.9975507] loss: 4.007572e-06\n",
      "W: [-0.9991769] b: [0.99758005] loss: 3.912206e-06\n",
      "W: [-0.99918675] b: [0.997609] loss: 3.819322e-06\n",
      "W: [-0.9991965] b: [0.9976376] loss: 3.7284258e-06\n",
      "W: [-0.9992061] b: [0.99766594] loss: 3.6392262e-06\n",
      "W: [-0.9992156] b: [0.9976939] loss: 3.5529988e-06\n",
      "W: [-0.999225] b: [0.9977215] loss: 3.468243e-06\n",
      "W: [-0.9992343] b: [0.9977488] loss: 3.385633e-06\n",
      "W: [-0.9992435] b: [0.99777573] loss: 3.3050794e-06\n",
      "W: [-0.99925256] b: [0.9978024] loss: 3.2261712e-06\n",
      "W: [-0.9992615] b: [0.9978287] loss: 3.149475e-06\n",
      "W: [-0.9992703] b: [0.9978547] loss: 3.0745712e-06\n",
      "W: [-0.9992791] b: [0.9978804] loss: 3.0015176e-06\n",
      "W: [-0.9992877] b: [0.9979058] loss: 2.929837e-06\n",
      "W: [-0.99929625] b: [0.9979309] loss: 2.8598963e-06\n",
      "W: [-0.99930465] b: [0.9979557] loss: 2.7921515e-06\n",
      "W: [-0.999313] b: [0.9979802] loss: 2.7254189e-06\n",
      "W: [-0.9993212] b: [0.9980044] loss: 2.6605464e-06\n",
      "W: [-0.9993294] b: [0.9980283] loss: 2.59715e-06\n",
      "W: [-0.99933743] b: [0.9980519] loss: 2.5353309e-06\n",
      "W: [-0.99934536] b: [0.99807525] loss: 2.4748904e-06\n",
      "W: [-0.9993532] b: [0.9980983] loss: 2.4159729e-06\n",
      "W: [-0.9993609] b: [0.9981211] loss: 2.3584448e-06\n",
      "W: [-0.9993686] b: [0.99814355] loss: 2.3021853e-06\n",
      "W: [-0.9993761] b: [0.9981658] loss: 2.2477125e-06\n",
      "W: [-0.9993836] b: [0.9981877] loss: 2.1942492e-06\n",
      "W: [-0.999391] b: [0.9982094] loss: 2.1420192e-06\n",
      "W: [-0.9993983] b: [0.9982309] loss: 2.0910002e-06\n",
      "W: [-0.9994055] b: [0.99825203] loss: 2.040971e-06\n",
      "W: [-0.9994126] b: [0.99827296] loss: 1.9924291e-06\n",
      "W: [-0.9994196] b: [0.99829364] loss: 1.945143e-06\n",
      "W: [-0.9994266] b: [0.9983141] loss: 1.898806e-06\n",
      "W: [-0.99943346] b: [0.9983343] loss: 1.8534247e-06\n",
      "W: [-0.99944025] b: [0.99835426] loss: 1.8092474e-06\n",
      "W: [-0.9994469] b: [0.998374] loss: 1.766286e-06\n",
      "W: [-0.99945354] b: [0.9983935] loss: 1.72422e-06\n",
      "W: [-0.9994601] b: [0.9984127] loss: 1.6829968e-06\n",
      "W: [-0.9994666] b: [0.99843174] loss: 1.6428967e-06\n",
      "W: [-0.999473] b: [0.9984505] loss: 1.6039311e-06\n",
      "W: [-0.9994793] b: [0.99846905] loss: 1.5657794e-06\n",
      "W: [-0.99948555] b: [0.9984874] loss: 1.5285556e-06\n",
      "W: [-0.9994917] b: [0.99850553] loss: 1.4920581e-06\n",
      "W: [-0.9994978] b: [0.9985234] loss: 1.4565753e-06\n",
      "W: [-0.9995038] b: [0.9985411] loss: 1.421846e-06\n",
      "W: [-0.99950975] b: [0.9985586] loss: 1.3878758e-06\n",
      "W: [-0.9995156] b: [0.99857587] loss: 1.3550404e-06\n",
      "W: [-0.99952143] b: [0.9985929] loss: 1.3225433e-06\n",
      "W: [-0.99952716] b: [0.9986098] loss: 1.2911473e-06\n",
      "W: [-0.9995328] b: [0.9986264] loss: 1.2604395e-06\n",
      "W: [-0.9995384] b: [0.99864286] loss: 1.2304312e-06\n",
      "W: [-0.99954396] b: [0.99865913] loss: 1.2010846e-06\n",
      "W: [-0.9995494] b: [0.99867517] loss: 1.1725413e-06\n",
      "W: [-0.99955475] b: [0.998691] loss: 1.1446723e-06\n",
      "W: [-0.9995601] b: [0.9987067] loss: 1.1175107e-06\n",
      "W: [-0.9995654] b: [0.9987222] loss: 1.0908703e-06\n",
      "W: [-0.9995706] b: [0.9987375] loss: 1.0647896e-06\n",
      "W: [-0.99957573] b: [0.99875265] loss: 1.0393928e-06\n",
      "W: [-0.9995808] b: [0.9987676] loss: 1.0146382e-06\n",
      "W: [-0.99958587] b: [0.99878234] loss: 9.905177e-07\n",
      "W: [-0.9995908] b: [0.99879694] loss: 9.669059e-07\n",
      "W: [-0.9995957] b: [0.99881136] loss: 9.439644e-07\n",
      "W: [-0.9996005] b: [0.9988256] loss: 9.2137077e-07\n",
      "W: [-0.9996053] b: [0.9988397] loss: 8.994385e-07\n",
      "W: [-0.99961] b: [0.99885356] loss: 8.7795627e-07\n",
      "W: [-0.9996147] b: [0.9988673] loss: 8.571626e-07\n",
      "W: [-0.99961936] b: [0.99888086] loss: 8.3671165e-07\n",
      "W: [-0.99962395] b: [0.9988943] loss: 8.166709e-07\n",
      "W: [-0.9996284] b: [0.9989075] loss: 7.9735565e-07\n",
      "W: [-0.9996329] b: [0.99892056] loss: 7.7847915e-07\n",
      "W: [-0.99963725] b: [0.9989335] loss: 7.598702e-07\n",
      "W: [-0.9996416] b: [0.99894625] loss: 7.417094e-07\n",
      "W: [-0.9996459] b: [0.9989589] loss: 7.240291e-07\n",
      "W: [-0.9996501] b: [0.99897134] loss: 7.0697416e-07\n",
      "W: [-0.9996543] b: [0.9989837] loss: 6.900376e-07\n",
      "W: [-0.99965847] b: [0.99899584] loss: 6.73615e-07\n",
      "W: [-0.9996626] b: [0.9990079] loss: 6.576387e-07\n",
      "W: [-0.99966663] b: [0.99901974] loss: 6.4200356e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9996706] b: [0.9990315] loss: 6.2663287e-07\n",
      "W: [-0.99967456] b: [0.9990431] loss: 6.1160574e-07\n",
      "W: [-0.99967843] b: [0.99905455] loss: 5.9723016e-07\n",
      "W: [-0.9996823] b: [0.9990659] loss: 5.8292716e-07\n",
      "W: [-0.9996861] b: [0.9990771] loss: 5.690242e-07\n",
      "W: [-0.9996899] b: [0.9990881] loss: 5.5542375e-07\n",
      "W: [-0.9996936] b: [0.999099] loss: 5.422995e-07\n",
      "W: [-0.9996972] b: [0.9991098] loss: 5.293937e-07\n",
      "W: [-0.99970084] b: [0.9991205] loss: 5.167816e-07\n",
      "W: [-0.9997044] b: [0.999131] loss: 5.0438996e-07\n",
      "W: [-0.99970794] b: [0.99914145] loss: 4.924376e-07\n",
      "W: [-0.99971145] b: [0.9991517] loss: 4.8073923e-07\n",
      "W: [-0.9997149] b: [0.99916184] loss: 4.693875e-07\n",
      "W: [-0.9997183] b: [0.99917185] loss: 4.5809816e-07\n",
      "W: [-0.9997217] b: [0.99918175] loss: 4.47282e-07\n",
      "W: [-0.99972504] b: [0.9991915] loss: 4.3665466e-07\n",
      "W: [-0.9997283] b: [0.9992012] loss: 4.263514e-07\n",
      "W: [-0.99973154] b: [0.9992108] loss: 4.1612634e-07\n",
      "W: [-0.99973476] b: [0.9992202] loss: 4.0624224e-07\n",
      "W: [-0.9997379] b: [0.99922955] loss: 3.966282e-07\n",
      "W: [-0.9997411] b: [0.9992388] loss: 3.8709587e-07\n",
      "W: [-0.9997442] b: [0.9992479] loss: 3.7794723e-07\n",
      "W: [-0.9997473] b: [0.9992569] loss: 3.6887556e-07\n",
      "W: [-0.99975026] b: [0.9992658] loss: 3.6006156e-07\n",
      "W: [-0.99975324] b: [0.9992746] loss: 3.515949e-07\n",
      "W: [-0.9997562] b: [0.9992833] loss: 3.4314627e-07\n",
      "W: [-0.99975914] b: [0.9992919] loss: 3.349744e-07\n",
      "W: [-0.99976206] b: [0.99930036] loss: 3.2706106e-07\n",
      "W: [-0.9997649] b: [0.99930876] loss: 3.191963e-07\n",
      "W: [-0.9997677] b: [0.99931705] loss: 3.1153303e-07\n",
      "W: [-0.99977046] b: [0.9993252] loss: 3.0424792e-07\n",
      "W: [-0.9997732] b: [0.99933326] loss: 2.9692467e-07\n",
      "W: [-0.99977595] b: [0.99934125] loss: 2.8989945e-07\n",
      "W: [-0.9997786] b: [0.9993491] loss: 2.83019e-07\n",
      "W: [-0.9997813] b: [0.9993569] loss: 2.763146e-07\n",
      "W: [-0.99978393] b: [0.9993646] loss: 2.6969212e-07\n",
      "W: [-0.9997865] b: [0.99937224] loss: 2.632578e-07\n",
      "W: [-0.99978906] b: [0.99937975] loss: 2.5704907e-07\n",
      "W: [-0.99979156] b: [0.9993872] loss: 2.5092038e-07\n",
      "W: [-0.99979407] b: [0.99939454] loss: 2.449491e-07\n",
      "W: [-0.9997965] b: [0.9994018] loss: 2.3905557e-07\n",
      "W: [-0.99979895] b: [0.99940896] loss: 2.333747e-07\n",
      "W: [-0.9998014] b: [0.99941605] loss: 2.2780239e-07\n",
      "W: [-0.9998038] b: [0.999423] loss: 2.223868e-07\n",
      "W: [-0.9998061] b: [0.99942994] loss: 2.170964e-07\n",
      "W: [-0.99980843] b: [0.99943674] loss: 2.1199307e-07\n",
      "W: [-0.9998107] b: [0.9994435] loss: 2.069188e-07\n",
      "W: [-0.99981296] b: [0.99945015] loss: 2.0198854e-07\n",
      "W: [-0.9998152] b: [0.9994567] loss: 1.9718198e-07\n",
      "W: [-0.99981743] b: [0.9994632] loss: 1.9254385e-07\n",
      "W: [-0.99981964] b: [0.99946964] loss: 1.879545e-07\n",
      "W: [-0.9998218] b: [0.999476] loss: 1.8341291e-07\n",
      "W: [-0.99982387] b: [0.9994823] loss: 1.7911616e-07\n",
      "W: [-0.999826] b: [0.9994885] loss: 1.7484537e-07\n",
      "W: [-0.9998281] b: [0.9994946] loss: 1.7062862e-07\n",
      "W: [-0.9998301] b: [0.9995007] loss: 1.6656634e-07\n",
      "W: [-0.9998322] b: [0.99950665] loss: 1.6263552e-07\n",
      "W: [-0.99983424] b: [0.99951255] loss: 1.5868464e-07\n",
      "W: [-0.9998362] b: [0.9995184] loss: 1.5494777e-07\n",
      "W: [-0.9998382] b: [0.9995242] loss: 1.5120884e-07\n",
      "W: [-0.9998401] b: [0.9995299] loss: 1.4760087e-07\n",
      "W: [-0.999842] b: [0.9995355] loss: 1.4418039e-07\n",
      "W: [-0.9998439] b: [0.99954104] loss: 1.4071772e-07\n",
      "W: [-0.99984574] b: [0.9995465] loss: 1.3737952e-07\n",
      "W: [-0.9998476] b: [0.99955195] loss: 1.3415081e-07\n",
      "W: [-0.99984944] b: [0.9995573] loss: 1.3091059e-07\n",
      "W: [-0.9998512] b: [0.9995626] loss: 1.278376e-07\n",
      "W: [-0.999853] b: [0.99956787] loss: 1.2479168e-07\n",
      "W: [-0.9998548] b: [0.99957305] loss: 1.21769e-07\n",
      "W: [-0.99985653] b: [0.9995782] loss: 1.1890235e-07\n",
      "W: [-0.99985826] b: [0.99958324] loss: 1.1603102e-07\n",
      "W: [-0.99986] b: [0.99958825] loss: 1.1325456e-07\n",
      "W: [-0.99986166] b: [0.9995932] loss: 1.10585574e-07\n",
      "W: [-0.9998633] b: [0.9995981] loss: 1.0787643e-07\n",
      "W: [-0.99986494] b: [0.9996029] loss: 1.0530513e-07\n",
      "W: [-0.99986655] b: [0.9996077] loss: 1.0285744e-07\n",
      "W: [-0.99986815] b: [0.9996124] loss: 1.00367146e-07\n",
      "W: [-0.9998697] b: [0.99961704] loss: 9.79864e-08\n",
      "W: [-0.9998713] b: [0.9996216] loss: 9.563887e-08\n",
      "W: [-0.99987286] b: [0.99962616] loss: 9.336297e-08\n",
      "W: [-0.99987435] b: [0.99963063] loss: 9.114984e-08\n",
      "W: [-0.9998759] b: [0.99963504] loss: 8.897513e-08\n",
      "W: [-0.9998774] b: [0.9996394] loss: 8.683544e-08\n",
      "W: [-0.9998788] b: [0.99964374] loss: 8.481959e-08\n",
      "W: [-0.9998803] b: [0.99964803] loss: 8.2756685e-08\n",
      "W: [-0.99988174] b: [0.99965227] loss: 8.080717e-08\n",
      "W: [-0.9998832] b: [0.99965644] loss: 7.884887e-08\n",
      "W: [-0.99988455] b: [0.99966055] loss: 7.6948666e-08\n",
      "W: [-0.9998859] b: [0.9996646] loss: 7.514916e-08\n",
      "W: [-0.9998873] b: [0.9996686] loss: 7.3368525e-08\n",
      "W: [-0.99988866] b: [0.9996726] loss: 7.161242e-08\n",
      "W: [-0.99988997] b: [0.9996765] loss: 6.989913e-08\n",
      "W: [-0.9998913] b: [0.9996804] loss: 6.826694e-08\n",
      "W: [-0.9998926] b: [0.9996842] loss: 6.664712e-08\n",
      "W: [-0.9998939] b: [0.99968797] loss: 6.503694e-08\n",
      "W: [-0.99989516] b: [0.9996917] loss: 6.348556e-08\n",
      "W: [-0.9998964] b: [0.9996954] loss: 6.1949294e-08\n",
      "W: [-0.99989766] b: [0.99969906] loss: 6.052721e-08\n",
      "W: [-0.9998989] b: [0.99970263] loss: 5.9069837e-08\n",
      "W: [-0.9999001] b: [0.9997062] loss: 5.7660404e-08\n",
      "W: [-0.9999013] b: [0.9997097] loss: 5.6261726e-08\n",
      "W: [-0.9999024] b: [0.9997132] loss: 5.4959646e-08\n",
      "W: [-0.9999036] b: [0.99971664] loss: 5.361655e-08\n",
      "W: [-0.99990475] b: [0.99972004] loss: 5.2364985e-08\n",
      "W: [-0.9999059] b: [0.9997234] loss: 5.1145904e-08\n",
      "W: [-0.999907] b: [0.99972665] loss: 4.994051e-08\n",
      "W: [-0.99990815] b: [0.99972993] loss: 4.8703058e-08\n",
      "W: [-0.9999092] b: [0.99973315] loss: 4.7594497e-08\n",
      "W: [-0.9999103] b: [0.99973637] loss: 4.6413177e-08\n",
      "W: [-0.99991137] b: [0.9997395] loss: 4.530556e-08\n",
      "W: [-0.99991244] b: [0.9997426] loss: 4.427712e-08\n",
      "W: [-0.9999135] b: [0.9997457] loss: 4.317049e-08\n",
      "W: [-0.9999145] b: [0.99974877] loss: 4.21679e-08\n",
      "W: [-0.99991554] b: [0.99975175] loss: 4.119545e-08\n",
      "W: [-0.99991655] b: [0.9997547] loss: 4.0189693e-08\n",
      "W: [-0.99991757] b: [0.99975765] loss: 3.9216662e-08\n",
      "W: [-0.9999185] b: [0.99976057] loss: 3.8321403e-08\n",
      "W: [-0.99991953] b: [0.9997634] loss: 3.738879e-08\n",
      "W: [-0.9999205] b: [0.99976623] loss: 3.652687e-08\n",
      "W: [-0.99992144] b: [0.99976903] loss: 3.5635285e-08\n",
      "W: [-0.99992234] b: [0.9997718] loss: 3.481911e-08\n",
      "W: [-0.9999233] b: [0.9997745] loss: 3.394814e-08\n",
      "W: [-0.9999242] b: [0.9997772] loss: 3.3185117e-08\n",
      "W: [-0.99992514] b: [0.9997799] loss: 3.2370085e-08\n",
      "W: [-0.99992603] b: [0.9997825] loss: 3.162029e-08\n",
      "W: [-0.9999269] b: [0.9997851] loss: 3.084651e-08\n",
      "W: [-0.9999278] b: [0.9997877] loss: 3.0091844e-08\n",
      "W: [-0.99992865] b: [0.99979025] loss: 2.9374078e-08\n",
      "W: [-0.9999295] b: [0.99979275] loss: 2.8711966e-08\n",
      "W: [-0.9999303] b: [0.99979526] loss: 2.7990964e-08\n",
      "W: [-0.99993116] b: [0.9997977] loss: 2.732482e-08\n",
      "W: [-0.999932] b: [0.99980015] loss: 2.6703106e-08\n",
      "W: [-0.9999328] b: [0.99980253] loss: 2.6067099e-08\n",
      "W: [-0.99993366] b: [0.9998049] loss: 2.5440727e-08\n",
      "W: [-0.99993443] b: [0.99980724] loss: 2.483948e-08\n",
      "W: [-0.9999352] b: [0.99980956] loss: 2.4212682e-08\n",
      "W: [-0.999936] b: [0.9998118] loss: 2.365815e-08\n",
      "W: [-0.99993676] b: [0.9998141] loss: 2.307435e-08\n",
      "W: [-0.99993753] b: [0.9998163] loss: 2.2558808e-08\n",
      "W: [-0.9999383] b: [0.9998185] loss: 2.2017915e-08\n",
      "W: [-0.999939] b: [0.9998207] loss: 2.1473756e-08\n",
      "W: [-0.99993974] b: [0.99982285] loss: 2.096387e-08\n",
      "W: [-0.99994045] b: [0.999825] loss: 2.0460124e-08\n",
      "W: [-0.9999412] b: [0.9998271] loss: 1.9989287e-08\n",
      "W: [-0.9999419] b: [0.9998292] loss: 1.9480684e-08\n",
      "W: [-0.9999426] b: [0.9998312] loss: 1.903378e-08\n",
      "W: [-0.99994326] b: [0.9998332] loss: 1.8567832e-08\n",
      "W: [-0.9999439] b: [0.9998352] loss: 1.8161586e-08\n",
      "W: [-0.9999446] b: [0.99983716] loss: 1.7714104e-08\n",
      "W: [-0.9999453] b: [0.9998391] loss: 1.728731e-08\n",
      "W: [-0.99994594] b: [0.99984103] loss: 1.6867602e-08\n",
      "W: [-0.9999466] b: [0.99984294] loss: 1.6477006e-08\n",
      "W: [-0.9999472] b: [0.99984485] loss: 1.6081774e-08\n",
      "W: [-0.99994785] b: [0.9998467] loss: 1.5700795e-08\n",
      "W: [-0.9999485] b: [0.99984854] loss: 1.5324392e-08\n",
      "W: [-0.9999491] b: [0.99985033] loss: 1.4962556e-08\n",
      "W: [-0.9999497] b: [0.9998521] loss: 1.4610723e-08\n",
      "W: [-0.9999503] b: [0.9998539] loss: 1.4258834e-08\n",
      "W: [-0.9999509] b: [0.99985564] loss: 1.3910878e-08\n",
      "W: [-0.9999515] b: [0.99985737] loss: 1.3591748e-08\n",
      "W: [-0.9999521] b: [0.9998591] loss: 1.3263502e-08\n",
      "W: [-0.9999527] b: [0.99986076] loss: 1.2950252e-08\n",
      "W: [-0.9999532] b: [0.99986243] loss: 1.2643046e-08\n",
      "W: [-0.99995375] b: [0.9998641] loss: 1.2339569e-08\n",
      "W: [-0.99995434] b: [0.9998657] loss: 1.2046716e-08\n",
      "W: [-0.9999549] b: [0.9998673] loss: 1.1748899e-08\n",
      "W: [-0.9999554] b: [0.9998689] loss: 1.1476036e-08\n",
      "W: [-0.99995595] b: [0.9998705] loss: 1.121721e-08\n",
      "W: [-0.9999565] b: [0.999872] loss: 1.0939349e-08\n",
      "W: [-0.999957] b: [0.9998736] loss: 1.0686676e-08\n",
      "W: [-0.99995756] b: [0.99987507] loss: 1.04241735e-08\n",
      "W: [-0.99995804] b: [0.99987656] loss: 1.01775335e-08\n",
      "W: [-0.9999585] b: [0.99987805] loss: 9.925131e-09\n",
      "W: [-0.999959] b: [0.99987954] loss: 9.696723e-09\n",
      "W: [-0.9999595] b: [0.99988097] loss: 9.474107e-09\n",
      "W: [-0.99996] b: [0.9998824] loss: 9.248325e-09\n",
      "W: [-0.9999605] b: [0.99988383] loss: 9.025271e-09\n",
      "W: [-0.99996096] b: [0.9998852] loss: 8.813419e-09\n",
      "W: [-0.99996144] b: [0.9998866] loss: 8.584607e-09\n",
      "W: [-0.99996185] b: [0.99988794] loss: 8.390089e-09\n",
      "W: [-0.99996233] b: [0.99988925] loss: 8.203003e-09\n",
      "W: [-0.9999628] b: [0.99989057] loss: 7.999233e-09\n",
      "W: [-0.9999632] b: [0.9998919] loss: 7.819484e-09\n",
      "W: [-0.9999637] b: [0.9998932] loss: 7.62056e-09\n",
      "W: [-0.9999641] b: [0.9998945] loss: 7.445152e-09\n",
      "W: [-0.99996454] b: [0.99989575] loss: 7.26898e-09\n",
      "W: [-0.99996495] b: [0.999897] loss: 7.0875785e-09\n",
      "W: [-0.99996537] b: [0.99989825] loss: 6.908479e-09\n",
      "W: [-0.9999658] b: [0.99989945] loss: 6.753911e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999662] b: [0.99990064] loss: 6.5869017e-09\n",
      "W: [-0.9999666] b: [0.99990183] loss: 6.4369523e-09\n",
      "W: [-0.999967] b: [0.999903] loss: 6.28393e-09\n",
      "W: [-0.9999674] b: [0.99990416] loss: 6.136748e-09\n",
      "W: [-0.9999678] b: [0.9999053] loss: 5.9922662e-09\n",
      "W: [-0.9999682] b: [0.9999064] loss: 5.8577996e-09\n",
      "W: [-0.9999685] b: [0.99990755] loss: 5.7098113e-09\n",
      "W: [-0.9999689] b: [0.9999087] loss: 5.5730567e-09\n",
      "W: [-0.9999693] b: [0.99990976] loss: 5.4324296e-09\n",
      "W: [-0.99996966] b: [0.99991083] loss: 5.312291e-09\n",
      "W: [-0.99997] b: [0.9999119] loss: 5.192664e-09\n",
      "W: [-0.9999704] b: [0.999913] loss: 5.0602473e-09\n",
      "W: [-0.99997073] b: [0.999914] loss: 4.935032e-09\n",
      "W: [-0.9999711] b: [0.999915] loss: 4.8191175e-09\n",
      "W: [-0.99997145] b: [0.999916] loss: 4.7181885e-09\n",
      "W: [-0.9999718] b: [0.99991703] loss: 4.5981388e-09\n",
      "W: [-0.9999721] b: [0.99991804] loss: 4.4872515e-09\n",
      "W: [-0.99997246] b: [0.999919] loss: 4.382006e-09\n",
      "W: [-0.99997276] b: [0.99991995] loss: 4.274696e-09\n",
      "W: [-0.9999731] b: [0.9999209] loss: 4.171966e-09\n",
      "W: [-0.9999734] b: [0.99992186] loss: 4.078462e-09\n",
      "W: [-0.9999737] b: [0.9999228] loss: 3.9750176e-09\n",
      "W: [-0.9999741] b: [0.9999237] loss: 3.887738e-09\n",
      "W: [-0.99997437] b: [0.9999246] loss: 3.797684e-09\n",
      "W: [-0.99997467] b: [0.9999255] loss: 3.7019454e-09\n",
      "W: [-0.99997497] b: [0.9999264] loss: 3.6193377e-09\n",
      "W: [-0.99997526] b: [0.9999273] loss: 3.5324668e-09\n",
      "W: [-0.99997556] b: [0.9999282] loss: 3.4474148e-09\n",
      "W: [-0.99997586] b: [0.999929] loss: 3.3607108e-09\n",
      "W: [-0.99997616] b: [0.99992985] loss: 3.2878837e-09\n",
      "W: [-0.99997646] b: [0.9999307] loss: 3.203354e-09\n",
      "W: [-0.9999767] b: [0.9999315] loss: 3.1392489e-09\n",
      "W: [-0.999977] b: [0.99993235] loss: 3.0572416e-09\n",
      "W: [-0.9999773] b: [0.9999332] loss: 2.9878038e-09\n",
      "W: [-0.9999776] b: [0.99993396] loss: 2.9124294e-09\n",
      "W: [-0.9999778] b: [0.99993473] loss: 2.8446472e-09\n",
      "W: [-0.99997807] b: [0.9999355] loss: 2.7841196e-09\n",
      "W: [-0.99997836] b: [0.9999363] loss: 2.7165044e-09\n",
      "W: [-0.9999786] b: [0.99993706] loss: 2.6462494e-09\n",
      "W: [-0.99997884] b: [0.99993783] loss: 2.5769396e-09\n",
      "W: [-0.9999791] b: [0.99993855] loss: 2.5180498e-09\n",
      "W: [-0.9999793] b: [0.99993926] loss: 2.459842e-09\n",
      "W: [-0.99997956] b: [0.99994] loss: 2.4023166e-09\n",
      "W: [-0.9999798] b: [0.9999407] loss: 2.3454731e-09\n",
      "W: [-0.99998003] b: [0.9999414] loss: 2.2893119e-09\n",
      "W: [-0.9999803] b: [0.9999421] loss: 2.2338327e-09\n",
      "W: [-0.9999805] b: [0.99994284] loss: 2.1790356e-09\n",
      "W: [-0.99998075] b: [0.9999435] loss: 2.1336355e-09\n",
      "W: [-0.999981] b: [0.99994415] loss: 2.0887256e-09\n",
      "W: [-0.9999812] b: [0.9999448] loss: 2.0398154e-09\n",
      "W: [-0.99998146] b: [0.99994546] loss: 1.98213e-09\n",
      "W: [-0.99998164] b: [0.9999461] loss: 1.9402933e-09\n",
      "W: [-0.9999819] b: [0.9999468] loss: 1.8934294e-09\n",
      "W: [-0.9999821] b: [0.9999474] loss: 1.8508644e-09\n",
      "W: [-0.99998236] b: [0.9999481] loss: 1.7998936e-09\n",
      "W: [-0.99998254] b: [0.99994874] loss: 1.7561383e-09\n",
      "W: [-0.9999828] b: [0.99994934] loss: 1.7104362e-09\n",
      "W: [-0.99998295] b: [0.99994993] loss: 1.6747919e-09\n",
      "W: [-0.99998313] b: [0.9999505] loss: 1.6400925e-09\n",
      "W: [-0.9999834] b: [0.9999511] loss: 1.5958683e-09\n",
      "W: [-0.99998355] b: [0.9999517] loss: 1.5619861e-09\n",
      "W: [-0.9999838] b: [0.9999523] loss: 1.5188562e-09\n",
      "W: [-0.99998397] b: [0.9999529] loss: 1.485791e-09\n",
      "W: [-0.9999842] b: [0.99995345] loss: 1.4509851e-09\n",
      "W: [-0.9999844] b: [0.999954] loss: 1.41371e-09\n",
      "W: [-0.99998456] b: [0.9999545] loss: 1.377316e-09\n",
      "W: [-0.99998474] b: [0.99995506] loss: 1.3526886e-09\n",
      "W: [-0.9999849] b: [0.9999556] loss: 1.3207e-09\n",
      "W: [-0.9999851] b: [0.99995613] loss: 1.2855281e-09\n",
      "W: [-0.9999853] b: [0.99995667] loss: 1.2504415e-09\n",
      "W: [-0.99998546] b: [0.9999572] loss: 1.219675e-09\n",
      "W: [-0.99998564] b: [0.99995774] loss: 1.196554e-09\n",
      "W: [-0.9999858] b: [0.9999582] loss: 1.1659758e-09\n",
      "W: [-0.999986] b: [0.9999587] loss: 1.1425101e-09\n",
      "W: [-0.9999861] b: [0.9999592] loss: 1.1172006e-09\n",
      "W: [-0.9999863] b: [0.99995965] loss: 1.0876455e-09\n",
      "W: [-0.99998647] b: [0.9999601] loss: 1.0649757e-09\n",
      "W: [-0.9999866] b: [0.9999606] loss: 1.0405756e-09\n",
      "W: [-0.99998677] b: [0.9999611] loss: 1.0120438e-09\n",
      "W: [-0.99998695] b: [0.99996156] loss: 9.901697e-10\n",
      "W: [-0.99998707] b: [0.99996203] loss: 9.666792e-10\n",
      "W: [-0.99998724] b: [0.9999625] loss: 9.391705e-10\n",
      "W: [-0.9999874] b: [0.9999629] loss: 9.208527e-10\n",
      "W: [-0.99998754] b: [0.9999634] loss: 8.981722e-10\n",
      "W: [-0.9999877] b: [0.9999638] loss: 8.7374374e-10\n",
      "W: [-0.99998784] b: [0.99996424] loss: 8.543566e-10\n",
      "W: [-0.99998796] b: [0.99996465] loss: 8.3537444e-10\n",
      "W: [-0.99998814] b: [0.9999651] loss: 8.1171336e-10\n",
      "W: [-0.99998826] b: [0.9999655] loss: 7.9292306e-10\n",
      "W: [-0.9999884] b: [0.9999659] loss: 7.768399e-10\n",
      "W: [-0.99998856] b: [0.9999663] loss: 7.5775475e-10\n",
      "W: [-0.9999887] b: [0.99996674] loss: 7.361507e-10\n",
      "W: [-0.9999888] b: [0.99996716] loss: 7.208065e-10\n",
      "W: [-0.9999889] b: [0.9999676] loss: 7.034515e-10\n",
      "W: [-0.9999891] b: [0.99996793] loss: 6.8415673e-10\n",
      "W: [-0.9999892] b: [0.9999683] loss: 6.7185013e-10\n",
      "W: [-0.99998933] b: [0.99996865] loss: 6.59373e-10\n",
      "W: [-0.99998945] b: [0.999969] loss: 6.419505e-10\n",
      "W: [-0.99998957] b: [0.99996936] loss: 6.2447114e-10\n",
      "W: [-0.9999897] b: [0.9999697] loss: 6.12733e-10\n",
      "W: [-0.9999898] b: [0.9999701] loss: 6.008243e-10\n",
      "W: [-0.9999899] b: [0.99997044] loss: 5.841976e-10\n",
      "W: [-0.99999005] b: [0.9999708] loss: 5.6751404e-10\n",
      "W: [-0.99999017] b: [0.99997115] loss: 5.563443e-10\n",
      "W: [-0.9999903] b: [0.9999715] loss: 5.4500404e-10\n",
      "W: [-0.9999904] b: [0.99997187] loss: 5.2917315e-10\n",
      "W: [-0.9999905] b: [0.9999722] loss: 5.132854e-10\n",
      "W: [-0.99999064] b: [0.9999726] loss: 5.026841e-10\n",
      "W: [-0.99999076] b: [0.9999729] loss: 4.8919446e-10\n",
      "W: [-0.9999909] b: [0.9999732] loss: 4.7835513e-10\n",
      "W: [-0.999991] b: [0.9999735] loss: 4.719283e-10\n",
      "W: [-0.9999911] b: [0.9999738] loss: 4.5917048e-10\n",
      "W: [-0.9999912] b: [0.9999741] loss: 4.4895287e-10\n",
      "W: [-0.9999913] b: [0.99997437] loss: 4.3664272e-10\n",
      "W: [-0.9999914] b: [0.99997467] loss: 4.285461e-10\n",
      "W: [-0.99999154] b: [0.99997497] loss: 4.181544e-10\n",
      "W: [-0.9999916] b: [0.99997526] loss: 4.0667203e-10\n",
      "W: [-0.9999917] b: [0.99997556] loss: 3.966143e-10\n",
      "W: [-0.9999918] b: [0.99997586] loss: 3.898073e-10\n",
      "W: [-0.9999919] b: [0.99997616] loss: 3.7971404e-10\n",
      "W: [-0.999992] b: [0.99997646] loss: 3.699121e-10\n",
      "W: [-0.9999921] b: [0.99997675] loss: 3.6330405e-10\n",
      "W: [-0.9999922] b: [0.99997705] loss: 3.5369396e-10\n",
      "W: [-0.9999923] b: [0.99997735] loss: 3.4264147e-10\n",
      "W: [-0.9999924] b: [0.99997765] loss: 3.3384495e-10\n",
      "W: [-0.9999925] b: [0.9999779] loss: 3.2637004e-10\n",
      "W: [-0.99999255] b: [0.9999782] loss: 3.2021674e-10\n",
      "W: [-0.99999267] b: [0.9999784] loss: 3.0907543e-10\n",
      "W: [-0.9999927] b: [0.99997866] loss: 3.0260594e-10\n",
      "W: [-0.99999285] b: [0.9999789] loss: 2.9903902e-10\n",
      "W: [-0.9999929] b: [0.99997914] loss: 2.9247715e-10\n",
      "W: [-0.99999297] b: [0.9999794] loss: 2.8600766e-10\n",
      "W: [-0.9999931] b: [0.9999796] loss: 2.7573321e-10\n",
      "W: [-0.99999315] b: [0.99997985] loss: 2.6939873e-10\n",
      "W: [-0.9999932] b: [0.9999801] loss: 2.6338398e-10\n",
      "W: [-0.9999933] b: [0.99998033] loss: 2.6004443e-10\n",
      "W: [-0.9999934] b: [0.99998057] loss: 2.5393732e-10\n",
      "W: [-0.99999344] b: [0.9999808] loss: 2.4792257e-10\n",
      "W: [-0.99999356] b: [0.99998105] loss: 2.3833024e-10\n",
      "W: [-0.9999936] b: [0.9999813] loss: 2.324505e-10\n",
      "W: [-0.9999937] b: [0.9999815] loss: 2.268905e-10\n",
      "W: [-0.9999938] b: [0.99998176] loss: 2.2377833e-10\n",
      "W: [-0.99999386] b: [0.999982] loss: 2.1812596e-10\n",
      "W: [-0.9999939] b: [0.99998224] loss: 2.1256596e-10\n",
      "W: [-0.99999404] b: [0.9999824] loss: 2.0631674e-10\n",
      "W: [-0.9999941] b: [0.99998266] loss: 1.995204e-10\n",
      "W: [-0.99999416] b: [0.9999829] loss: 1.9563018e-10\n",
      "W: [-0.9999942] b: [0.9999831] loss: 1.9166535e-10\n",
      "W: [-0.9999943] b: [0.99998325] loss: 1.8775737e-10\n",
      "W: [-0.9999944] b: [0.99998343] loss: 1.8181368e-10\n",
      "W: [-0.99999446] b: [0.99998367] loss: 1.7677237e-10\n",
      "W: [-0.9999945] b: [0.99998385] loss: 1.7414337e-10\n",
      "W: [-0.9999946] b: [0.999984] loss: 1.7037749e-10\n",
      "W: [-0.99999464] b: [0.9999842] loss: 1.6683899e-10\n",
      "W: [-0.9999947] b: [0.9999844] loss: 1.6426682e-10\n",
      "W: [-0.99999475] b: [0.99998456] loss: 1.5907986e-10\n",
      "W: [-0.9999948] b: [0.99998474] loss: 1.5548451e-10\n",
      "W: [-0.9999949] b: [0.9999849] loss: 1.5194601e-10\n",
      "W: [-0.99999493] b: [0.9999851] loss: 1.4710011e-10\n",
      "W: [-0.999995] b: [0.9999853] loss: 1.4469848e-10\n",
      "W: [-0.99999505] b: [0.99998546] loss: 1.4127366e-10\n",
      "W: [-0.9999951] b: [0.99998564] loss: 1.3807622e-10\n",
      "W: [-0.9999952] b: [0.9999858] loss: 1.3573143e-10\n",
      "W: [-0.99999523] b: [0.999986] loss: 1.3099921e-10\n",
      "W: [-0.9999953] b: [0.9999862] loss: 1.2774493e-10\n",
      "W: [-0.99999535] b: [0.99998635] loss: 1.2454748e-10\n",
      "W: [-0.9999954] b: [0.9999865] loss: 1.2015633e-10\n",
      "W: [-0.99999547] b: [0.9999867] loss: 1.1798207e-10\n",
      "W: [-0.9999955] b: [0.9999869] loss: 1.1489831e-10\n",
      "W: [-0.9999956] b: [0.99998707] loss: 1.1204193e-10\n",
      "W: [-0.99999565] b: [0.9999872] loss: 1.08641984e-10\n",
      "W: [-0.9999957] b: [0.99998736] loss: 1.06638254e-10\n",
      "W: [-0.99999577] b: [0.99998754] loss: 1.04662945e-10\n",
      "W: [-0.9999958] b: [0.9999877] loss: 9.961809e-11\n",
      "W: [-0.9999959] b: [0.9999879] loss: 9.681855e-11\n",
      "W: [-0.99999595] b: [0.999988] loss: 9.586998e-11\n",
      "W: [-0.999996] b: [0.99998814] loss: 9.2882146e-11\n",
      "W: [-0.999996] b: [0.9999883] loss: 9.125145e-11\n",
      "W: [-0.99999607] b: [0.99998844] loss: 9.040946e-11\n",
      "W: [-0.9999961] b: [0.99998856] loss: 8.744294e-11\n",
      "W: [-0.9999962] b: [0.9999887] loss: 8.462564e-11\n",
      "W: [-0.9999962] b: [0.9999888] loss: 8.3900886e-11\n",
      "W: [-0.99999624] b: [0.9999889] loss: 8.11049e-11\n",
      "W: [-0.99999624] b: [0.99998903] loss: 8.04512e-11\n",
      "W: [-0.9999963] b: [0.99998915] loss: 7.767653e-11\n",
      "W: [-0.99999636] b: [0.9999893] loss: 7.6870066e-11\n",
      "W: [-0.9999964] b: [0.9999894] loss: 7.598544e-11\n",
      "W: [-0.9999965] b: [0.9999895] loss: 7.3374196e-11\n",
      "W: [-0.9999965] b: [0.9999896] loss: 7.2692075e-11\n",
      "W: [-0.99999654] b: [0.99998975] loss: 7.015899e-11\n",
      "W: [-0.9999966] b: [0.99998987] loss: 6.766143e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999966] b: [0.99999] loss: 6.697931e-11\n",
      "W: [-0.99999666] b: [0.9999901] loss: 6.450307e-11\n",
      "W: [-0.99999666] b: [0.9999902] loss: 6.3892e-11\n",
      "W: [-0.9999967] b: [0.99999034] loss: 6.143708e-11\n",
      "W: [-0.9999967] b: [0.99999046] loss: 6.0897065e-11\n",
      "W: [-0.9999968] b: [0.9999906] loss: 5.8463456e-11\n",
      "W: [-0.99999684] b: [0.9999907] loss: 5.7770677e-11\n",
      "W: [-0.9999969] b: [0.9999908] loss: 5.6999738e-11\n",
      "W: [-0.99999696] b: [0.99999094] loss: 5.4729554e-11\n",
      "W: [-0.99999696] b: [0.99999106] loss: 5.416112e-11\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    sess.run(train, {x: x_train, y: y_train})\n",
    "    curr_W, curr_b, curr_loss = sess.run([W,b,loss],{x:x_train,y:y_train})\n",
    "    print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các toán tử tỏng tensorlow:\n",
    "* TensorBoard\n",
    "* Constant, Variable, Placeholder, Operations\n",
    "* Lazy loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>constant Đặc biệt</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tạo một tensor có kích thước là shape, và tất cả các phần tử đều có giá trị 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zeros:0' shape=(2, 3) dtype=int32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros([2, 3], tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tạo một tensor có kích thước và kiểu dữ liệu giống với input_tensor(trừ khi kiểu dữ liệu được chỉ định) và tất cả các phần tử đều có giá trị 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zeros_like:0' shape=(3, 2) dtype=int32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf.zeros_like([[0, 1], [2, 3], [4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ones:0' shape=(2, 3) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones([2, 3], dtype=tf.float32, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ones_like:0' shape=(3, 2) dtype=int32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones_like([[0, 1], [2, 3], [4, 5]], dtype=None, name=None, optimize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lazy Loading</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy loading là khái niệm mô tả việc chỉ tạo đối tượng khi cần và tạo nó ngay trong session thay vì phải định nghĩa và thêm vào graph trước khi chạy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết luận: Hạn chế sử dụng lazy loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 4.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 =  tf.constant([2,3,4], dtype=tf.float32)\n",
    "sess = tf.Session()\n",
    "v2.eval(session=sess) # thay cho sess.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.constant([3,2,3], dtype=tf.float32, name=\"v1\")\n",
    "v = v1 + v2\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "x = tf.add(a,b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
