{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nội dung chính:\n",
    "1. Xây dựng mô hình Word2Vec với Tensorflow <br/>\n",
    "    1.1 Load dữ liệu <br/>\n",
    "    1.2 Định nghĩa weights <br/>\n",
    "    1.3 Định nghĩa loss function <br/>\n",
    "    1.4 Định nghĩa optimizer <br/>\n",
    "    1.5 Thực thi <br/>\n",
    "2. Làm thế nào để xây dựng một mô hình Tensorflow <br/>\n",
    "3. Variable sharing <br/>\n",
    "    3.1 Name scope <br/>\n",
    "    3.2 Variable scope <br/>\n",
    "    3.3 Graph collections <br/>\n",
    "4. Kinh nghiệm quản lý mô hình với Tensorflow<br/>\n",
    "    4.1 tf.train.Saver <br/>\n",
    "    4.2 tf.summary <br/>\n",
    "    4.3 Kiểm soát ngẫu nhiên"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Xây dựng mô hình Word2Vec với Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở bài trước, chúng ta đã cùng tìm hiểu về các mô hình word embbeding phổ biến. Chúng ta quan tâm tới Word2Vec vì mô hình này không quá phức tạp nhưng độ hiệu quả cao. Trong phần này, chúng ta sẽ cùng nhau xây dựng mô hình Word2Vec bằng Tensorflow.\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "text8 là dataset khoảng 100MB text sạch được lấy từ Wikipedia Tiếng Anh và Mar, 3. 2016. Chúng ta sử dụng bộ data này để pre-processed bởi vì nó mục tiêu của chúng là là tập trung vào xử lý bằng Tensorflow chứ không phải là đi cleaned data nữa, clean data là một bước mất rất nhiều thời gian.\n",
    "\n",
    "Dataset tải tại: http://mattmahoney.net/dc/text8.zip\n",
    "\n",
    "100MB là không đủ để train một mô hình word embedding thực sự tốt, nhưng đủ để chúng ta xem được các mối quan hệ thú vị giữa các từ với nhau. Có 17 005 207 token nếu bạn đếm token bằng cách split text bởi khoảng trắng. \n",
    "\n",
    "Chúng ta xây dựng mô hình skip-gram word2vec với hàm loss là NCE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input là từ trung tâm, output là từ hàng xóm. Để feeding các từ vào model của chúng ta, chúng ta tạo một dictionary của các từ phổ biến nhất và sẽ feed các từ này. Ví dụ là từ trung tâm là từ thứ 1000th, nếu windown_size là 2 chúng ta có thể có từ hàng xóm là 998, 999, 1001, 1002.\n",
    "\n",
    "Mỗi mẫu input là một scalar, vì vậy BATCH_SIZE của các mẫu input có số chiều là [BATCH_SIZE]. Cũng giống như vậy, BATCH_SIZE của các mẫu output có chiều là [BATCH_SIZE, 1].\n",
    "\n",
    "Đầu tiên chúng ta đọc file nén và extract các từ trong file này ra thành 1 list, chúng ta gọi list đó là `words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "def read_words(pathFileZip):\n",
    "    with zipfile.ZipFile(pathFileZip) as f:\n",
    "        words = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return words\n",
    "words = read_words('./text8.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lúc này `words` sẽ là list danh sách các từ, `len(words) = 17005207`.\n",
    "\n",
    "Tiếp theo, chúng ta xây dựng dictionary, `dictionary` sẽ có key là các từ, value là các id chúng ta đánh số, `index_dictionary` sẽ ngược lại. Chúng ta lấy bộ từ vựng là 50000 từ phổ biến nhất, các từ mà không được đưa vào bộ từ vựng chúng ta sẽ gọi là các từ `UNK` và đánh id là `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocab_size = 50000 # kích cỡ từ điển\n",
    "def convert_to_dict(words):\n",
    "    dictionary = dict()\n",
    "    count = [('UNK', -1)]\n",
    "    index = 0\n",
    "    count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = index\n",
    "        index += 1\n",
    "    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, index_dictionary\n",
    "\n",
    "dictionary, index_dictionary = convert_to_dict(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, chúng ta đánh chỉ số cho các từ, những từ có trong dictionary sẽ đánh số theo id chúng được gán, ngược lại đánh số 0 cho các từ còn lại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_words = [dictionary[word] if word in dictionary else 0 for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thiết lập hàm sinh mẫu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "skip_window = 1\n",
    "def generate_sample(index_words, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n",
    "    for index, center in enumerate(index_words):\n",
    "        context = random.randint(1, context_window_size)\n",
    "        # Lấy ngẫu nhiên 1 từ mục tiêu trước từ trung tâm\n",
    "        for target in index_words[max(0, index - context): index]:\n",
    "            yield center, target\n",
    "        # Lấy ngẫu nhiên 1 từ mục tiêu sau từ trung tâm\n",
    "        for target in index_words[index + 1: index + context + 1]:\n",
    "            yield center, target\n",
    "            \n",
    "single_gen = generate_sample(index_words, skip_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lúc này `single_gen` sẽ là một generator, chứa các cặp (id1, id2) với id1 là từ trung tâm, id2 là từ hàng xóm ngẫu nhiên trong phạm vi skip_window.\n",
    "\n",
    "Tiếp theo chúng ta sẽ xây dựng hàm để tạo ra các batch cho việc train. Trả về center_batch là một array numpy chứa danh sách các từ trung tâm, và target_batch là một ndarray tương ứng với nhãn, tức là một từ hàng xóm ứng với từ trung tâm đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(batch_size):\n",
    "    while True:\n",
    "        center_batch= np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1])\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(single_gen)\n",
    "        yield center_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def gen():\n",
    "    yield from batch_gen(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(gen, (tf.int32, tf.int32), (tf.TensorShape([batch_size]), tf.TensorShape([batch_size, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lúc này dataset của chúng ta sẽ là một đối tượng tf.data, thay vì phải feed data và placeholder, thì chúng ta sẽ dùng tf.data để thay thế nhằm tăng tốc độ mô hình.\n",
    "\n",
    "Chúng ta tạo iterator cho data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-4bf054028433>:1: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_initializable_iterator()\n",
    "center_words, target_words = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Định nghĩa weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mỗi hàng cỉa `embed_matrix` tương ứng với vector biểu diễn một từ (V). Nếu một được biểu diễn với một vector kích thước `embed_size` (N), thì matrix embedding sẽ có số chiều là [V, N] (hay vocab_size, embed_size). Chúng ta sẽ khởi tạo ma trận nhúng cho giá trị là một phân phối ngẫu nhiên. Trong đây, chúng ta chọn phân phối đều."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "embed_matrix = tf.get_variable('embed_matrix',\n",
    "                                 shape=[vocab_size, embed_size],\n",
    "                                 initializer = tf.random_uniform_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mục tiêu của chúng ta là biểu diễn các vector từ trong dictionary của chúng ta. Nhớ rằng embed_matrix có số chiều là vocab_size X embed_size, với mỗi hàng của embedding matrix biểu diễn vector cho từ tại tương ứng index đó. vì vậy, để lấy biểu diễn của tất cả các từ trung tâm trong batch, chúng ta get slice của tất cả các hàng trong ma trận embedding, Tensorflow hỗ trợ một phương thức là embedding_lookup. Phương thức này hữu ích khi nó nhân với một one-hot vector. Hoạt đâu của nó như hình:\n",
    "\n",
    "<img src=\"./images/matrix_mult_w_one_hot.png\"/>\n",
    "\n",
    "Hay tóm lại, đây tương ứng là W' theo lý thuyết của chúng ta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Định nghĩa loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta sử dụng hàm `nce loss`, tensorflow có hỗ trợ phương thức này:\n",
    "\n",
    "`\n",
    "tf.nn.nce_loss(\n",
    "    weights,\n",
    "    biases,\n",
    "    labels,\n",
    "    inputs,\n",
    "    num_sampled,\n",
    "    num_classes,\n",
    "    num_true=1,\n",
    "    sampled_values=None,\n",
    "    remove_accidental_hits=False,\n",
    "    partition_strategy='mod',\n",
    "    name='nce_loss'\n",
    ")\n",
    "`\n",
    "\n",
    "Lưu ý rằng để hàm thực thi trong trường hợp của chúng ta thì đối số thứ 3 thực sự là inputs, còn thứ 4 là labels. Sự nhập nhằng này có vẻ khá phiền, nhưng tf1 còn khá mới và chưa hoàn thiện.\n",
    "\n",
    "Với nce loss, chúng ta cần weights và biases cho các layer ẩn để tính toán NCE loss. Chúng ta sẽ updated trong lúc training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sampled = 64\n",
    "nce_weight = tf.get_variable('nce_weight', shape=[vocab_size, embed_size],\n",
    "                        initializer=tf.truncated_normal_initializer(stddev=1.0 / (embed_size ** 0.5)))\n",
    "nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([vocab_size]))\n",
    "\n",
    "# define loss function to be NCE loss function\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                            biases=nce_bias, \n",
    "                                            labels=target_words, \n",
    "                                            inputs=embed, \n",
    "                                            num_sampled=num_sampled, \n",
    "                                            num_classes=vocab_size), name='loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Định nghĩa optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.0\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Thực thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4999:  65.3\n",
      "Average loss at step 9999:  18.4\n",
      "Average loss at step 14999:   9.5\n",
      "Average loss at step 19999:   6.7\n",
      "Average loss at step 24999:   5.7\n",
      "Average loss at step 29999:   5.3\n",
      "Average loss at step 34999:   5.0\n",
      "Average loss at step 39999:   4.9\n",
      "Average loss at step 44999:   4.8\n",
      "Average loss at step 49999:   4.8\n",
      "Average loss at step 54999:   4.8\n",
      "Average loss at step 59999:   4.7\n",
      "Average loss at step 64999:   4.7\n",
      "Average loss at step 69999:   4.7\n",
      "Average loss at step 74999:   4.7\n",
      "Average loss at step 79999:   4.7\n",
      "Average loss at step 84999:   4.7\n",
      "Average loss at step 89999:   4.6\n",
      "Average loss at step 94999:   4.6\n",
      "Average loss at step 99999:   4.6\n"
     ]
    }
   ],
   "source": [
    "num_train_steps = 100000\n",
    "skip_step = 5000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    total_loss = 0\n",
    "    for index in range(num_train_steps):\n",
    "        try:\n",
    "            loss_batch, _ = sess.run([loss, optimizer])\n",
    "            total_loss += loss_batch\n",
    "            if (index+1) % skip_step == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss/skip_step))\n",
    "                total_loss = 0\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            sess.run(iterator.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Làm thế nào để xây dựng một mô hình Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tất cả các mô hình chúng ta xây dựng đến nay đều có cấu trúc giống nhau:\n",
    "\n",
    "**Phần 1: Xây dựng đồ thị**\n",
    "1. Import data (với tf.data hoặc placeholder)\n",
    "2. Định nghĩa weights\n",
    "3. Định nghĩa inference model\n",
    "4. Định nghĩa loss function\n",
    "5. Đinh nghĩa optimizer\n",
    "\n",
    "**Phần 2: Thực thi tính toán**\n",
    "1. Khởi tạo tất cả các biến\n",
    "2. Khởi tạo interator (tf.data) hoặc feed data (placeholder)\n",
    "3. Thực thi train model\n",
    "4. Tính toán cost\n",
    "5. Điều chỉnh tham số để cực tiểu/cực đại cost\n",
    "\n",
    "<img src=\"./images/model.png\"/>\n",
    "\n",
    "Chúng ta đã mất khá ngắn các dòng mã để xây dựng mô hình word2vec, nhưng xa hơn, chúng ta cần tái sử dụng. Vậy làm thế nào để chúng ta thực hiện tái sử dụng, thì cách giải quyết đó là chúng ta xây dựng model như một class.\n",
    "\n",
    "Model của chúng ta nên được thiết lập như dưới, chúng ta gộp bước 3 và bước 4 bởi vì muốn đặt chúng vào name scope \"NCE loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SkipGramModel:\n",
    "    \"\"\" Build the graph for word2vec model \"\"\"\n",
    "    def __init__(self, params):\n",
    "        pass\n",
    "\n",
    "    def _import_data(self):\n",
    "        \"\"\" Step 1: import data \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _create_embedding(self):\n",
    "        \"\"\" Step 2: in word2vec, it's actually the weights that we care about \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _create_loss(self):\n",
    "        \"\"\" Step 3 + 4: define the inference + the loss function \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        \"\"\" Step 5: define optimizer \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các bạn hãy tự hoàn thành class trên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Variable sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Name scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như chúng ta đã từng xuất graph Tensorflow, chúng ta sẽ thấy các nút được phân tán khắp nơi trên đồ thị, từ đó rất khó đọc. Chúng ta có thể nhóm các nút lại với nhau thành từng nhóm dựa vào name scope, điều này có thể giúp giảm khó khăn khi chúng ta thực hiện với hàng trăm operator.\n",
    "\n",
    "`\n",
    "with tf.name_scope(name_of_that_scope):\n",
    "    # do something\n",
    "`\n",
    "Ví dụ đồ thị của chúng ta có thể có 4 name scope: \"data\", \"embed\", \"loss\", \"optimizer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('data'):\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    center_words, target_words = iterator.get_next()\n",
    "\n",
    "with tf.name_scope('embed'):\n",
    "    embed_matrix = tf.get_variable('embed_matrix', \n",
    "                                    shape=[vocab_size, embed_size],\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "    embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embedding')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    nce_weight = tf.get_variable('nce_weight', shape=[vocab_size, embed_size],\n",
    "                                initializer=tf.truncated_normal_initializer())\n",
    "    nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                        biases=nce_bias, \n",
    "                                        labels=target_words, \n",
    "                                        inputs=embed, \n",
    "                                        num_sampled=num_sampled, \n",
    "                                        num_classes=vocab_size), name='loss')\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi hiển thị trên TensorBoard, kết quả như sau:\n",
    "\n",
    "<img src=\"./images/namescope.png\"/>\n",
    "\n",
    "Bạn có thể ấn dấu cộng trên đầu mỗi khối để xem tất cả các operator của khối đó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Variable scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một trong những câu hỏi tuyển dụng phổ biến có thể là namescope khác gì so với variable socpe. Cả 2 đều tạo namespace, vấn đề chính là variable_scope để chia sẻ biến. Hãy cùng khám phá lý do tại sao chúng ta cần chia sẻ biến.\n",
    "\n",
    "Giả sử rằng chúng ta muốn tạo một mạng neural với 2 layer ẩn. chúng ta gọi 2 layer ẩn với 2 input x1 và x2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.truncated_normal([200, 100], name='x1')\n",
    "x2 = tf.truncated_normal([200, 100], name='x2')\n",
    "\n",
    "def two_hidden_layers(x):\n",
    "    assert x.shape.as_list() == [200, 100]\n",
    "    w1 = tf.Variable(tf.random_normal([100, 50]), name=\"h1_weights\")\n",
    "    b1 = tf.Variable(tf.zeros([50]), name=\"h1_biases\")\n",
    "    h1 = tf.matmul(x, w1) + b1\n",
    "    assert h1.shape.as_list() == [200, 50]  \n",
    "    w2 = tf.Variable(tf.random_normal([50, 10]), name=\"h2_weights\")\n",
    "    b2 = tf.Variable(tf.zeros([10]), name=\"h2_biases\")\n",
    "    logits = tf.matmul(h1, w2) + b2\n",
    "    return logits\n",
    "\n",
    "logits1 = two_hidden_layers(x1)\n",
    "logits2 = two_hidden_layers(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu hiển thị trên TensorBoard sẽ được kết quả như sau:\n",
    "\n",
    "<img src=\"./images/2layer.png\"/>\n",
    "\n",
    "Mỗi lần bạn gọi 2 mạng, Tensorflow tạo một tập các variable khác nhau, trong khi đó, bạn muốn mạng dùng chung biến cho tất cả input. Để làm điều này thì chúng ta cần sử dụng `tf.get_variable()`. Khi chúng ta tạo một variable với `tf.get_variable()`, thì đầu tiên hàm này sẽ check xem variable tồn tại hay không. Nếu không thì tạo nó, nếu có rồi thì dùng lại. Chúng ta thay thế hàm trên lại như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_hidden_layers_2(x):\n",
    "    assert x.shape.as_list() == [200, 100]\n",
    "    w1 = tf.get_variable(\"h1_weights\", [100, 50], initializer=tf.random_normal_initializer())\n",
    "    b1 = tf.get_variable(\"h1_biases\", [50], initializer=tf.constant_initializer(0.0))\n",
    "    h1 = tf.matmul(x, w1) + b1\n",
    "    assert h1.shape.as_list() == [200, 50]  \n",
    "    w2 = tf.get_variable(\"h2_weights\", [50, 10], initializer=tf.random_normal_initializer())\n",
    "    b2 = tf.get_variable(\"h2_biases\", [10], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(h1, w2) + b2\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu chạy thì sẽ báo lỗi: `ValueError: Variable h1_weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?`\n",
    "\n",
    "Để tránh điều này, chúng ta cần đặt tất cả các biến chúng ta muốn tạo sử dụng trong varscope để thiết lập tái sử dụng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(x, output_dim, scope):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        w = tf.get_variable(\"weights\", [x.shape[1], output_dim], initializer=tf.random_normal_initializer())\n",
    "        b = tf.get_variable(\"biases\", [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "        return tf.matmul(x, w) + b\n",
    "\n",
    "def two_hidden_layers(x):\n",
    "    h1 = fully_connected(x, 50, 'h1')\n",
    "    h2 = fully_connected(h1, 10, 'h2')\n",
    "\n",
    "with tf.variable_scope('two_layers') as scope:\n",
    "    logits1 = two_hidden_layers(x1)\n",
    "    scope.reuse_variables()\n",
    "    logits2 = two_hidden_layers(x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả trên Tensorboard:\n",
    "\n",
    "<img src=\"./images/varscope.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Graph collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi bạn tạo một mô hình, bạn phải đặt các biến của bạn vào các phần của đồ thị. Đôi khi, bạn muốn dễ dàng truy cập chúng. `tf.get_collection` cho phép bạn truy cập vào một tập xác định các biến, với key là tên của collection, scope là scope của các biến.\n",
    "\n",
    "`\n",
    "tf.get_collection(\n",
    "    key,\n",
    "    scope=None\n",
    ")\n",
    "`\n",
    "\n",
    "Mặc định, tất cả các biến ở trong `tf.GraphKeys.GLOBAL_VARIABLES`. Để get tất cả các biến trong scope \"my_scope\". Ta làm như sau:\n",
    "\n",
    "`\n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='my_scope')\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'two_layers/h1/weights:0' shape=(100, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'two_layers/h1/biases:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'two_layers/h2/weights:0' shape=(50, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'two_layers/h2/biases:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='two_layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Kinh nghiệm quản lý mô hình với Tensorflow\n",
    "## 4.1 tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thực tiễn chúng ta cần kiểm định lại các tham số của mô hình sau khi qua một số epochs. Vì vậy, chúng ta có thể restore/retrain mô hình của chúng ta từ bước nào đó nếu cần thiết. Class `tf.train.Saver()` cho phép chúng ta lưu lại các variable của đồ thị trong file nhị phân.\n",
    "\n",
    "`\n",
    "tf.train.Saver.save(\n",
    "    sess,\n",
    "    save_path,\n",
    "    global_step=None,\n",
    "    latest_filename=None,\n",
    "    meta_graph_suffix='meta',\n",
    "    write_meta_graph=True,\n",
    "    write_state=True\n",
    ")\n",
    "`\n",
    "\n",
    "Ví dụ, nếu chúng ta muốn lưu các biến của đồ thị sau mỗi 1000 bước training, chúng ta làm như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "# create a saver object\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# launch a session to execute the computation\n",
    "with tf.Session() as sess:\n",
    "    # actual training loop\n",
    "    for step in range(training_steps): \n",
    "        sess.run([optimizer])\n",
    "        if (step + 1) % 1000 == 0:\n",
    "            saver.save(sess, 'checkpoint_directory/model_name', global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong thuật ngữ Tensorflow, các bước mà bạn lưu các biến của grahp được gọi là một checkpoint. Từ đó, chúng ta sẽ tạo nhiều checkpoint, nó rất hữu ích, chúng ta thêm số bước train của mô hình vào một biến gọi là `global_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta cần cho global_step như một tham số tối ưu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss,global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để lưu giá trị hiện tại của các biến trong thư mục checkpoints, chúng ta làm như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.save(sess, 'checkpoints/model-name', global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để khôi phục lại các biến, chúng ta sử dụng `tf.train.Saver.restore(sess, save_path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess, 'checkpoints/skip-gram-10000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhưng tất nhiên, chúng ta chỉ có thể load các biến đã lưu nếu checkpoint hợp lệ. Những gì bạn có thể làm là nếu tồn tại checkpoint, khôi phục (restore) nó. Nó không tồn tại, train từ đầu. Tensorflow cho phép bạn get checkpoint từ một thư mục với `tf.train.get_checkpoint_state('directory-name')`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "     saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint tự động theo dõi checkpoint mới nhất trong thư mục."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc định, `saver.save()` lưu tất cả các biến của đồ thị, và điều này được khuyến nghị. Tuy nhiên, bạn có thể chỉ chọn một số biến để lưu trữ chúng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = tf.Variable(..., name='v1') \n",
    "v2 = tf.Variable(..., name='v2') \n",
    "\n",
    "# pass the variables as a dict: \n",
    "saver = tf.train.Saver({'v1': v1, 'v2': v2}) \n",
    "\n",
    "# pass them as a list\n",
    "saver = tf.train.Saver([v1, v2]) \n",
    "\n",
    "# passing a list is equivalent to passing a dict with the variable op names # as keys\n",
    "saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 tf.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể sử dụng matplotlib để mô phỏng loss và độ chính xác, tuy nhiên TensorBoard hỗ trợ công cụ để chúng ta thống kê mô phòng loss, average loss, accuracy tuyệt vời hơn, Bạn có thể mô phỏng dưới dạng scalar plot, histogram, hoặc even images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_summaries(self):\n",
    "     with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.scalar(\"accuracy\", self.accuracy)            \n",
    "            tf.summary.histogram(\"histogram loss\", self.loss)\n",
    "            # because you have several summaries, we should merge them all\n",
    "            # into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], \n",
    "                                  feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ bạn đã nhận được summary, bạn cần ghi summary vào file giống như FileWrite mà chúng ta đã sử dụng để tạo đồ thị, chúng ta lưu summary như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_summary(summary, global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chạy Tensorboard, trong page Scalars, bạn sẽ thấy các đồ thị summary.\n",
    "\n",
    "<img src=\"./images/sum1.png\"/>\n",
    "\n",
    "<img src=\"./images/sum2.png\"/>\n",
    "\n",
    "Nếu bạn lưu summary khác nhau, bạn có thể so sánh chúng, ví dụ lần đầu chạy với learning rate là 1.0, lưu vào `improved_graph/lr1.0`, lần 2 learning rate là 0.5 lưu vào `improved_graph/lr0.5` .chúng ta có thể so sánh 2 lần lần.\n",
    "\n",
    "<img src=\"./images/sum3.png\"/>\n",
    "\n",
    "Bạn có thể mô phỏng các thống kê như ảnh sử dụng: `tf.summary.image`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Kiểm soát ngẫu nhiên"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cách 1: Sử dụng seed cho ngẫu nhiên mức operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.random_uniform([], -10, 10, seed=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c)) # >> 3.57493\n",
    "    print(sess.run(c)) # >> -5.97319"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.random_uniform([], -10, 10, seed=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c)) # >> 3.57493\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c)) # >> 3.57493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.random_uniform([], -10, 10, seed=2)\n",
    "d = tf.random_uniform([], -10, 10, seed=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(c) # >> 3.57493\n",
    "    print sess.run(d) # >> 3.57493"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cách 2: Sử dụng `tf.set_random_seed(sees)` để ngẫu nhiên mức đồ thị."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu bạn không quan tâm đến ngẫu nhiên cho mỗi toán tử trong đồ thị, nhưng bạn muốn kết quả lặp lại trên đồ thị khác, bạn có thể sử dụng `tf.set_random_seed`.\n",
    "\n",
    "Ví dụ chúng ta có 2 mô hình `a.py` và `b.py` có mã giống hệt nhau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(2)\n",
    "c = tf.random_uniform([], -10, 10)\n",
    "d = tf.random_uniform([], -10, 10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu không có seed level graph thì chạy a và b sẽ ra kết quả khác nhau, nhưng với tf.set_random_seed, ta có 2 kêt quả hoàn toàn giống hệt nhau:\n",
    "\n",
    "\n",
    "python a.py <br/>\n",
    "> -4.00752 <br/>\n",
    "> -2.98339<br/>\n",
    "\n",
    "python b.py <br/>\n",
    "> -4.00752<br/>\n",
    "> -2.98339<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tham khảo thêm:**\n",
    "1. <a href=\"https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/4_Utils/save_restore_model.ipynb\">Save and Store a model</a><br/>\n",
    "2. <a href=\"https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/4_Utils/tensorboard_basic.ipynb\">Tensorboard - Graph and loss visualization</a><br/>\n",
    "3. <a href=\"https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/4_Utils/tensorboard_advanced.ipynb\">Tensorboard - Advanced visualization</a><br/>\n",
    "4. <a href=\"https://github.com/chiphuyen/stanford-tensorflow-tutorials/blob/master/examples/04_word2vec_visualize.py\">Source code Word2Vec hoàn chỉnh</a><br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
