{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nội dung chính:\n",
    "1. Xây dựng mạng CNN với Tensorflow\n",
    "2. Bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nguồn tham khảo:\n",
    "1. http://cs20si.stanford.edu/\n",
    "2. https://www.kaggle.com/phamdinhkhanh/convolutional-neural-network/\n",
    "3. https://github.com/aymericdamien/TensorFlow-Examples\n",
    "4. https://github.com/chiphuyen/stanford-tensorflow-tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Xây dựng mạng CNN với Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bên dưới ta sẽ tiến hành xây dựng một mạng CNN phân biệt chữ số viết tay. Dataset bao gồm 60000 mẫu training và 10000 mẫu testing. Các hình ảnh được chuẩn hoá kích thước 28x28 với giá trị từ 0 tới 1.\n",
    "\n",
    "Xem thêm: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ảnh đen trắng nên thiết lập số chiều cuối là 1\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# chuyển y_train về dạng one-hot vector\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "\n",
    "# chuyển y_test về dạng one-hot vector\n",
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-7c2408dd8b02>:8: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(dataset)`.\n",
      "WARNING:tensorflow:From <ipython-input-5-7c2408dd8b02>:9: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py:347: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py:348: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py:350: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n"
     ]
    }
   ],
   "source": [
    "#X = tf.placeholder(tf.float32, [None, 28, 28, 1]) # kích cỡ ảnh 28x28x1\n",
    "#Y = tf.placeholder(tf.float32, [None, 10]) # có 10 lớp đầu ra\n",
    "\n",
    "# Tạo dataset\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.shuffle(10000) # xoá trộn data\n",
    "train_data = train_data.batch(batch_size) # minibatch = 256\n",
    "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                           train_data.output_shapes)\n",
    "X, Y = iterator.get_next()\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_data = test_data.shuffle(10000) # xoá trộn data\n",
    "test_data = test_data.batch(batch_size) # minibatch = 256\n",
    "\n",
    "train_init = iterator.make_initializer(train_data)\n",
    "test_init = iterator.make_initializer(test_data)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) # droupout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-1c539586de5e>:28: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-6-1c539586de5e>:59: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tạo một số hàm đơn giản\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "# tạo model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # convolution layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # max pooling \n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    \n",
    "    # convolution layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # apply dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    \n",
    "    # ouput class\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    \n",
    "    return out\n",
    "    \n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, 10]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}\n",
    "\n",
    "# Xây dựng model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# định nghĩa loss và optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss = 18086862.0, Accuracy = 0.078125\n",
      "Step 50, Minibatch Loss = 1205086.25, Accuracy = 0.765625\n",
      "Step 100, Minibatch Loss = 507255.46875, Accuracy = 0.890625\n",
      "Step 150, Minibatch Loss = 396804.125, Accuracy = 0.90625\n",
      "Step 200, Minibatch Loss = 485988.5, Accuracy = 0.890625\n",
      "Step 250, Minibatch Loss = 71291.8125, Accuracy = 0.953125\n",
      "Step 300, Minibatch Loss = 198825.359375, Accuracy = 0.9453125\n",
      "Step 350, Minibatch Loss = 202802.140625, Accuracy = 0.953125\n",
      "Step 400, Minibatch Loss = 92994.28125, Accuracy = 0.9609375\n",
      "Step 450, Minibatch Loss = 73372.609375, Accuracy = 0.9765625\n",
      "Step 500, Minibatch Loss = 95527.1875, Accuracy = 0.96875\n",
      "Optimization finished!\n",
      "Test accuracy:  0.953125\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1, num_steps+1):\n",
    "        sess.run(train_init)\n",
    "        sess.run(train_op, feed_dict={keep_prob: 0.5})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={keep_prob: 1.0})\n",
    "            print(\"Step {}, Minibatch Loss = {}, Accuracy = {}\".format(step, loss, acc))\n",
    "    print(\"Optimization finished!\")\n",
    "    sess.run(test_init)\n",
    "    print(\"Test accuracy: \", sess.run(accuracy, feed_dict={keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 1:** Hình dưới là mô hình nhận diện chữ số MNIST, K: số kernel, S: stride, P: padding. Tính số\n",
    "lượng parameter ở layer và output tương ứng (Tìm a, b, c, d, e, f, g, h, i).\n",
    "\n",
    "<img src=\"./images/bai1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 2:** Xây dựng model CNN cho bài toán phân loại ảnh với dữ liệu CIFAR10 dataset bao gồm\n",
    "50,000 training set và 10.000 test set ảnh màu kích thước 32x32 cho 10 thể loại khác nhau\n",
    "(máy bay, ô tô, thuyền, chim, chó, mèo, ngựa,...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 85s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load dữ liệu cifar10\n",
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 3:** Xử lý bài toán phân loại chó mèo:\n",
    "\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 4:** Hoàn thiện và chỉnh sửa code dưới đây sao cho phù hợp với bài toán MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "\n",
    "def conv_relu(inputs, filters, k_size, stride, padding, scope_name):\n",
    "    '''\n",
    "    A method that does convolution + relu on inputs\n",
    "    '''\n",
    "    #############################\n",
    "    ########## TO DO ############\n",
    "    #############################\n",
    "    return None\n",
    "\n",
    "def maxpool(inputs, ksize, stride, padding='VALID', scope_name='pool'):\n",
    "    '''A method that does max pooling on inputs'''\n",
    "    #############################\n",
    "    ########## TO DO ############\n",
    "    #############################\n",
    "    return None\n",
    "\n",
    "def fully_connected(inputs, out_dim, scope_name='fc'):\n",
    "    '''\n",
    "    A fully connected linear layer on inputs\n",
    "    '''\n",
    "    #############################\n",
    "    ########## TO DO ############\n",
    "    #############################\n",
    "    return None\n",
    "\n",
    "class ConvNet(object):\n",
    "    def __init__(self):\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 128\n",
    "        self.keep_prob = tf.constant(0.75)\n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32, \n",
    "                                trainable=False, name='global_step')\n",
    "        self.n_classes = 10\n",
    "        self.skip_step = 20\n",
    "        self.n_test = 10000\n",
    "\n",
    "    def get_data(self):\n",
    "        with tf.name_scope('data'):\n",
    "            train_data, test_data = utils.get_mnist_dataset(self.batch_size)\n",
    "            iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                                   train_data.output_shapes)\n",
    "            img, self.label = iterator.get_next()\n",
    "            self.img = tf.reshape(img, shape=[-1, 28, 28, 1])\n",
    "            # reshape the image to make it work with tf.nn.conv2d\n",
    "\n",
    "            self.train_init = iterator.make_initializer(train_data)  # initializer for train_data\n",
    "            self.test_init = iterator.make_initializer(test_data)    # initializer for train_data\n",
    "\n",
    "    def inference(self):\n",
    "        '''\n",
    "        Build the model according to the description we've shown in class\n",
    "        '''\n",
    "        #############################\n",
    "        ########## TO DO ############\n",
    "        #############################\n",
    "        self.logits = None\n",
    "\n",
    "    def loss(self):\n",
    "        '''\n",
    "        define loss function\n",
    "        use softmax cross entropy with logits as the loss function\n",
    "        tf.nn.softmax_cross_entropy_with_logits\n",
    "        softmax is applied internally\n",
    "        don't forget to compute mean cross all sample in a batch\n",
    "        '''\n",
    "        #############################\n",
    "        ########## TO DO ############\n",
    "        #############################\n",
    "        self.loss = None\n",
    "    \n",
    "    def optimize(self):\n",
    "        '''\n",
    "        Define training op\n",
    "        using Adam Gradient Descent to minimize cost\n",
    "        Don't forget to use global step\n",
    "        '''\n",
    "        #############################\n",
    "        ########## TO DO ############\n",
    "        #############################\n",
    "        self.opt = None\n",
    "\n",
    "    def summary(self):\n",
    "        '''\n",
    "        Create summaries to write on TensorBoard\n",
    "        Remember to track both training loss and test accuracy\n",
    "        '''\n",
    "        #############################\n",
    "        ########## TO DO ############\n",
    "        #############################\n",
    "        self.summary_op = None\n",
    "        \n",
    "    def eval(self):\n",
    "        '''\n",
    "        Count the number of right predictions in a batch\n",
    "        '''\n",
    "        with tf.name_scope('predict'):\n",
    "            preds = tf.nn.softmax(self.logits)\n",
    "            correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.label, 1))\n",
    "            self.accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "    def build(self):\n",
    "        '''\n",
    "        Build the computation graph\n",
    "        '''\n",
    "        self.get_data()\n",
    "        self.inference()\n",
    "        self.loss()\n",
    "        self.optimize()\n",
    "        self.eval()\n",
    "        self.summary()\n",
    "\n",
    "    def train_one_epoch(self, sess, saver, init, writer, epoch, step):\n",
    "        start_time = time.time()\n",
    "        sess.run(init) \n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        try:\n",
    "            while True:\n",
    "                _, l, summaries = sess.run([self.opt, self.loss, self.summary_op])\n",
    "                writer.add_summary(summaries, global_step=step)\n",
    "                if (step + 1) % self.skip_step == 0:\n",
    "                    print('Loss at step {0}: {1}'.format(step, l))\n",
    "                step += 1\n",
    "                total_loss += l\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        saver.save(sess, 'checkpoints/convnet_starter/mnist-convnet', step)\n",
    "        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        return step\n",
    "\n",
    "    def eval_once(self, sess, init, writer, epoch, step):\n",
    "        start_time = time.time()\n",
    "        sess.run(init)\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch, summaries = sess.run([self.accuracy, self.summary_op])\n",
    "                writer.add_summary(summaries, global_step=step)\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('Accuracy at epoch {0}: {1} '.format(epoch, total_correct_preds/self.n_test))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        '''\n",
    "        The train function alternates between training one epoch and evaluating\n",
    "        '''\n",
    "        utils.safe_mkdir('checkpoints')\n",
    "        utils.safe_mkdir('checkpoints/convnet_starter')\n",
    "        writer = tf.summary.FileWriter('./graphs/convnet_starter', tf.get_default_graph())\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_starter/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            step = self.gstep.eval()\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                step = self.train_one_epoch(sess, saver, self.train_init, writer, epoch, step)\n",
    "                self.eval_once(sess, self.test_init, writer, epoch, step)\n",
    "        writer.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = ConvNet()\n",
    "    model.build()\n",
    "    model.train(n_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài 5:** Tham khảo code build CNN với tf layer sau, chỉnh sửa để code chạy được."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "class ConvNet(object):\n",
    "    def __init__(self):\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 128\n",
    "        self.keep_prob = tf.constant(0.75)\n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32, \n",
    "                                trainable=False, name='global_step')\n",
    "        self.n_classes = 10\n",
    "        self.skip_step = 20\n",
    "        self.n_test = 10000\n",
    "        self.training=False\n",
    "\n",
    "    def get_data(self):\n",
    "        with tf.name_scope('data'):\n",
    "            train_data, test_data = utils.get_mnist_dataset(self.batch_size)\n",
    "            iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                                   train_data.output_shapes)\n",
    "            img, self.label = iterator.get_next()\n",
    "            self.img = tf.reshape(img, shape=[-1, 28, 28, 1])\n",
    "            # reshape the image to make it work with tf.nn.conv2d\n",
    "\n",
    "            self.train_init = iterator.make_initializer(train_data)  # initializer for train_data\n",
    "            self.test_init = iterator.make_initializer(test_data)    # initializer for train_data\n",
    "\n",
    "    def inference(self):\n",
    "        conv1 = tf.layers.conv2d(inputs=self.img,\n",
    "                                  filters=32,\n",
    "                                  kernel_size=[5, 5],\n",
    "                                  padding='SAME',\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  name='conv1')\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                        pool_size=[2, 2], \n",
    "                                        strides=2,\n",
    "                                        name='pool1')\n",
    "\n",
    "        conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "                                  filters=64,\n",
    "                                  kernel_size=[5, 5],\n",
    "                                  padding='SAME',\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  name='conv2')\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                        pool_size=[2, 2], \n",
    "                                        strides=2,\n",
    "                                        name='pool2')\n",
    "\n",
    "        feature_dim = pool2.shape[1] * pool2.shape[2] * pool2.shape[3]\n",
    "        pool2 = tf.reshape(pool2, [-1, feature_dim])\n",
    "        fc = tf.layers.dense(pool2, 1024, activation=tf.nn.relu, name='fc')\n",
    "        dropout = tf.layers.dropout(fc, \n",
    "                                    self.keep_prob, \n",
    "                                    training=self.training, \n",
    "                                    name='dropout')\n",
    "        self.logits = tf.layers.dense(dropout, self.n_classes, name='logits')\n",
    "\n",
    "    def loss(self):\n",
    "        '''\n",
    "        define loss function\n",
    "        use softmax cross entropy with logits as the loss function\n",
    "        compute mean cross entropy, softmax is applied internally\n",
    "        '''\n",
    "        # \n",
    "        with tf.name_scope('loss'):\n",
    "            entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.label, logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(entropy, name='loss')\n",
    "    \n",
    "    def optimize(self):\n",
    "        '''\n",
    "        Define training op\n",
    "        using Adam Gradient Descent to minimize cost\n",
    "        '''\n",
    "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, \n",
    "                                                global_step=self.gstep)\n",
    "\n",
    "    def summary(self):\n",
    "        '''\n",
    "        Create summaries to write on TensorBoard\n",
    "        '''\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "            tf.summary.histogram('histogram loss', self.loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    def eval(self):\n",
    "        '''\n",
    "        Count the number of right predictions in a batch\n",
    "        '''\n",
    "        with tf.name_scope('predict'):\n",
    "            preds = tf.nn.softmax(self.logits)\n",
    "            correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.label, 1))\n",
    "            self.accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "    def build(self):\n",
    "        '''\n",
    "        Build the computation graph\n",
    "        '''\n",
    "        self.get_data()\n",
    "        self.inference()\n",
    "        self.loss()\n",
    "        self.optimize()\n",
    "        self.eval()\n",
    "        self.summary()\n",
    "\n",
    "    def train_one_epoch(self, sess, saver, init, writer, epoch, step):\n",
    "        start_time = time.time()\n",
    "        sess.run(init) \n",
    "        self.training = True\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        try:\n",
    "            while True:\n",
    "                _, l, summaries = sess.run([self.opt, self.loss, self.summary_op])\n",
    "                writer.add_summary(summaries, global_step=step)\n",
    "                if (step + 1) % self.skip_step == 0:\n",
    "                    print('Loss at step {0}: {1}'.format(step, l))\n",
    "                step += 1\n",
    "                total_loss += l\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        saver.save(sess, 'checkpoints/convnet_layers/mnist-convnet', step)\n",
    "        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        return step\n",
    "\n",
    "    def eval_once(self, sess, init, writer, epoch, step):\n",
    "        start_time = time.time()\n",
    "        sess.run(init)\n",
    "        self.training = False\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch, summaries = sess.run([self.accuracy, self.summary_op])\n",
    "                writer.add_summary(summaries, global_step=step)\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('Accuracy at epoch {0}: {1} '.format(epoch, total_correct_preds/self.n_test))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        '''\n",
    "        The train function alternates between training one epoch and evaluating\n",
    "        '''\n",
    "        utils.safe_mkdir('checkpoints')\n",
    "        utils.safe_mkdir('checkpoints/convnet_layers')\n",
    "        writer = tf.summary.FileWriter('./graphs/convnet_layers', tf.get_default_graph())\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_layers/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            step = self.gstep.eval()\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                step = self.train_one_epoch(sess, saver, self.train_init, writer, epoch, step)\n",
    "                self.eval_once(sess, self.test_init, writer, epoch, step)\n",
    "        writer.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = ConvNet()\n",
    "    model.build()\n",
    "    model.train(n_epochs=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
