{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nội dung chính\n",
    "1. Lý thuyết mạng RNN\n",
    "2. Xây dựng mạng RNN bằng Python\n",
    "3. Xây dựng mạng RNN bằng Keras\n",
    "\n",
    "Nguồn:\n",
    "- https://nttuan8.com/bai-13-recurrent-neural-network/\n",
    "- http://web.stanford.edu/class/cs20si/syllabus.html\n",
    "- https://dominhhai.github.io/vi/2017/10/implement-rnn-with-python/\n",
    "- https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb\n",
    "* https://dominhhai.github.io/vi/2017/10/what-is-rnn/\n",
    "* https://github.com/dennybritz/rnn-tutorial-rnnlm/blob/master/RNNLM.ipynb\n",
    "* https://www.tensorflow.org/tutorials/text/text_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lý thuyết mạng RNN\n",
    "Trong các bài trước, chúng ta đã cùng tìm hiểu về mạng neural, nó đem lại những tính chất dường như là vi diệu, tuy nhiên, các mô hình này có vẻ vẫn còn hạn chế. Các input chúng ta đưa vào hoàn toàn độc lập với nhau. Trên thực tế, rất nhiều dữ liệu cũng như sự vật trong cuộc sống có mối tương quan chứ không hề độc lập với nhau. Ví như như 1 video bao gồm nhiều frame nối tiếp nhau, âm nhạc cũng là một dạng chuỗi tuần tự âm thanh, các text xử lý cũng là dạng chuỗi tuần tự... Câu hỏi đặt ra là vậy làm thế nào chúng ta có thể làm cho mô hình của chúng ta có khả năng xử lý chuỗi theo như cách làm của con người.\n",
    "\n",
    "RNN được tạo ra với mục đích nắm bắt được thông tin dạng chuỗi. Simple Recurrent Network (SRN) được giới thiệu lần đầu bởi Jeff Elman trong paper \"Finding structure in time\" (Elman, 1990). RNN được gọi là hồi quy (Recurrent) bởi lẽ chúng thực hiện cùng một tác vụ cho tất cả các phần tử của một chuỗi với đầu ra phụ thuộc vào cả các phép tính trước đó. Nói cách khác, RNN có khả năng nhớ các thông tin được tính toán trước đó. Trên lý thuyết, RNN có thể sử dụng được thông tin của một văn bản rất dài, tuy nhiên thực tế thì nó chỉ có thể nhớ được một vài bước trước đó (ta cùng bàn cụ thể vấn đề này sau) mà thôi. Về cơ bản một mạng RNN có dạng như sau:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](./images/rnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô hình trên mô tả phép triển khai nội dung của một RNN. Triển khai ở đây có thể hiểu đơn giản là ta vẽ ra một mạng nơ-ron chuỗi tuần tự. Ví dụ ta có một câu gồm 5 chữ “Đẹp trai lắm gái theo”, thì mạng nơ-ron được triển khai sẽ gồm 5 tầng nơ-ron tương ứng với mỗi chữ một tầng. Lúc đó việc tính toán bên trong RNN được thực hiện như sau:\n",
    "* $x_t$ là đầu vào bước $t$. Ví dụ, $x_1$ là một vector one-hot tương ứng với từ thứ 2 của câu (trai).\n",
    "* $s_t$ là trạng thái ẩn tại bước $t$. Nó chính là **bộ nhớ** của mạng, $s_t$ được tính toán dựa trên cả các trạng thái ẩn phía trước và đầu vào tại bước đó: $s_t = f(U x_t + W s_{t-1})$. Hàm $f$ thường là một hàm phi tuyến như tanh hoặc ReLu... Để làm phép toán cho phần tử đầu tiên, ta cần khởi tạo thêm $s_{-1}$, thường giá trị khởi tạo là 0.\n",
    "* $o_t$ là đầu ra tại bước $t$. Ví dụ ta muốn dự đoán từ tiếp theo có thể xuất hiện trong câu thì $o_t$ chính là một vector xác suất các từ trong danh sách từ vựng của ta: $o_t=g(V s_t)$. $g$ thường là hàm softmax trong bài toán phân loại."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tóm lại ta có công thức: \n",
    "$$s_{t+1} = f(U x_{t+1} + W s_{t})$$\n",
    "$$o_{t+1} = g(V s_{t+1}) $$\n",
    "Có thể có thêm các hệ số bias trong công thức trên, trong đó:\n",
    "* $x_t$ là input vector tại bước t\n",
    "* $s_t$ là vector lớp ẩn tại bước t\n",
    "* $o_t$ là output vector tại bước t\n",
    "* $U, V, W$ là các ma trận tham số ta cần học\n",
    "* $f, g$ là các hàm activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trực quan trên mô hình mạng neural, chúng ta có thể thấy mạng như sau. Tại bước thứ 0: mạng nhận vào input và sinh ra các lớp ẩn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN-Step0](./images/rnn-step0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tại bước 1, output của lớp 0 sẽ được đưa vào lớp ẩn như là input:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN-Step1](./images/rnn-step1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loss fuction\n",
    "Với bài toán phân loại, chúng ta thường sử dụng hàm cross entropy:\n",
    "$$E(y, \\hat{y}) = \\sum_t E_t (y_t, \\hat{y}_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Lan truyền ngược"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhắc lại công thức lan truyền tiến:\n",
    "$$s_{t+1} = f(U x_{t+1} + W s_{t})$$\n",
    "$$o_{t+1} = g(V s_{t+1}) $$\n",
    "Trong đó $o_{t+1} = \\hat{y}$ là output cuối cùng của RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong RNN, loss của cả mô hình bằng tổng loss của mỗi output. Các tham số chúng ta cần phải tìm là $U, V, W$. Ta cần tính:\n",
    "$$ \\frac{\\partial L}{\\partial U}, \\frac{\\partial L}{\\partial V}, \\frac{\\partial L}{\\partial W}$$\n",
    "Tính đạo hàm với $V$ thì khá đơn giản, ta có $o_{t+1}$ là đầu ra cuối cùng của chúng ta. Do đó:\n",
    "$$ \\frac{\\partial L}{\\partial V} = \\frac{\\partial L}{\\partial o_{t+1}} * \\frac{\\partial o_{t+1}}{\\partial V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, đạo hàm với $U, W$ thì có:\n",
    "$$\\frac{\\partial L}{\\partial U} = \\frac{\\partial L}{\\partial o_{t+1}} * \\frac{\\partial o_{t+1}}{\\partial s_{t+1}} * \\frac{\\partial s_{t+1}}{\\partial U} = \\frac{\\partial L}{\\partial o_{t+1}} * \\frac{\\partial o_{t+1}}{\\partial s_{t+1}} * \\frac{\\partial s_{t+1}}{\\partial s_{t}}*...*\\frac{\\partial s_1}{\\partial U}$$\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial o_{t+1}} * \\frac{\\partial o_{t+1}}{\\partial s_{t+1}} * \\frac{\\partial s_{t+1}}{\\partial U} = \\frac{\\partial L}{\\partial o_{t+1}} * \\frac{\\partial o_{t+1}}{\\partial s_{t+1}} * \\frac{\\partial s_{t+1}}{\\partial s_{t}}*...*\\frac{\\partial s_1}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ công thức trên, Đầu ra của bài toán chúng ta thường xét dạng xác suất nên giá trị đầu ra từ 0 đến 1. Từ đó, ta thấy có xảy ra hiện tức **vanishing gradient** (tức là nhiều đạo hàm liên tiếp nhỏ hơn 1 nhân với nhau - nếu dãy nhân nhau dài sẽ dấn đến hội tụ về 0 ==> từ đó gây ra hiện tượng \"không nhớ\"). Còn nếu ngược lại gọi là **exploding gradient** (tích của rất nhiều số lớn hơn 1 sẽ dấn đến vô cùng làm cho bước cập nhật hệ số gradient không còn chính xác). Điều này sẽ được cải tiến trong mô hình LSTM sau này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 RNN mở rộng\n",
    "Trong nhiều năm, các nhà nghiên cứu đã phát triển nhiều kiểu RNN tinh vi để xử lý các nhược điểm của mô hình RNN truyền thống. Chúng ta sẽ xem chi tiết một vài mô hình đó ở các bài viết sau, còn ở bài này, tôi chỉ giới thiệu ngắn ngọn 2 mô hình dưới đây.\n",
    "### 1.3.1 RNN 2 chiều\n",
    "Ở mô hình RNN 2 chiều (Bidirectional RNN), đầu ra tại bước t không những phụ thuộc vào các phần tử phía trước mà còn phụ thuộc cả vào các phần tử phía sau. Ví dụ, để dự đoán từ còn thiếu trong câu, thì việc xem xét cả phần trước và phần sau của câu là cần thiết. Vì vậy, ta có thể coi mô hình là việc chồng 2 mạng RNN ngược hướng nhau lên nhau. Lúc này đầu ra được tính toán dựa vào cả 2 trạng thái ẩn của 2 mạng RNN ngược hướng này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN-2-dis](./images/bidirectional-rnn-300x196.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.2 RNN (2 chiều) sâu\n",
    "RNN sâu (Deep (Bidirectional) RNN) cũng tương tự như RNN 2 chiều, nhưng khác nhau ở chỗ chúng chứa nhiều tầng ẩn ở mỗi bước. Trong thực tế, chúng giúp cho việc học ở mức độ cao hơn, tuy nhiên ta cũng cần phải có nhiều dữ liệu huấn luyện hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Rnn-deep](./images/Screen-Shot-2015-09-16-at-2.21.51-PM-272x300.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Xây dựng mạng RNN bằng Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để hiểu sâu hơn về cách hoạt động của mô hình RNN, trong phần này, chúng ta sẽ cùng xây dựng mô hình RNN sinh ra các câu hội thoại (hay còn gọi là mô hình ngôn ngữ). Tôi sử dụng <a href=\"./reddit-comments-2015-08.csv\">15000 bình luận trên Reddit</a> của <a href=\"https://console.cloud.google.com/bigquery/?GK=fh-bigquery&page=table&t=2015_08&d=reddit_comments&p=fh-bigquery&redirect_from_classic=true\">BigQuery của Google</a>. Và hy vọng các văn bản sinh ra trông có vẻ như người dùng của Reddit. Cũng như các dự án học máy khác, đầu tiên ta cần xử lý dữ liệu thô cho đúng định dạng đầu vào.\n",
    "## 2.1 Phân rã dữ liệu thô\n",
    "Ta có dữ liệu văn bản thô, nhưng ta lại muốn dự đoán từng từ một, nên ta cần phải phân ra dữ liệu ta thành từng từ riêng biệt. Đầu tiên ta sẽ phân ra thành từng câu một, sau đó lại phân câu thành từng từ riêng biệt. Ta có thể chia các bình luận bằng dấu cách, nhưng cách đó không giúp ta phân tách được các dấu chấm câu. Ví dụ: “He left!” cần phải chia thành 3 phần: “He”, “left”, ”!”. Để đỡ phải vất vả, ta sẽ sử dụng NLTK với hàm word_tokenize và sent_tokenize để phân tách dữ liệu. Ví dụ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK model data (you need to do this once)\n",
    "# nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thắng', 'đẹp', 'trai', 'thế', 'nhỉ', '!']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.word_tokenize(\"Thắng đẹp trai thế nhỉ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Loại bỏ stop word\n",
    "Trong hầu hết các văn bản có những từ chỉ xuất hiện 1 hoặc 2 lần, những từ không xuất hiện thường xuyên như thế này ta hoàn toàn có thể loại bỏ. Càng nhiều từ thì mô hình của ta học càng chậm (ta sẽ nói lý do sau), và chúng ta không có nhiều ví dụ sử dụng những từ đó nên không thể nào mà học cách sử dụng chúng sao cho chính xác được. Việc này cũng khá giống với cách con người học. Để hiểu cách sử dụng một từ chuẩn xác, bạn cần phải xem xét nó ở nhiều ngữ cảnh khác nhau.\n",
    "\n",
    "Ta sẽ giới hạn lượng từ vựng phổ biến của ta bằng biến `vocabulary_size` (ở đây, tôi để là 8000, nhưng bạn cứ thay đổi nó thoải mái). Những từ ít gặp không nằm trong danh sách các từ phổ biến, ta sẽ thay thế nó bằng `UNKNOWN_TOKEN`. Ví dụ, nếu danh sách vựng của ta không có từ `“nonlinearities”` thì câu `“nonlineraties are important in Neural Networks”` sẽ được chuyển hoá thành `“UNKNOWN_TOKEN are important in Neural Networks”`. Ta sẽ coi `UNKNOWN_TOKEN` cũng là 1 phần của danh sách từ vựng và cũng sẽ dự đoán nó như các từ khác. Khi một từ mới được sinh ra, ta có thể thay thế `UNKNOWN_TOKEN` lại bằng cách lấy ngẫu nhiên một từ nào đó không nằm trong danh sách từ vựng của ta, hoặc ta có thể tạo ra một các từ cho tới khi từ được sinh ra nằm trong danh sách từ của ta.\n",
    "## 2.3 Thêm ký tự đầu, cuối\n",
    "Ta cũng muốn xem từ nào là từ bắt đầu và từ nào là từ kết thúc của một câu. Để làm được chuyện đó, ta cần phải thêm vào 2 kí tự đặc biệt cho mỗi câu là: `SENTENCE_START` liền trước câu và `SENTENCE_END` liền sau câu. Nó sẽ cho phép ta đặt câu hỏi là: Giờ ta có một từ là `SENTENCE_START`, thì từ tiếp theo của ta sẽ là gì? Từ tiếp theo chính là từ đầu tiên của câu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Ma trận hoá dữ liệu\n",
    "Đầu vào của RNN là các vec-tơ chứ không phải là các chuỗi. Nên ta cần chuyển đổi giữa các từ và địa chỉ tương ứng với `index_to_word` và `word_to_index`. Ví dụ, từ `“friendly”` ở vị trí 2001 trong danh sách từ vựng thì địa chỉ của nó sẽ là 2001. Như vậy tập dữ liệu x của sẽ có dạng: `[0, 179, 314, 416]`, trong đó 0 tương ứng với `SENTENCE_START`. Còn các nhãn (dự đoán) y sẽ là `[179, 341, 416, 1]`, trong đó 1 tương ứng với `SENTENCE_END`. Vì mục tiêu của ta là dự đoán các từ tiếp theo, nên y đơn giản là dịch một vị trí so với x, và kết câu là `SENTENCE_END`. Nói cách khác, với dự đoán chuẩn xác cho từ 179 sẽ là 314."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "data = pd.read_csv('reddit-comments-2015-08.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I joined a new league this year and they have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In your scenario, a person could just not run ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They don't get paid for how much time you spen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I dunno, back before the August update in an A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No, but Toriyama sometimes would draw himself ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>I've got such a good feeling about this season...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>insider rank (no cute sign cuz that will make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>See here for why this isn't as clear a compari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>&amp;gt;  If you cannot stop the bleeding by apply...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>Just got denied with POID BUIU 0001 and offer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body\n",
       "0      I joined a new league this year and they have ...\n",
       "1      In your scenario, a person could just not run ...\n",
       "2      They don't get paid for how much time you spen...\n",
       "3      I dunno, back before the August update in an A...\n",
       "4      No, but Toriyama sometimes would draw himself ...\n",
       "...                                                  ...\n",
       "14995  I've got such a good feeling about this season...\n",
       "14996  insider rank (no cute sign cuz that will make ...\n",
       "14997  See here for why this isn't as clear a compari...\n",
       "14998  &gt;  If you cannot stop the bleeding by apply...\n",
       "14999  Just got denied with POID BUIU 0001 and offer ...\n",
       "\n",
       "[15000 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['body'].to_list()\n",
    " # Append SENTENCE_START and SENTENCE_END\n",
    "sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SENTENCE_START I joined a new league this year and they have different scoring rules than I'm used to. It's a slight PPR league- .2 PPR. Standard besides 1 points for 15 yards receiving, .2 points per completion, 6 points per TD thrown, and some bonuses for rec/rush/pass yardage. My question is, is it wildly clear that QB has the highest potential for points? I put in the rules at a ranking site and noticed that top QBs had 300 points more than the top RB/WR. Would it be dumb not to grab a QB in the first round? SENTENCE_END\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xử lý 15000 sentences\n"
     ]
    }
   ],
   "source": [
    "print(\"Xử lý %d sentences\"%(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76325 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 68328), (',', 52137), ('the', 48270), ('to', 35179), ('a', 30853)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = [x[0] for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word.append(unknown_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'reliably' and appeared 10 times.\n"
     ]
    }
   ],
   "source": [
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: \n",
      "'SENTENCE_START I joined a new league this year and they have different scoring rules than I'm used to. It's a slight PPR league- .2 PPR. Standard besides 1 points for 15 yards receiving, .2 points per completion, 6 points per TD thrown, and some bonuses for rec/rush/pass yardage. My question is, is it wildly clear that QB has the highest potential for points? I put in the rules at a ranking site and noticed that top QBs had 300 points more than the top RB/WR. Would it be dumb not to grab a QB in the first round? SENTENCE_END'\n",
      "Example sentence after Pre-processing: \n",
      "'['SENTENCE_START', 'I', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'I', \"'m\", 'used', 'to', '.', 'It', \"'s\", 'a', 'slight', 'PPR', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'PPR', '.', 'UNKNOWN_TOKEN', 'besides', '1', 'points', 'for', '15', 'yards', 'receiving', ',', 'UNKNOWN_TOKEN', 'points', 'per', 'completion', ',', '6', 'points', 'per', 'TD', 'thrown', ',', 'and', 'some', 'bonuses', 'for', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', '.', 'My', 'question', 'is', ',', 'is', 'it', 'wildly', 'clear', 'that', 'QB', 'has', 'the', 'highest', 'potential', 'for', 'points', '?', 'I', 'put', 'in', 'the', 'rules', 'at', 'a', 'ranking', 'site', 'and', 'noticed', 'that', 'top', 'QBs', 'had', '300', 'points', 'more', 'than', 'the', 'top', 'UNKNOWN_TOKEN', '.', 'Would', 'it', 'be', 'dumb', 'not', 'to', 'grab', 'a', 'QB', 'in', 'the', 'first', 'round', '?', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "print(\"Example sentence: \\n'%s'\" % sentences[0])\n",
    "print(\"Example sentence after Pre-processing: \\n'%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Xây dựng mạng RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu vào $x$ sẽ là một chuỗi các từ và mỗi $x_t$ sẽ là một từ đơn. Nhưng vì phép nhân ma trận không cho phép ta sử dụng một địa chỉ của từ để làm việc, nên ta phải biểu diễn từ đó bằng véc-tơ one-hot với kích thước là `vocabulary_size`. Ví dụ, từ có địa chỉ là 36 thì sẽ có véc-tơ tương ứng là: vị trí thứ 36 là 1, còn lại là 0 cả. Vì mỗi $x_t$ là một véc-tơ, nên $x$ lúc này sẽ là một ma trận với mỗi hàng biểu diễn một từ. Ta sẽ thực hiện việc chuyển đổi này ở phần mã mạng nơ-ron chứ không thực hiện ở phần tiền xử lý. Đầu ra của mạng oo cũng sẽ có dạng tương tự. Mỗi $o_t$ là một véc-tơ có kích cỡ `vocabulary_size` và mỗi phần tử thể hiện xác xuất xuất hiện kế tiếp của từ tương ứng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta sẽ thiết lập mạng RNN như sau:\n",
    "$$s_t = tanh(U x_t + W s_{t-1})$$\n",
    "$$o_t = softmax(V s_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử ta chọn lượng từ vựng $C=8000$ và số tầng ẩn là $H=100$. Ta có thể coi tầng ẩn là tầng *bộ nhớ* của mạng, càng nhiều tầng ẩn thì ta càng học được nhiều mẫu phức tạp, nhưng đổi lại thời gian tính toán cũng tăng lên. Với trường hợp này, các véc-tơ và ma trận của ta có kích cỡ như sau:\n",
    "$$x_t \\in \\mathbb{R}^{8000}$$\n",
    "$$o_t \\in \\mathbb{R}^{8000}$$\n",
    "$$s_t \\in \\mathbb{R}^{100}$$\n",
    "$$U \\in \\mathbb{R}^{100x8000}$$\n",
    "$$V \\in \\mathbb{R}^{8000x100}$$\n",
    "$$W \\in \\mathbb{R}^{100x100}$$\n",
    "Những thông tin này cực kì có giá trị trong quá trình xây dựng mạng. $U, V$ và $W$ là các tham số của mạng mà ta cần phải học từ tập dữ liệu. Vì vậy, ta sẽ cần học cả thảy là $2HC + H^2$ tham số. Với $C = 8000$ và $H = 100$ thì tổng số tham số là $1,610,000$. Ngoài ra, các kích cỡ này cho cho ta biết được nút thắt của mô hình khi hoạt động. Lưu ý rằng, vì $x_t$ là véc-tơ one-hot nên khi nhân nó với $U$ thì chỉ cần lấy cột tương ứng của $U$ là được chứ không cần phải thực hiện phép nhân ma trận đầy đủ. Vì vậy, phép nhân lớn nhất của mạng là $V s_t$, đó chính là lý do mà ta muốn giữ cho lượng từ vựng của ta ít nhất có thể."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Khởi tạo\n",
    "Ta sẽ bắt đầu bằng việc khởi tạo các tham số của mạng trong lớp RNN. Tôi sẽ đặt tên lớp này là RNNNumpy. Khởi tạo các tham số có chút ràng buộc là không thể để chúng bằng 0 ngay được. Vì như vậy sẽ làm cho mạng của ta không thể học được. Ta phải khởi tạo chúng một cách ngẫu nhiên. Hiện nay đã có nhiều nghiên cứu chỉ ra việc khởi tạo tham số có ảnh hưởng tới kết quả huấn luyện ra sao. Việc khởi tạo còn phụ thuộc vào hàm kích hoạt (activation function) của ta là gì nữa. Trong trường hợp của ta là hàm $\\tanh$, nên giá trị khởi tạo được khuyến khích nằm trong khoảng $\\left[ -\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}} \\right]$. Trong đó $n$ là lượng kết nối từ tầng mạng trước. Nhìn nó có vẻ phức tạp, nhưng đừng lo lắng nhiều về nó. Chỉ cần bạn khởi tạo các tham số của mình ngẫu nhiên đủ nhỏ thì thường mạng của ta sẽ hoạt động tốt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # random init parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word_dim` ở trên là kích cỡ của tập từ vựng, `hidden_dim` là số lượng tầng ẩn của ta. Còn `bptt_truncate` thì ta sẽ giải thích sau.\n",
    "### 2.5.2 Lan truyền tiến\n",
    "Tiếp theo, ta sẽ cài đặt hàm lan truyền tiến (forward propagation) để thực hiện việc tính xác xuất của từ như sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)\n",
    "\n",
    "def forward_propagation(self, x):\n",
    "    # tổng số time step\n",
    "    T = len(x)\n",
    "    # trong quá trình lan truyền tiến, ta phải lưu lại hết trạng thái s\n",
    "    # vì cần sau này, chúng ta sẽ khởi tạo là 0\n",
    "    s = np.zeros((T+1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    # output mỗi time step\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    # lặp lan truyền tiến\n",
    "    for t in range(T):\n",
    "        # do x_t là vector one-hot nên chỉ cần lấy ra chỉ số cột của U = U.x_t\n",
    "        # chỗ này tương đương với việc chuyển x sang dạng one-hot\n",
    "        s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở đây, ta không chỉ trả ra kết quả tính toán được mà còn trả ra cả trạng thái ẩn, để phục vụ cho việc tính đạo hàm, việc này tránh cho ta phải tính lại lần nữa khi tính đạo hàm. Mỗi $o_t$ là một véc-tơ xác xuất của mỗi từ trong danh sách từ vựng của ta, nhưng đôi lúc ta chỉ cần lấy từ có xác xuất cao nhất. Ở đây ta sẽ định nghĩa một hàm dự đoán như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giờ ta thử chảy các hàm vừa cài đặt xem kết quả ra sao:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 8000)\n",
      "[[0.00012409 0.00012392 0.00012493 ... 0.00012486 0.00012504 0.00012556]\n",
      " [0.00012467 0.00012558 0.00012469 ... 0.0001245  0.00012512 0.00012532]\n",
      " [0.00012503 0.00012528 0.00012491 ... 0.0001252  0.00012566 0.00012572]\n",
      " ...\n",
      " [0.00012406 0.00012494 0.00012494 ... 0.0001248  0.0001245  0.00012491]\n",
      " [0.00012585 0.0001248  0.00012486 ... 0.00012505 0.00012617 0.00012532]\n",
      " [0.00012382 0.00012419 0.00012616 ... 0.00012522 0.00012464 0.00012505]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với mỗi từ trong câu (242 ở trên), mô hình của ta sẽ tính 8000 xác xuất có thể của từ tiếp theo. Chú ý rằng, ta khởi tạo $U, V, W$ ngẫu nhiên nên lúc này các xác xuất dự đoán được ở trên cũng là ngẫu nhiên. Với đầu ra như vậy, ta có thể lấy địa chỉ của từ có xác xuất cao nhất cho mỗi từ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242,)\n",
      "[2133 4965 3497 7532 6827 5186 7848 1820 2979 6562 5027 7859 7797 3158\n",
      " 7754 7784 5785 1002 3678 7477 4276 2762 7178 3497 7316 7123 7673 2080\n",
      "  911 2972 4128  884 4757 6530 2007 6496 5995 2691 3760 7703  865 2906\n",
      " 6814  854 1337 3497 2026 6208 4886 6876 1285 6511  711 6208 2638 6564\n",
      " 6821 4401 3827 4279 6821 2175 1313 1964 3541 1163 2326 6446 7015 4440\n",
      " 1925 2963 1820 6037 7800  226  357 7311 4315 2026 3334 3025 1899 3989\n",
      " 1847  497 7687 3989 5394 4936 7868 5860 2158 3074 3334 3025 1935 4427\n",
      " 3984 4551 7100 3109  756 4529  743  865 4401   64 2630  862 2026 3334\n",
      " 7742 2563 6127 1820 7316 3025 5465 1964 5234 6454 1425 7697 1636 4720\n",
      " 4003 2998 7049   57 1826 1847 4864 7088 2201 6601 2082 5216 5232 1820\n",
      " 1042 6872 6530 3384 2719 7018 1833 3376 2434 4864 1929  881 1815 1976\n",
      " 1384 4864 4315 7652 2390 6546 1447 6601 4381  865  454 7377 7282 6564\n",
      "  865 5394 3497 4467 1796 3989 2915 5337 2201 6601 2082 6821 1386 2098\n",
      " 6505 1207 1546 3984 7673 2175 7186 6396 6501  161 5064   94 2042 1250\n",
      " 1067 2102 6947 6096 5037 3226 6821   17 3464 3648 1195 7830 5479 7652\n",
      " 2007 7712 6609 6876 4931 4854 3530  359 1386 5698 2998 7017 1131 6530\n",
      " 2209 3990 1977 7694 7960 5441 7123 2292 4591 1267 4927  624 3025 2042\n",
      " 5344 7532 2175 1833]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 Tính lỗi\n",
    "Để huấn luyện mạng, ta cần phải đánh giá được lỗi cho từng tham số. Và mục tiêu của ta là tìm các tham số $U, V, W$ để tối thiểu hàm lỗi (loss function) $L$ của ta trong quá trình huấn luyện. Một trong số các hàm đánh giá lỗi thường được sử dụng là `cross-entropy`. Nếu ta có $N$ mẫu huấn luyện (số từ trong văn bản) và $C$ lớp (số từ vựng) thì lỗi tương ứng với dự đoán o và nhãn chuẩn $y$ sẽ là:\n",
    "$$ L(y,o)= -\\frac{1}{N} \\sum y_n \\log{o_n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    for i in range(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giờ nhìn lại một chút và nghĩ xem lỗi sẽ thế nào với các tham số được khởi tạo ngẫu nhiên. Nó sẽ giúp ta đảm bảo được việc cài đặt của ta là chính xác. Ta có $C$ từ trong tập từ vựng, vì vậy mỗi từ sẽ có dự đoán trung bình là $1/C$, nên lỗi của ta sẽ là $L=-\\frac{1}{N}\\log{\\frac{1}{C}} = \\log{C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.987197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss: 8.987196\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print(\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print(\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có vẻ khá gần với kết quả chuẩn xác rồi! Cũng nói luôn rằng việ đánh giá lỗi cho toàn bộ tập dữ liệu là một thao tác tốn kém, có thể mất tới hàng giờ đồng hồ tùy thuộc vào lượng dữ liệu mà ta đưa vào huấn luyện.\n",
    "### 2.5.4 BPTT\n",
    "Nhớ lại rằng, ta cần tìm các tham số $U, V, W$ sao cho tổng lỗi của ta là nhỏ nhất với tập dữ liệu huấn luyện. Cách phổ biến nhất là sử dụng SGD (Stochastic Gradient Descent - trượt đồi). Ý tưởng đằng sau SGD khác đơn giản. Ta sẽ lặp đi lặp lại suốt tập dữ liệu của ta và tạo mỗi bước lặp ta sẽ thay đổi tham số của ta sao cho tổng lỗi có thể giảm đi. Hướng của việc cập nhập tham số được tính dựa vào đạo hàm của hàm lỗi: $\\frac{\\partial L}{\\partial U}, \\frac{\\partial L}{\\partial V}, \\frac{\\partial L}{\\partial W}$. Để thực hiện SGD, ta cần phải có độ học (learning rate) để xác định các mức độ thay đổi tham số của ta ở mỗi bước lặp. SGD không chỉ là phương thức tối ưu phổ biến nhất trong mạng nơ-ron mà còn trong nhiều giải thuật học máy khác nữa. Cho tới thời điểm này, ta có rất nhiều các nghiên cứu làm sao để tối ưu SGD bằng cách sử dụng các lô dữ liệu (mini-batch), bằng cách song song hoá và thay đổi tham số học trong quá trình huấn luyện. Thậm chí với nhiều ý tưởng đơn giản để thực hiện SGD một cách hiệu quả cũng khiến nó trở lên rất phức tạp để cài đặt. Trên mạng hiện có rất nhiều bài hướng dẫn về SGD, nên tôi sẽ không bàn cụ thể nó ở đây nữa. Tôi sẽ chỉ cài đặt phiên bản đơn giản của SGD để cho cả các bạn không có kiến thức về tối ứu hoá có thể dễ nắm bắt được vấn đề."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape) \n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.5 Kiểm tra đạo hàm\n",
    "Khi cài đặt thuật toán lan truyền ngược thì cũng nên cài đặt luôn phép kiểm tra đạo hàm (gradient checking) để kiểm chứng rằng giải thuật ta cài đặt không bị sai. Ý tưởng đằng sau phép kiểm tra đạo hàm là đạo hàm riêng của mỗi tham số tương đương với độ dốc tại điểm đó. Vì vậy ta có thể thay đổi giá tham số một chút rồi chia cho khoảng thay đổi đó để được sấp sỉ đạo hàm riêng theo tham số đó.\n",
    "$$ \\frac{\\partial L}{\\partial \\theta} = \\lim_{h \\to 0} \\frac{J(\\theta+h) - J(\\theta-h)}{2h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó ta sẽ kiểm tra đạo hảm thu được bằng giải thuật lan truyền ngược với giá trị thu được ở công thức trên. Nếu sự khác biệt không lớn thì giải thuật vừa cải đặt là chấp nhận được. Ta cần phải tính đạo hàm riêng bằng công thức trên cho tất cả các tham số, nên việc kiểm tra đạo hàm cũng là một thao tác tốn kém (lưu ý rằng ta có tới hơn một triệu tham số ở ví dụ trên nhé). Nên trong thực tế ta chỉ cần thực hiện phép kiểm định đó trên một tập từ vựng nhỏ hơn thực tế."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = self.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print(\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = self.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = self.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print(\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                print(\"+h Loss: %f\" % gradplus)\n",
    "                print(\"-h Loss: %f\" % gradminus)\n",
    "                print(\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                print(\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print(\"Relative Error: %f\" % relative_error)\n",
    "                return\n",
    "            it.iternext()\n",
    "        print(\"Gradient check for parameter %s passed.\" % (pname))\n",
    "\n",
    "RNNNumpy.gradient_check = gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.6 Thực hiện SGD\n",
    "Giờ ta đã có thể tính được đạo hàm cho từng tham số nên có thể cài đặt được SGD. Ta sẽ thực hiện nó qua 2 bước: \n",
    "1. Xây dựng hàm `sdg_step` để tính đạo hàm và thực hiện việc cập nhập cho mỗi lô dữ liệu. \n",
    "2. Chạy một vòng lặp bên ngoài suốt toàn bộ tập dữ liệu và điều chỉnh độ học."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNNNumpy.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            print(\"Loss after num_examples_seen=%d epoch=%d: %f\" % (num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print(\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3 s ± 54.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after num_examples_seen=0 epoch=0: 8.987180\n",
      "Loss after num_examples_seen=100 epoch=1: 9.043326\n",
      "Setting learning rate to 0.002500\n",
      "Loss after num_examples_seen=200 epoch=2: 6.923875\n",
      "Loss after num_examples_seen=300 epoch=3: 6.510091\n",
      "Loss after num_examples_seen=400 epoch=4: 6.342430\n",
      "Loss after num_examples_seen=500 epoch=5: 6.259318\n",
      "Loss after num_examples_seen=600 epoch=6: 6.202807\n",
      "Loss after num_examples_seen=700 epoch=7: 6.166752\n",
      "Loss after num_examples_seen=800 epoch=8: 6.121506\n",
      "Loss after num_examples_seen=900 epoch=9: 6.165038\n",
      "Setting learning rate to 0.001250\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tốt, có vẻ như ta cài đặt nó không sai và lỗi đang được giảm đi rồi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.7 Sinh văn bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây là một số câu được sinh ra sau khi ta code thêm sinh văn bản:\n",
    "* Anyway, to the city scene you’re an idiot teenager.\n",
    "* What ? ! ! ! ! ignore!\n",
    "* Screw fitness, you’re saying: https\n",
    "* Thanks for the advice to keep my thoughts around girls.\n",
    "* Yep, please disappear with the terrible generation.\n",
    "\n",
    "Nhìn vào các câu được sinh ra, ta có thể thu được vài thứ đáng lưu tâm ở đây. Mô hình của ta đã học được cách sử dụng cú pháp câu thành công, nó thêm được các dấu phẩy (and’s và or’s) và chấm hếtcaau nữa. Đôi lúc nó còn thêm được cả các dấu chấm cảm và mặt cười như các bình luận trên SNS.\n",
    "\n",
    "Tuy nhiên vẫn các câu sinh ra vẫn gặp một điểm yếu lớn là ngữ pháp chưa chính xác (các câu ở trên là tôi đã nhặt các câu tốt nhất rồi đó). Một lý do có thể là do ta chưa huấn luyện nó đủ lâu, nhưng hình như đó không phải là lý do chính. RNN thuần không thể sinh được các câu có nghĩa vì nó không thể học được các phụ thuộc giữa các từ cách xa nhau. Đó cũng là lý do mà RNN không được ưu chuộng khi nó được sáng tạo ra.\n",
    "\n",
    "STM hiện này là một phương pháp chính được sử dụng cho rất nhiều bài toán NLP (và có thể sinh ra các bình luận Reddit hợp lý hơn). Tất cả những điều bạn học được trong phần này sẽ được áp dụng cho LSTM và các mô hình RNN khác nữa, nên đừng cảm thấy thất vọng ngay với kết quả của RNN thuần thu được."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Xây dựng mạng RNN bằng Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có cách đơn giản hơn rất nhiều để xây dựng mạng RNN là sử dụng thư viện Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([13, 5, 3676, 4, 176, 1203, 31, 230, 6, 33, 21, 213, 5105, 377, 90, 5, 65, 220, 3, 0, 58, 16, 4, 3127, 7441, 7999, 7999, 7441, 0, 7999, 2164, 212, 622, 15, 782, 3677, 3053, 1, 7999, 622, 501, 6216, 1, 473, 622, 501, 7007, 2748, 1, 6, 76, 5874, 15, 7999, 7999, 0, 217, 281, 11, 1, 11, 9, 6604, 626, 10, 3572, 73, 2, 1727, 1003, 15, 622, 30, 5, 231, 12, 2, 377, 45, 4, 3274, 671, 6, 1111, 10, 338, 4727, 87, 2451, 622, 56, 90, 2, 338, 7999, 0, 1195, 9, 24, 1457, 28, 3, 1793, 4, 3572, 12, 2, 150, 1231, 30]),\n",
       "       list([13, 226, 35, 1458, 1, 4, 255, 103, 43, 28, 347, 2, 3371, 1050, 385, 22, 2, 4546, 6, 124, 730, 2, 335, 3, 2, 5317, 0, 181, 16, 88, 112, 3, 3128, 9, 0, 1617, 1169, 4052, 11, 122, 3, 28, 730, 2, 335, 3, 71, 74, 33, 114, 33, 72, 4, 5317, 22, 2, 1050, 385, 0, 205, 7982, 4052, 11, 17, 122, 3, 347, 2, 385, 12, 2, 150, 280, 0, 265, 67, 11, 122, 3, 24, 1169, 221, 3, 347, 2, 385, 1, 114, 33, 72, 4, 5317, 1, 6, 99, 54, 7, 4, 3196, 1066, 519, 7982, 6, 138, 46, 7983, 1, 8, 94, 63, 1, 146, 16, 35, 335, 789, 0, 37, 152, 47, 17, 347, 2, 403, 385, 12, 2, 150, 280, 1, 4889, 0, 333, 1242, 7442, 11, 7999, 7999, 0, 129, 11, 173, 57, 229, 50, 85, 41, 616, 16, 55, 291, 1, 173, 28, 106, 4, 1050, 385, 371, 148, 9, 47, 24, 1392, 3, 730, 562, 3, 7999, 30, 137, 92, 17, 2379, 8, 30, 7999, 0, 184, 269, 403, 21, 10, 0, 235, 25, 17, 8, 879, 49, 31, 30, 68, 58, 16, 43, 745, 28, 810, 3, 989, 7984, 0, 5, 140, 42, 3, 106, 9, 810, 1, 27, 66, 16, 88, 186, 12, 277, 9, 3371, 0, 50, 85, 41, 80, 72, 43, 2131, 9, 3, 106, 4, 7999, 715, 1050, 385, 5580, 12, 214, 1703, 0, 252, 4, 335, 1750, 89, 73, 7999, 88, 67, 6, 73, 7999, 2590, 946, 1, 5, 160, 29, 17, 207, 2, 7985, 372, 4, 1015, 153, 0, 58, 16, 57, 42, 8, 10, 2379, 59, 3, 28, 228, 55, 6217, 45, 54, 0, 5, 65, 7999, 3, 5318, 7999, 3, 989, 7984, 32, 3788, 0, 1490, 9, 34, 499, 9, 1, 70, 9, 11, 17, 122, 3, 24, 197, 3371, 0, 295, 638, 463, 484, 0, 7443, 0]),\n",
       "       list([13, 152, 29, 17, 60, 767, 15, 96, 107, 83, 8, 707, 735, 35, 2696, 0, 152, 60, 767, 74, 8, 4728, 9, 0, 2095, 4, 7986, 4729, 4365, 63, 1, 711, 1059, 30, 3475, 10, 83, 33, 104, 197, 177, 1060, 6, 124, 21, 2, 742, 3, 106, 177, 1414, 38, 8, 1009, 3, 4728, 9, 0]),\n",
       "       ...,\n",
       "       list([13, 1536, 146, 15, 173, 31, 11, 17, 32, 626, 4, 1876, 19, 34, 32, 303, 55, 7999, 18, 32, 2, 6358, 47, 21, 8, 321, 20, 117, 20, 7999]),\n",
       "       list([13, 50, 85, 41, 82, 8, 44, 28, 433, 2, 6195, 78, 6092, 1444, 1, 8, 44, 143, 2, 6186, 3, 7999, 2, 5370, 0, 832, 851, 3764, 6, 1330, 3, 24, 220, 38, 1552, 227, 11, 86, 443, 7, 1861, 1, 9, 11, 160, 410, 1312, 0, 258, 17, 321, 59, 30, 1171, 9, 0, 5, 29, 17, 321, 8, 0, 986, 1, 38, 9, 16, 6195, 6, 8, 7999, 9, 1, 8, 21, 3, 7999, 2, 671, 7, 2, 7999, 34, 8, 72, 43, 2157, 55, 3043, 7999, 120, 55, 3723, 7999, 0, 1006, 8, 104, 7999, 9, 1, 131, 8, 21, 3, 493, 23, 2, 2069, 0, 7999, 1298, 56, 391, 1, 25, 124, 126, 6322, 3, 6793, 1, 25, 851, 3764, 1, 6, 999, 35, 135, 15, 569, 0, 7999, 11, 4, 937, 683, 0, 7999, 1, 1259, 1444, 1, 1444, 7999, 1, 1444, 186, 1, 34, 38, 8, 72, 4430, 4, 7999, 0, 82, 1495, 7, 10, 5428, 9, 1, 99, 43, 1119, 2, 7999, 159, 6, 294, 9, 7999, 1, 8, 136, 24, 61, 7236, 789, 6, 9, 136, 1107, 4, 155, 251, 0, 7999, 1, 42, 7999, 1, 11, 67, 7, 167, 804, 7999, 53, 2, 7999, 10, 101, 17, 672, 74, 9, 111, 21, 0]),\n",
       "       list([13, 308, 151, 3661, 23, 7999, 7999, 7999, 6, 1074, 763, 7999, 7999, 7999, 7999, 0, 52, 4914, 19, 7999, 18, 180, 10, 271, 73, 102, 1348, 12, 23, 183, 6, 10, 5, 170, 17, 143, 2, 763, 70, 9, 16, 830, 3, 24, 7999, 0])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([5, 3676, 4, 176, 1203, 31, 230, 6, 33, 21, 213, 5105, 377, 90, 5, 65, 220, 3, 0, 58, 16, 4, 3127, 7441, 7999, 7999, 7441, 0, 7999, 2164, 212, 622, 15, 782, 3677, 3053, 1, 7999, 622, 501, 6216, 1, 473, 622, 501, 7007, 2748, 1, 6, 76, 5874, 15, 7999, 7999, 0, 217, 281, 11, 1, 11, 9, 6604, 626, 10, 3572, 73, 2, 1727, 1003, 15, 622, 30, 5, 231, 12, 2, 377, 45, 4, 3274, 671, 6, 1111, 10, 338, 4727, 87, 2451, 622, 56, 90, 2, 338, 7999, 0, 1195, 9, 24, 1457, 28, 3, 1793, 4, 3572, 12, 2, 150, 1231, 30, 14]),\n",
       "       list([226, 35, 1458, 1, 4, 255, 103, 43, 28, 347, 2, 3371, 1050, 385, 22, 2, 4546, 6, 124, 730, 2, 335, 3, 2, 5317, 0, 181, 16, 88, 112, 3, 3128, 9, 0, 1617, 1169, 4052, 11, 122, 3, 28, 730, 2, 335, 3, 71, 74, 33, 114, 33, 72, 4, 5317, 22, 2, 1050, 385, 0, 205, 7982, 4052, 11, 17, 122, 3, 347, 2, 385, 12, 2, 150, 280, 0, 265, 67, 11, 122, 3, 24, 1169, 221, 3, 347, 2, 385, 1, 114, 33, 72, 4, 5317, 1, 6, 99, 54, 7, 4, 3196, 1066, 519, 7982, 6, 138, 46, 7983, 1, 8, 94, 63, 1, 146, 16, 35, 335, 789, 0, 37, 152, 47, 17, 347, 2, 403, 385, 12, 2, 150, 280, 1, 4889, 0, 333, 1242, 7442, 11, 7999, 7999, 0, 129, 11, 173, 57, 229, 50, 85, 41, 616, 16, 55, 291, 1, 173, 28, 106, 4, 1050, 385, 371, 148, 9, 47, 24, 1392, 3, 730, 562, 3, 7999, 30, 137, 92, 17, 2379, 8, 30, 7999, 0, 184, 269, 403, 21, 10, 0, 235, 25, 17, 8, 879, 49, 31, 30, 68, 58, 16, 43, 745, 28, 810, 3, 989, 7984, 0, 5, 140, 42, 3, 106, 9, 810, 1, 27, 66, 16, 88, 186, 12, 277, 9, 3371, 0, 50, 85, 41, 80, 72, 43, 2131, 9, 3, 106, 4, 7999, 715, 1050, 385, 5580, 12, 214, 1703, 0, 252, 4, 335, 1750, 89, 73, 7999, 88, 67, 6, 73, 7999, 2590, 946, 1, 5, 160, 29, 17, 207, 2, 7985, 372, 4, 1015, 153, 0, 58, 16, 57, 42, 8, 10, 2379, 59, 3, 28, 228, 55, 6217, 45, 54, 0, 5, 65, 7999, 3, 5318, 7999, 3, 989, 7984, 32, 3788, 0, 1490, 9, 34, 499, 9, 1, 70, 9, 11, 17, 122, 3, 24, 197, 3371, 0, 295, 638, 463, 484, 0, 7443, 0, 14]),\n",
       "       list([152, 29, 17, 60, 767, 15, 96, 107, 83, 8, 707, 735, 35, 2696, 0, 152, 60, 767, 74, 8, 4728, 9, 0, 2095, 4, 7986, 4729, 4365, 63, 1, 711, 1059, 30, 3475, 10, 83, 33, 104, 197, 177, 1060, 6, 124, 21, 2, 742, 3, 106, 177, 1414, 38, 8, 1009, 3, 4728, 9, 0, 14]),\n",
       "       ...,\n",
       "       list([1536, 146, 15, 173, 31, 11, 17, 32, 626, 4, 1876, 19, 34, 32, 303, 55, 7999, 18, 32, 2, 6358, 47, 21, 8, 321, 20, 117, 20, 7999, 14]),\n",
       "       list([50, 85, 41, 82, 8, 44, 28, 433, 2, 6195, 78, 6092, 1444, 1, 8, 44, 143, 2, 6186, 3, 7999, 2, 5370, 0, 832, 851, 3764, 6, 1330, 3, 24, 220, 38, 1552, 227, 11, 86, 443, 7, 1861, 1, 9, 11, 160, 410, 1312, 0, 258, 17, 321, 59, 30, 1171, 9, 0, 5, 29, 17, 321, 8, 0, 986, 1, 38, 9, 16, 6195, 6, 8, 7999, 9, 1, 8, 21, 3, 7999, 2, 671, 7, 2, 7999, 34, 8, 72, 43, 2157, 55, 3043, 7999, 120, 55, 3723, 7999, 0, 1006, 8, 104, 7999, 9, 1, 131, 8, 21, 3, 493, 23, 2, 2069, 0, 7999, 1298, 56, 391, 1, 25, 124, 126, 6322, 3, 6793, 1, 25, 851, 3764, 1, 6, 999, 35, 135, 15, 569, 0, 7999, 11, 4, 937, 683, 0, 7999, 1, 1259, 1444, 1, 1444, 7999, 1, 1444, 186, 1, 34, 38, 8, 72, 4430, 4, 7999, 0, 82, 1495, 7, 10, 5428, 9, 1, 99, 43, 1119, 2, 7999, 159, 6, 294, 9, 7999, 1, 8, 136, 24, 61, 7236, 789, 6, 9, 136, 1107, 4, 155, 251, 0, 7999, 1, 42, 7999, 1, 11, 67, 7, 167, 804, 7999, 53, 2, 7999, 10, 101, 17, 672, 74, 9, 111, 21, 0, 14]),\n",
       "       list([308, 151, 3661, 23, 7999, 7999, 7999, 6, 1074, 763, 7999, 7999, 7999, 7999, 0, 52, 4914, 19, 7999, 18, 180, 10, 271, 73, 102, 1348, 12, 23, 183, 6, 10, 5, 170, 17, 143, 2, 763, 70, 9, 16, 830, 3, 24, 7999, 0, 14])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu vào và đầu ra một mô hình là cố định. Tuy nhiên các câu của chúng ta đang câu dài câu ngắn, chúng ta cần chuẩn hoá, cố định kích thước cho input và output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2128"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(sent) for sent in X_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu dài nhất có độ dài 2128 từ. Tuy nhiên, chúng ta không cần phải thiết lập độ dài max của câu đến tận 2128 mà chỉ cần 120. Trên thực tế thì các mô hình nhớ chỉ xử lý được câu dưới 120 từ. Với những câu có độ dài ngắn hơn 120, chúng ta sẽ thêm viền (gọi là padding), ta sẽ gọi từ viền của 1 câu là PAD. Với các câu dài hơn 120, chúng ta sẽ cắt bớt nó đi. Để làm điều này ta sử dụng `pad_sequences`. Tham khảo tại: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thêm từ PAD vào bộ từ vựng\n",
    "word_to_index['PAD'] = len(word_to_index)\n",
    "index_to_word.append('PAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_pad = pad_sequences(X_train, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post', value=word_to_index['PAD'])\n",
    "y_pad = pad_sequences(y_train, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post', value=word_to_index['PAD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  13,    5, 3676, ..., 8000, 8000, 8000],\n",
       "       [  13,  226,   35, ...,   37,  152,   47],\n",
       "       [  13,  152,   29, ..., 8000, 8000, 8000],\n",
       "       ...,\n",
       "       [  13, 1536,  146, ..., 8000, 8000, 8000],\n",
       "       [  13,   50,   85, ...,  126, 6322,    3],\n",
       "       [  13,  308,  151, ..., 8000, 8000, 8000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5, 3676,    4, ..., 8000, 8000, 8000],\n",
       "       [ 226,   35, 1458, ...,  152,   47,   17],\n",
       "       [ 152,   29,   17, ..., 8000, 8000, 8000],\n",
       "       ...,\n",
       "       [1536,  146,   15, ..., 8000, 8000, 8000],\n",
       "       [  50,   85,   41, ..., 6322,    3, 6793],\n",
       "       [ 308,  151, 3661, ..., 8000, 8000, 8000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhớ rằng đầu ra có dạng như bài toán phân lớp, ứng với mỗi số chỉ 1 từ nào đó đầu vào, thì đầu ra là một vector xác suất có 8001 phần tử, xác suất tại vị trí nào lớn nhất thì từ ứng với vị trí đó được coi là dự đoán của mô hình. Đầu ra có dạng one-hot, vì vậy, ta cần chuyển các số ở y_pad sang các vector one-hot để khớp với mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (120, 8001) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-4dc267a74308>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_categorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_pad\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-4dc267a74308>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_categorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_pad\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m   \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m   \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (120, 8001) and data type int32"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_categorical = [to_categorical(idx, num_classes=len(word_to_index),  dtype='int32') for idx in y_pad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data lớn gây ra hiện tượng không thể đủ không gian bộ nhớ để chứa được ma trận y_categori. Để khắc phục điều này, chúng ta sau này sẽ thường generator data (tức là chia data thành từng batch nhỏ để đưa vào mô hình chứ không xử lý cả cục). Trong bài này, để đơn giản cho tiếp cận. Tôi sẽ lấy 1000 data để train demo thay vì lấy tất. Các project sau chúng ta sẽ tiếp cận với fit mô hình bằng generator sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "X_pad = X_pad[:1000]\n",
    "y_pad = y_pad[:1000]\n",
    "y_categorical = [to_categorical(idx, num_classes=len(word_to_index),  dtype='int32') for idx in y_pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo xây dựng mô hình và train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 120, 200)          1600200   \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 120, 120)          38520     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 120, 120)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120, 8001)         968121    \n",
      "=================================================================\n",
      "Total params: 2,606,841\n",
      "Trainable params: 2,606,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dropout, Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_to_index), output_dim=200, input_length=MAX_LEN))\n",
    "model.add(SimpleRNN(120, return_sequences=True))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(8001, activation='softmax'))\n",
    "model.compile('adam','categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/8\n",
      "1000/1000 [==============================] - 16s 16ms/step - loss: 7.2295 - categorical_accuracy: 0.2948\n",
      "Epoch 2/8\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 4.8764 - categorical_accuracy: 0.3309\n",
      "Epoch 3/8\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 4.7283 - categorical_accuracy: 0.3309\n",
      "Epoch 4/8\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 4.7187 - categorical_accuracy: 0.3309\n",
      "Epoch 5/8\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 4.7134 - categorical_accuracy: 0.3309\n",
      "Epoch 6/8\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 4.6840 - categorical_accuracy: 0.3309\n",
      "Epoch 7/8\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 4.5657 - categorical_accuracy: 0.3309\n",
      "Epoch 8/8\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 4.2566 - categorical_accuracy: 0.3548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x23cfabe3a88>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(X_pad), np.array(y_categorical), batch_size=32, epochs=8, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô hình hoạt động chưa thực sự tốt cho lắm! Vì vậy, các cải tiến của RNN đã ra đời sau này như GRU, LSTM... Chúng ta sẽ cùng tìm hiểu các mô hình này ở các bài viết sau."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
