{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nguồn tham khảo:\n",
    "* https://github.com/chiphuyen/stanford-tensorflow-tutorials/blob/master/examples/11_char_rnn.py\n",
    "* https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb\n",
    "* https://docs.google.com/presentation/d/1QydMhsGFeUzDYZr7dV0tt9WJryleOoKUyJth4ftRDbA/edit#slide=id.g1ce324a1fd_0_403"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này, chúng ta sẽ cùng tìm hiểu về RNN và LSTM trong Tensorflow.\n",
    "\n",
    "Trong Tensorflow, ngoài module keras thì Tensorflow còn hỗ trợ một loạt các cell (trong class `tf.nn.rnn_cell`):\n",
    "* **BasicRNNCell**: Cell cơ bản nhất của RNN\n",
    "* **RNNCell**: Đối tượng trừu tượng đại diện cho một cell RNN\n",
    "* **BasicLSTMCell**: Cell mạng hồi quy LSTM cơ bản (https://arxiv.org/pdf/1409.2329.pdf)\n",
    "* **LSTMCell**: Cell LSTM\n",
    "* **GRUCell**: Cell GRU\n",
    "\n",
    "Để biết thêm chi tiết, các bạn có thể tham khảo thêm document của Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này, chúng ta sẽ cùng xây dựng mạng LSTM để phân loại chữ số viết tay trên bộ dữ liệu MNIST. Tương tự có thể làm với RNN. Dataset gồm 60000 mẫu để training và 10000 mẫu để test. Các ảnh số có kích thước chuẩn là 28x28 pixels với giá trị đã được chuẩn hoá từ 0 đến 1. Để đơn giản, mỗi ảnh đã được \"duỗi thẳng\" và thành mảng numpy 1 chiều 784 đặc trưng (28 * 28).\n",
    "\n",
    "![digit](./images/digits.png)\n",
    "\n",
    "Để phân lớp sử dụng mạng hồi quy LSTM, chúng ta xem mỗi hàng của ảnh như 1 chuỗi các pixel. Nởi vì chiều của các ảnh MNIST là 28 * 28px, tôi sẽ xử lý 28 chuỗi của 28 timestep cho mỗi mẫu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-e3bb87aa45ca>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tham số training\n",
    "learning_rate = 0.001\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Tham số mạng\n",
    "num_input = 28 # Mnist data input (img shape: 28*28)\n",
    "timesteps = 28\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10\n",
    "\n",
    "# tf Graph\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "def LSTM(x, weights, biases):\n",
    "    # prepare data shape to match 'LSTM' function requirements\n",
    "    # current data input shape: (batch_size, timesteps, n_input)\n",
    "    # required shape: 'timestep' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # unstack to get a list of 'timestep' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "    \n",
    "    # define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    \n",
    "    # get LSTM cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "    \n",
    "    # linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-f704c7496a68>:11: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-f704c7496a68>:14: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-5-a0ce98c37919>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = LSTM(X, weights, biases)\n",
    "prediction  = tf.nn.softmax(logits)\n",
    "\n",
    "# define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# initialize the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch loss=2.4365, Traning accuracy = 0.172\n",
      "Step 200, Minibatch loss=1.9859, Traning accuracy = 0.398\n",
      "Step 400, Minibatch loss=1.8468, Traning accuracy = 0.445\n",
      "Step 600, Minibatch loss=1.8170, Traning accuracy = 0.383\n",
      "Step 800, Minibatch loss=1.7426, Traning accuracy = 0.383\n",
      "Step 1000, Minibatch loss=1.5940, Traning accuracy = 0.477\n",
      "Step 1200, Minibatch loss=1.5104, Traning accuracy = 0.516\n",
      "Step 1400, Minibatch loss=1.5210, Traning accuracy = 0.516\n",
      "Step 1600, Minibatch loss=1.4317, Traning accuracy = 0.555\n",
      "Step 1800, Minibatch loss=1.2745, Traning accuracy = 0.633\n",
      "Step 2000, Minibatch loss=1.3996, Traning accuracy = 0.531\n",
      "Step 2200, Minibatch loss=1.2006, Traning accuracy = 0.641\n",
      "Step 2400, Minibatch loss=1.2262, Traning accuracy = 0.586\n",
      "Step 2600, Minibatch loss=1.1255, Traning accuracy = 0.695\n",
      "Step 2800, Minibatch loss=1.1237, Traning accuracy = 0.664\n",
      "Step 3000, Minibatch loss=1.1135, Traning accuracy = 0.625\n",
      "Step 3200, Minibatch loss=1.0435, Traning accuracy = 0.734\n",
      "Step 3400, Minibatch loss=1.0611, Traning accuracy = 0.672\n",
      "Step 3600, Minibatch loss=0.9248, Traning accuracy = 0.734\n",
      "Step 3800, Minibatch loss=0.9669, Traning accuracy = 0.727\n",
      "Step 4000, Minibatch loss=0.9994, Traning accuracy = 0.672\n",
      "Step 4200, Minibatch loss=0.8231, Traning accuracy = 0.766\n",
      "Step 4400, Minibatch loss=0.9590, Traning accuracy = 0.664\n",
      "Step 4600, Minibatch loss=0.8340, Traning accuracy = 0.766\n",
      "Step 4800, Minibatch loss=0.9104, Traning accuracy = 0.742\n",
      "Step 5000, Minibatch loss=0.7432, Traning accuracy = 0.727\n",
      "Step 5200, Minibatch loss=0.5927, Traning accuracy = 0.812\n",
      "Step 5400, Minibatch loss=0.7037, Traning accuracy = 0.820\n",
      "Step 5600, Minibatch loss=0.7487, Traning accuracy = 0.773\n",
      "Step 5800, Minibatch loss=0.6570, Traning accuracy = 0.711\n",
      "Step 6000, Minibatch loss=0.6499, Traning accuracy = 0.789\n",
      "Step 6200, Minibatch loss=0.5892, Traning accuracy = 0.789\n",
      "Step 6400, Minibatch loss=0.6515, Traning accuracy = 0.820\n",
      "Step 6600, Minibatch loss=0.7962, Traning accuracy = 0.758\n",
      "Step 6800, Minibatch loss=0.6400, Traning accuracy = 0.805\n",
      "Step 7000, Minibatch loss=0.6555, Traning accuracy = 0.805\n",
      "Step 7200, Minibatch loss=0.6254, Traning accuracy = 0.805\n",
      "Step 7400, Minibatch loss=0.5351, Traning accuracy = 0.852\n",
      "Step 7600, Minibatch loss=0.5268, Traning accuracy = 0.859\n",
      "Step 7800, Minibatch loss=0.4948, Traning accuracy = 0.852\n",
      "Step 8000, Minibatch loss=0.6375, Traning accuracy = 0.828\n",
      "Step 8200, Minibatch loss=0.5149, Traning accuracy = 0.820\n",
      "Step 8400, Minibatch loss=0.4308, Traning accuracy = 0.891\n",
      "Step 8600, Minibatch loss=0.4352, Traning accuracy = 0.867\n",
      "Step 8800, Minibatch loss=0.5855, Traning accuracy = 0.836\n",
      "Step 9000, Minibatch loss=0.6012, Traning accuracy = 0.820\n",
      "Step 9200, Minibatch loss=0.4538, Traning accuracy = 0.867\n",
      "Step 9400, Minibatch loss=0.5893, Traning accuracy = 0.789\n",
      "Step 9600, Minibatch loss=0.4448, Traning accuracy = 0.852\n",
      "Step 9800, Minibatch loss=0.4130, Traning accuracy = 0.906\n",
      "Step 10000, Minibatch loss=0.3693, Traning accuracy = 0.883\n",
      "optinmization Finised!\n",
      "Testing accuracy: 0.890625\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # run initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "            print(\"Step {}, Minibatch loss={:.4f}, Traning accuracy = {:.3f}\".format(step, loss, acc))\n",
    "    print(\"optinmization Finised!\")\n",
    "    \n",
    "    # calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing accuracy:\", sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
