{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giống như SVM, **Decision Tree** là thuật toán linh hoạt trong Machine Learning áp dụng được cho cả bài toán phân lớp và hồi quy. Chúng là một thuật toán mạnh mẽ, phù hợp với các datasets phức tạp. Ví dụ, trong chương 2, bạn đã train một mô hình **DecisionTreeRegressor** trên dữ liệu California housiing, fitting chúng hiệu quả.\n",
    "\n",
    "Decision Tree cũng là thành phần cơ bản của **Random Forest** (xem Chapter 7), đó là một trong những thuật toán Machine Learning mạnh mẽ.\n",
    "\n",
    "Trong chương này, chúng tôi sẽ bắt đầu thảo luận làm thế nào để train, mô phỏng (visualize) và dự đoán (predictions) với Decision Trees. Và rồi chúng tôi sẽ đi qua bài toán training CART sử dụng Scikit-learn, và chúng tôi sẽ thảo luận như thế nào là *regularize trees* và sử dụng chúng cho các bài toán hồi quy. Cuối cùng chúng tôi sẽ thảo luận một vài hạn chế của Dicision Trees.\n",
    "\n",
    "# Training and Visualizing a Decision Tree\n",
    "Để hiểu Decision Trees, cùng bắt đầu build nó và dùng nó để dự đoán xem như thế nào. Theo dõi code train một **DeicisionTreeClassifier** trên dataset iris: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # chiều dài và rộng cánh hoa\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn có thể mô phỏng (visualize) Decision Tree đã train bằng cách sử dụng phương thức **export_graphviz()** để output một đồ thị và lưu trong file *iris_tree.dot*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "f = open('iris_tree.dot', 'w')\n",
    "export_graphviz(tree_clf,\n",
    "               out_file=f,\n",
    "               feature_names=iris.feature_names[2:],\n",
    "               class_names=iris.target_names,\n",
    "               rounded=True,\n",
    "               filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó bạn có thể convert file *.dot* này thành một định dạng như PDF hoặc PNG sử dụng dot command-line tool từ packet *graphviz*. \n",
    "\n",
    "`dot -Tpng iris_tree.dot -o iris_tree.png`\n",
    "\n",
    "Hoặc chúng ta cũng có thể convert online, ví dụ như sử dụng trang web này: https://onlineconvertfree.com/convert-format/dot-to-png/\n",
    "\n",
    "Sau khi convert chúng ta thu được hình sau:\n",
    "\n",
    "<img src=\"./iris_tree.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhìn hình bên trên để dự đoán. Giả sử rằng bạn tìm một hoa iris và bạn muốn phân lớp nó. Bạn bắt đầu tại *root node* (depth 0, trên đỉnh): node này yêu cầu độ dài (length) của hoa pental là nhỏ hơn 2.45cm. Nếu nó đúng, thì bạn di chuyển xuống node dưới bên trái gọi là con trái, còn sai thì sang bên node phải gọi là con phải. Tương tự như vậy, bạn có thể dự đoán với 1 input hoa iris. Các node lá chính là các class. (setosa, versicolor, virginnica).\n",
    "\n",
    "Một trong những tính tốt của Decision Tree là chúng yêu cầu rất ít dữ liệu. Đặc biệt là thuật toán này không yêu câu các đặc trưng scaling hoặc centering cho tất cả dữ liệu.\n",
    "\n",
    "Nhìn sơ đồ trên, ta thấy có 100 mẫu có độ dài cánh hoa lớn hơn 2.45, và 50 mẫu có độ dài cánh hoa nhỏ hơn 2.45 và xếp vào class sentosa. Tiếp đí, với 100 mẫu kia thì lại có 54 mẫu được xếp vào class versicoloer và 46 mẫu vào class virginica. Tham số *gini* cho biết sự bất ổn, ví dụ gini=0 cho thấy sự thuần tuý của class đó, tức là tất cả các mẫu thuộc về cùng lớp đó. Nút bên trái độ sâu 2 tính như sau: $1-\\frac{0}{54}^{2}-\\frac{49}{54}^{2}-\\frac{5}{54}^2=0.168$\n",
    "\n",
    "Công thức tính $G_i$ của node thứ i như sau:\n",
    "\n",
    "$G_i=1-\\sum_{k=1}^{n} p_{i,k}^{2}$\n",
    "\n",
    "trong đó $p_{i,k}$ là ratio (tỷ lệ) mẫu của class $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: Scikit-learn sử dụng thuật toán CART, chỉ sử dụng cây nhị phân: Các node không phải lá luôn luôn có 2 con (chỉ trả lời câu hỏi có yes/no). Tuy nhiên, các thuật toán khác như là ID3 có thể đưa Decision Trees với các node có nhiều hơn 2 con."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hình dưới đây cho thấy biên của thuật toán Decision Tree bên trên.\n",
    "\n",
    "<img src=\"./images/boun.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation: White Box Versus Black Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như bạn đã thấy Decision Trees là khá trực quan và rất dễ dàng diễn giải. Những mô hình này thường được goại là *while box models*. Ngược lại, Random forests hoặc neural networks nói chung là *black box models*. Chúng đưa ra các dự đoán và bạn có thể dễ dàng kiểm tra kết quả mà chúng đã thực hiện để đưa ra dự đoán này, nó luôn luôn khó để giải thích với các quy tắc đơn giản là dự đoán đã thực hiện như thế nào. Ví dụ, nếu một mạng neural nói rằng một người cụ thể xuất hiện trong một bức tranh, nó là khó để biết những gì góp phần làm nên dự đoán này: nhận ra mắt người chưa? mồm cô ấy? mũi? giày và cô ấy ngồi hay đứng? Ngược lại, Decision Trees thường tốt và có các quy tắc phân lớp dơn giản."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esimating Class Probabilities\n",
    "Một Decision Tree có thể cũng ước lượng xác xuất thuộc về một class $k$: đầu tiên nó đi qua cây để tìm các node lá, và rồi nó trả về ratio (tỷ lệ, hệ số) training của class k trong node. Ví dụ. giả sử bạn có một hoa với kích cỡ cánh hoa dài 5cm, và rộng 1.5cm. Thuật toán trả về node lá ở độ sâu là 2 bên trái, vậy nên Decision Tree nên output xác xuất: 0% cho Iris-Setosa (0/54), 90.7% cho Iris-Versicolor (49/54), và 9.3% cho Iris-Virginica (5/54). Và tất nhiên nếu như bạn cần dự đoán class, nó nên output Iris-Versicolor từ xác suất cao nhất. Cùng check điều này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CART Training Algorithm\n",
    "Scikit-Learn sử dụng thuật toán *Classification And Regression Tree* (CART) để train Decision Trees (cũng gọi là \"growing\" trees. Ý tưởng thực chất đơn giản: thuật toán split tập training thành 2 tập con sử dụng một đặc trưng $k$ với một ngưỡng (threshold) $t_k$ (Ví dụ độ dài cánh hoa <= 2.45 cm). Vậy làm thế nào để chọn $k$ và $t_k$? Nó tìm kiếm các cặp $(k,t_k)$. Hàm chi phí là cực tiểu hàm dưới đây.\n",
    "\n",
    "$J(k,t_k)=\\frac{m_{\\text{left}}}{m} G_{\\text{left}}+\\frac{m_{\\text{right}}}{m}G_{\\text{right}}$\n",
    "\n",
    "Với $G_{\\text{left/right}}$ đo lường sự ô uế của tập con trái/phải. <br/>\n",
    "$m_{\\text{left/right}}$ là số trường hợp ở tập con trái/phải.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một khi nó đã cắt thành công tập training thành 2 phần, nó tiếp tục cắt tiếp các tập con sử dụng logic tương tự, rồi các tập con của tập con, và cứ như vậy, đệ quy. Nó dừng đệ quy chỉ khi đạt độ sâu max (định nghĩa bởi tham số **max_depth**), hoặc nó có thể tìm một các cắt mà phân lớp thành công. Một số các tham số khác điều khiển luồng dừng điều kiện như là: **min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_leaf_nodes)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP:** Như bạn thấy, thuật toán CART là một thuậ toán tham lam: Nó tìm kiếm một cách split tối ưu nhất tại level cao nhất, rồi lặp lại quá trình này tại mỗi level. Nó không kiểm tra split dẫn đến việc có thể tạo ra rất nhiều tầng. Một thuậ toán tham lam thường tạo ra một giải pháp hợp lý tốt, nhưng nó không phải là giải pháp tối ưu.\n",
    "\n",
    "Thật đáng tiếc, tìm cây tối ưu là một bài toán NP-Complete: Nó yêu cầu độ phức tạp O(exp(m), đây là vấn đề khó kể cả cho tập training nhỏ. Điều này là lý do tại sao chúng ta phải tìm một giải pháp hợp lý hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Complexity (Độ phức tạp tính toán)\n",
    "Dự đoán đòi hỏi đi qua Decision Tree từ node gốc tới lá. Decision Tree là nói chung là cân bằng, vì vậy, duyệt một Decision Tree cần qua $O(log_2(m))$ node. Mỗi node chỉ yêu cầu check giá trị của một đặc trưng, độ phức tạp tổng là $O(log_2(m))$, độc lập với số đặc trưng. Do vậy, dự đoán là rất nhanh với tập training set lớn.\n",
    "\n",
    "Tuy  nhiên, thuật toán training so sánh tất cả các cặp trên mẫu tại mỗi node. Kết quả độ phức tạo training là O(nxmlog(m)). Với các tập training nhỏ (ít hơn vài nghìn trường hợp), Scikit-Learn có thể training nhanh hơn bởi presorting data (sắp xếp data) (set *presort=True*), nhưng điều này giảm tốc độ training cho data lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini Impurity or Entropy?\n",
    "Mặc định, Gini được sử dụng để đo nhiễu, nhưng bạn có thể chọn *entropy* bằng cách cài đặt tham số **criterion** là **entropy**. Khái niệm entropy gốc trong nhiệt động lực học như một sự đo rối loạn phân tử: entropy tiến đến 0 khi phân tử đứng im hoặc sắp xếp tốt. Sau đó lan sang nhiều lĩnh vực, bao gồm Lý thuyết thông tin Shanon, tại đây nó đo thông tin trung bình nội dung của một tin nhắn: entropy là 0 khi tất cả các tin nhắn là đúng. Trong Machine Learning, nó thường sử dụng để đo sự pha tạp: một tập entroypy là một khi nó chỉ bao gồm các phần tử một class. Công thức dưới đây cho thấy định nghĩa của entropy node thứ i:\n",
    "\n",
    "$H_i=-\\sum_{k=1}^{n}p_{i,k}log_2(p_{i,k})$\n",
    "\n",
    "Vậy nên sử dụng Gini hay entropy? Sự thật là, hầu hết là chúng không tạo sự khác biệt lớn. Gini so sánh nhanh hơn, vì vậy nó là mặc định. tuy nhiên, khi chúng khác nhau, Gini có xu hướng cô lập các điểm tạp chất trong nhánh riêng của cây, trong khi đó entropy có xu hướng sản sinh ít nhánh cây hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularrization Hyperparameters\n",
    "... *Nói về vấn đề overfiting nếu không giới hạn độ sâu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "Decision Trees có thể xử lý các tác vụ hồi quy. Build một regression tree sử dụng class Scikit learn **DecisionTreeGressor**, training nó với *max_depth=2*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=None, splitter='best')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "f = open('iris_tree_regress.dot', 'w')\n",
    "export_graphviz(tree_reg,\n",
    "               out_file=f,\n",
    "               feature_names=iris.feature_names[2:],\n",
    "               class_names=iris.target_names,\n",
    "               rounded=True,\n",
    "               filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/tree.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cây này rất giống với cây phân lớp mà chúng ta đã xây dựng ở trên. Cái khác chính là thay vì giá trị dự đoán một class tại mỗi node, nó dự đoán một giá trị. Ví dụ, giả sử rằng bạn muốn dự đoán cho một giá trị mới với $x_1=0.6$. Bạn duyệt cây bắt đầu từ node gốc, và cuối cùng bạn ra tới nút lá để dự đoán giá trị *value=0.111*. Dự đoán này đơn giản là trung bình các value của 110 mẫu training ở nút lá đó. Dự đoán này trả về một MSE bằng 0.015 cho 100 mẫu.\n",
    "\n",
    "Dự đoán của model được biểu diễn như hình dưới đây. Nếu bạn đặt **max_depth=3**, bạn biểu diễn mô hình dự đoán như bên phải. Thuật toán split mỗi vùng theo một cách mà làm cho hầu hết các trường training càng gần càng tốt với giá trị dự đoán.\n",
    "\n",
    "<img src=\"./images/reg_tree.png\"/>\n",
    "\n",
    "Thuật toán CART làm việc chủ yếu theo cùng một cách như trước, ngoại trừ việc thay vì phân chia dữ liệu training một cách tối thiểu nhất, nó bây giờ cố gắng chia tập training trong một cách giảm thiểu MSE. Công thức hàm chi phí thuật toán cố gắng cực tiểu hoá như sau:\n",
    "\n",
    "$J(k,t_k)=\\frac{m_{\\text{left}}}{m} MSE_{\\text{left}}+\\frac{m_{\\text{right}}}{m} MSE_{\\text{right}}$\n",
    "\n",
    "Với: <img src=\"./images/ct.png\"/>\n",
    "\n",
    "Giống như bài toán phân lớp, Decision Tree có xu hướng overfitting khi chia nhỏ với nhiều tác vụ hồi quy. Nếu không regularization (quy tắc) (Ví dụ sử dụng tham số mặc định), bạn sẽ dự đoán như bên trái hình dưới đây. Nó rõ ràng là overfitting rất xấu. Nếu chỉ setting **min_samples_leaf=10** kết quả model hợp lý hơn, như bên phải hình dưới đây.\n",
    "\n",
    "<img src=\"./images/over.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Instability (Tính không ổn định)\n",
    "Hy vọng rằng bây giờ bạn đang thuyết phục rằng Decision tree có nhiều dự định cho chúng: Chúng đơn giản để hiểu và làm sáng tỏ, dễ dàng để sử dụng, linh hoạt và mạnh mẽ. Tuy nhiên, chúng có một số hạn chế. Đầu tiên, bạn có thể để ý, Decison Tree yêu thích ranh giới trực giao (tất split theo một trực toạ độ), điều này nhạy cảm với bộ dữ liệu xoay vòng. Ví dụ, hình dưới đây: bên trái, một cây Decision có thể split dễ dàng, trong khi bên phải, sau khi xoay 45 độ, biên decision trông có vẻ phức tạp. Mặc dù cả 2 Cây Decision fit với tập training, rất có khả năng là mô hình bên phải sẽ không khái quát tốt. Một cách hạn chế điều này là sử dụng PCA (xem chương 8), cái mà thường trả về một định hướng tốt hơn trong dữ liệu train.\n",
    "\n",
    "<img src=\"./images/rota.png\"/>\n",
    "\n",
    "Tổng quát hơn, vấn đề chính với Decision Tree là chúng rất nhạy cảm với biến động nhỏ trong tập training. Ví dụ, nếu bạn xoá widest Iris-Versicolor từ tập dataset và train Decision Tree, ạn có thể thấy model biểu diễn như hifh dưới đây, bạn sẽ thấy sự khác nhau so với mô hình chúng ta đã train. Hiện tại, từ thuật toán training sử dụng Scikit-Learn là stochastic, bạn có thể lấy model khác nhau mỗi mẫu training (trừ khi bạn cài đặt tham số **random_state**)\n",
    "\n",
    "<img src=\"./images/depth.png\"/>\n",
    "\n",
    "Ramdom forest có thể hạn chế sự bất ổn này bằng cách dự đoán trung bình với nhiều cây, bạn sẽ tìm hiểu về chúng trong chương tiếp theo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Độ sâu gần đúng của một Decision Tree (không hạn chế) trên tập training với một triệu trường là bao nhiêu?\n",
    "2. Một Gini node impurity nói chung thâp hay cao hơn cha của nó? Thường thấp/cao hơn? hay là luôn luôn thấp/cao hơn?\n",
    "3. Nếu một Decision Tree overfitting trên tập training, có một ý tưởng tốt là giảm max_depth?\n",
    "4. Nếu một Decision Tree underfitting trên tập training, có một ý tưởng tốt là cố gắng scaling các đặc trưng input?\n",
    "5. Nếu phải mất một giờ để train một Decision Tree trên một tập train gồm 1 triệu đặc trưng, mất khoảng bao nhiêu thời gian để train trên tập dữ liệu gồm 10 triệu đặc trưng?\n",
    "6. Nếu tập train của bạn bao gồm 100000 đặc trưng, sẽ setting tham số *presort=True* có làm tăng tốc độ train?\n",
    "7. Train và fine-tune một Decision Tree cho dataset mặt trăng. <br/>\n",
    "a. Sinh một dataset mặt trưng sử dụng *make_moons(n_samples=10000, noise=0.4)*. <br/>\n",
    "b. Split nó vào tập train và test sử dụng *train_test_split()*.\n",
    "c. Sử dụng grid search với cross-validation (Với sự hỗ trợ của class *GridSearchCV*) để tìm các tham số tốt cho một *DecisionTreeClassifer*. Gợi ý: cố gắng thử các giá trị khác nhau cho *max_leaf_nodes*.<br/>\n",
    "8. Grow a forest.<br/>\n",
    "a. Tiếp nối bài trước, sinh 1000 tập con của training set, mỗi tập gồm 100 đặc trưng được chọn ngẫu nhiên. Gợi ý: Dùng *ShuffleSplit*. <br/>\n",
    "b. Train một Decision Tree cho mỗi tập con, sử dụng các tham số tốt nhất đã tìm ở bài trên. Tính toán 1000 Decision tree trên tập test. Thấy rằng, chúng được train trên các tập nhỏ, các Decision Tree sẽ có hiệu suất tốt hơn một Decision Tree, đạt được độ chính xác khoảng 80%. <br/>\n",
    "c. Bây giờ cùng làm ảo thuật nào. Với mỗi tập test set, sinh dự đoán của 1000 Decision Tree, và chỉ giữ dự đoán có tần suất cao nhất (bạn có thể dử dụng hàm Sipy's *mode*). Điều này cho bạn *majority-vote predictions* qua test set.<br/>\n",
    "d. Tính toán các dự đoán trên tập test: bạn đạt được một độ chính xác cao hơn ở mô hình của bạn (khoảng 0.5 tới 1.5%). Chúc mừng, bạn đã trained một phân lớp **Random Forest**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
