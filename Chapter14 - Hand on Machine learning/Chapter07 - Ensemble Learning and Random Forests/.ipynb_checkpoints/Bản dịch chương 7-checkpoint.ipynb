{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử bạn hỏi một câu hỏi phức tạp cho hàng nghìn người, rồi kết hợp các câu trả lời của họ. Trong nhiều trường hợp bạn sẽ thấy các câu trả lời kết hợp là tốt hơn một câu trả lời của một chuyên gia. Điều này gọi là *wisdom of crowd* (trí tuệ của đám đông). Cũng giống như vậy, nếu bạn kết hợp các dự đoán của một nhóm dự đoán (như là phân lớp hoặc hồi quy), bạn sẽ thường lấy các dự đoán tốt hơn với nhiều bình chọn nhất. Một nhóm dự đoán (predictors) được gọi là một *ensemble*; Như vậy, kỹ thuật này gọi là *Ensemble Learning*, và một thuật toán Ensemble learning được gọi là một *Ensemble Method*.\n",
    "\n",
    "Ví dụ, bạn có thể train một nhóm các Decision Tree phân lớp, trên mỗi một tập con ngẫu nhiên của training set. Khi dự đoán, bạn thu được tất cả các dự đoán của tất cả các cây, rồi dự đoán class mà có bình chọn nhiều nhất (xem chương 6). Như vậy một ensemble của Decision Tree gọi là một *Random Forest*, và mặc dù nó đơn giản, nhưng nó là một trong những thuật toán mạnh mẽ nhất trong Machine learning hiện nay.\n",
    "\n",
    "Tuy nhiên, như chúng ta thảo luận ở chương 2, bạn sẽ thường sử dụng Ensemble method gần cuối project, bạn đã thực sự xây dựng một vài dự đoán tốt, tổ hợp chúng lại thành một dự đoán tốt hơn. Trong thực tế, các giải pháp chiến thắng trong Machine Learning thường liên quan đến các phương pháp Ensemble.\n",
    "\n",
    "Trong chương này, chúng tôi sẽ thảo một các phương pháp Ensemble phổ biến nhất, bao gồm *bagging, boosting, stacking* và một số phương pháp khác. Chúng ta sẽ khám phá *Random Forests.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử rằng bạn đã train một vài phương pháp phân lớp, mỗi một phương pháp đạt độ chính xác khoảng 80%. Bạn có thể dùng Logistic Regression, SVM, Random Forest, K-Nearest Neightbors, và vân vân...\n",
    "\n",
    "<img src=\"./images/classifier.png\"/>\n",
    "\n",
    "Một phương pháp đơn giản là tổng hợp những dự đoán của các classifier và dự đoán class được nhiều vote nhất. Các vote phần đông này gọi là *hard voting*.\n",
    "\n",
    "<img src=\"./images/vote.png\"/>\n",
    "\n",
    "Hơi ngạc nhiên 1 chút, nhưng classifier voteing này thường đạt được độ chính xác cao hơn classifier tốt nhất mà ta dùng. Thực tế, nếu mỗi classifier là một *weak learner* (nghĩa là nó chỉ tốt hơn chút so với đoán bừa), thì toàn bộ có thể là một *strong learner* (độ chính xác cao), với điều kiện là đủ số lượng của các weak learner và chúng gồm nhiều loại khác nhau.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Làm thế nào để được điều này? Điều tương tự sau đây có thể làm sáng tỏ màu nhiệm này. Giả sử bạn có một đồng xu thiên vị mà có cơ hội là 51% ngửa và 49% úp. Nếu bạn tung nó 100 lần, bạn sẽ thường nhận được nhiều hơn hoặc ít hơn 510 ngửa và 490 úp, và phần lớn là ngửa. Nếu bạn sử dụng toán học, bạn sẽ tìm ra xác suất thu được các măt ngửa sau 1000 lần tung là 75%. Bạn càng quăng đồng xu, xác suất này càng cao (với 10000 lần thì khoảng 97%). Điều này là do *law of large number* (luật số lớn). Nếu bạn tiếp tục tung đồng xu thì tỷ lệ sẽ tiến gần đến 51%.  Hình dưới đây cho thấy khi tung loạt 10 đồng xu thiên vị.\n",
    "\n",
    "<img src=\"./images/coin.png\"/>\n",
    "\n",
    "Cũng giống như vậy, giả sử bạn xây dựng một ensemble gồm 1000 classifier, mỗi cái chỉ đúng khoảng 51% (chỉ hơn random 1 tý). Nếu bạn dự đoán các vote, bạn có thể hy vọng độ chính xác cao hơn 75%! Tuy nhiên, điều này chỉ đúng nếu tất cả các classifier là độc lập, không có tương quan với nhau, mà điều này rõ ràng không phải nếu như đào tạo trên dữ liệu giống nhau. Chúng có khả năng làm cho các loại cùng 1 lỗi, vì vậy sẽ có đa số sai, giảm độ chính xác.\n",
    "\n",
    "**TIP:** Phương pháp Ensemble làm việc tốt nhất khi gi thiết đều độc lập với nhau. Phương pháp gồm nhiều loại classifier khác nhau để train, chúng sử dụng các thuật toán khác nhau. Điều này làm tăng cơ hội mà chúng sẽ làm cho tất cả các loại khác nhau của lỗi, cải thiện độ chính xác.\n",
    "\n",
    "Theo dõi code tạo và train một voting classifier trong Scikit-Learn, sử dụng 3 classifier sau: (train trên dataset moon (dạng mặt trăng)) (Chương 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('cf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     cr...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=None,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=False, random_state=None,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('cf', rnd_clf), ('svc', svm_clf)],\n",
    "                             voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8336666666666667\n",
      "RandomForestClassifier 0.8513333333333334\n",
      "SVC 0.8633333333333333\n",
      "VotingClassifier 0.862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phân loại do voting có độ chính xác cao hơn so với phân loại của các classifier.\n",
    "\n",
    "Nếu tất cả các classifier có thể ước lượng xác suất lớp (ví dụ chúng đều có phương thức *predict_proba()*), thì bạn có thể gọi Scikit-Learn để dự đoán class với xác suất lớp cao nhất, trung bình qua tất cả các classifier. Điều này gọi là *soft voting*. Nó thường đạt được hiệu suất cao hơn là hard voting vì nó cung cấp cả weight cho vote. Tất cả bạn cần là thay **voting='hard'** thành **voting=='sort'** và bảo đảm rằng tất cả các classifier có thể ước lượng xác suất class. Điều này không đúng trong trường hợp SVC mặc định, vì nó cần set tham số *probability* của nó là *True* (Điều này sẽ làm cho class SVC sử dụng class-validation để ước lượng xác suất lớp, làm chậm training, và nó sẽ thêm một phương thức *predict_proba()*). Nếu bạn sửa đổi code trước sử dụng soft voting, bạn sẽ thấy độ chính xác cao hơn nữa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8336666666666667\n",
      "RandomForestClassifier 0.8473333333333334\n",
      "SVC 0.8633333333333333\n",
      "VotingClassifier 0.8643333333333333\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('cf', rnd_clf), ('svc', svm_clf)],\n",
    "                             voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một cách để có được một tập phân loại là sử dụng các thuật toán training khác nhau, như đã thảo luận. Ngoài ra, sử dụng thuật toán training giống nhau cho mỗi predictor, nhưng train chúng trên các tập con ngẫu nhiên của tập training. Khi lấy mẫu biểu diễn với sự sắp đặt lại chỗ, phương pháp này gọi là **bagging** (tên ngắn cho *bootstrap aggegating*). Khi lấy mẫu biểu không sắp đặt lại chỗ, nó gọi là **pasting**.\n",
    "\n",
    "Nói cách khác, cả bagging và pasting cho phép các trường training được lấy mẫu trên nhiều dự đoán, nhưng chỉ bagging cho phép các trường training lấy mẫu nhiều lần cho các predictor tương tự. Sự lấy mẫu và tiến trình training biểu diễn như hình dưới.\n",
    "\n",
    "<img src=\"./images/bagging.png\"/>\n",
    "\n",
    "Khi tất cả các predictor đã train, ensemble có thể dự đoán một mẫu mới bằng cách kết hợp đơn giản các dự đoán của tất cả các predictor. Hàm kết hợp tiêu biểu là *statistical mode* (dự đoán phổ biến nhất, giống như hard voting classifier) cho phân lớp, hoặc trung bình cho hồi quy. Mỗi predictor riêng có một bias cao hơn nếu nó đã train trên tập dữ liệu góc, nhưng kết hượp giảm cả bias và variance. Nói chúng, mạng lưới trả về là ensemble có bias giống nhưng phương sai thấp hơn predictor đơn trên dữ liệu train gốc.\n",
    "\n",
    "Như bạn có thể nhìn hình trên, các predictor có thể tất cả cùng train song song, trên các core CPU hoặc server khác nhau. Giống như vậy, dự đoán có thể là song song. Điều này là lý do tại sao bagging và pasting là phương pháp phổ biến: chúng scale rất tốt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting in Scikit-Learn\n",
    "Scikit-Learn thường dùng một API đơn giản cho cả bagging và pasting với class **BaggingClassifier** (hoặc **BaggingRegressor** cho hồi quy). Theo dõi code tran một ensemble của 500 Decision tree sau, mỗi trained trên 100 mẫu training tuỳ ý từ tập training với replacement (đặt lại chỗ) (Đây là một ví dụ bagging, nếu bạn muốn sử dụng pasting, set **bootstrap=False**). Tham số **n_jobs** gọi số core CPU sử dụng để train và dự đoán (-1 thì sẽ gọi tất cả các core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hình dưới đây cho thấy sự khác nhau nếu dùng một Decision Tree (trái) với bagging ensemble của 500 cây (phải)\n",
    "\n",
    "<img src=\"./images/vs.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Bag Evaluation\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*BaggingClassifier* hỗ trợ lấy các đặc trưng tốt. Sự điều khiển này bởi 2 thám số **max_features** và **bootstrap_features**. Chúng làm việc giống **max_samples** và **bootstrap**, nhưng đối với feature sampling thay vì instance sampling. Do vậy, mỗi predictor sẽ train trên một tập con của input features.\n",
    "\n",
    "Điều này đặc biệt hữu ích khi bạn xử lý với input nhiều chiều (như là ảnh). Sampling và cả training instances và features gọi là **Random Patches method**. giữ tất cả các thí dụ training (ví dụ bootstrap=False và max_samples=1.0) nhưng sampling feature (ví dụ bootstrap_features=True và/hoặc max_features nhỏ hơn 1.0) gọi là **Random Subspaces method**\n",
    "\n",
    "Sampling features trả kết quả đa dạng dự đoán nhiều hơn, trading một bỉ nhiều bias for phương sai nhỏ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như chúng ta đã thảo luận, một **Random Forest** là một ensemble (tập hợp) của Decision Tree, thường đã train qua phương pháp bagging (hoặc đôi khi là pasting), điển hình với *max_samples*cài đặt size của tập training. Thay vì xây dựng một **BaggingClassifier** và pasting nó một **DecisionTreeClassifier**, bạn có thể sử dụng **RandomForestClassifier**, cái mà thích hợp và tối ưu cho Decision Tree. (giống như vậy **RandomForestRegressor** cho bài toán hồi quy). Cùng theo dõi code train Random Forest với 500 cây (mỗi cây giới hạn tối đa 16 node), sử dụng tất cả core CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trừ một số ngoại lệ, một **RandomForestClassifier** có tất cả các tham số của **DecisionTreeClassifier**, và thêm tất cả các tham số của **BaggingClassifier** để điều khiển tập hợp cây trong chính nó.\n",
    "\n",
    "Thuật toán Random Forest giới thiệu thêm tính ngẫu nhiên khi growing trees; thay thế tìm kiếm cho các đặc trưng rất tốt khi split một node (xem chương 6), nó tìm kiếm các đặc trưng tốt nhất giữa một tập ngẫu nhiên các đặc trừng. Kết quả này hơn một cây, cái mà trades bias cao cho phương sai thấp, nói chung trả về một model tốt hơn. Theo dõi **BaggingClassifier** tương đương với **RandomForestClassifier** trước:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16), n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra-Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
