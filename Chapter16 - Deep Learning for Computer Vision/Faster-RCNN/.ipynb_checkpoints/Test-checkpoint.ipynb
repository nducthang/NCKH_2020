{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nguồn:\n",
    "- Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (Paper)\n",
    "* Deeplearning for computer vision with Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Abstract\n",
    "Kiến trúc các mạng object detect tuỳ theo thuật toán trên region proposal với giả thuyết object locations. Nâng cao như SPPnet và Fast R-CNN có giảm thời gian chạy của các mạng detection, phơi bày tính toán region proposal một nút cổ chai. Trong công việc này, chúng tối giới thiệu *Region proposal network (RPN)* mà có thể chia sẻ tất cả các đặc trưng convolution của ảnh với mạng detection,do đó cho phép giảm chi phí tính toán region proposal. Một RPN là một mạng fully convolutional mà đồng thời dự đoán bao đóng của đối tượng và đánh giá điểm số khách quan tại mỗi vị trí. RPN đã đươc train end-to-end để sinh các region proposal chất lượng cao, cái mà đã được sử dụng bởi Fast R-CNN cho detection. Với mội tối ưu xen kẽ đơn đản, RPN và Fast R-CNN có thể train để chia sẻ các convolution feature. Với mạng rất sâu VGG16, hệ thống detection của chúng tôi a frame rateof 5fps (bao gồm tất cả các bước) on a GPU, trong khi độ chính xác của object detection trên PASCAL VOC 2007 (73.2% mAP) và 2012 (70.4$ mAP) sử dụng 300 proposals per image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "Các tiến bộ mới trong object detection thúc đẩy sự thành công của các phương pháp region proposal và region-bases convolution neural network (R-CNN). Mặc dù region-based CNN tính toán tốn kém, chi phí của họ mạnh mẽ nhờ nhiều lớp convolution chia sẻ các proposal. Hiện thân mới nhất, Fast R-CNN, đạt được ́gần real-time sử dụng mạng rất sau, khi *bỏ qua thời gian dành cho đề xuất các vùng*. Bây giờ, các đề xuất là nút cổ chai tính toán trong kiến trúc hệ thống detection.\n",
    "\n",
    "Các phương pháp region proposal thường tin cậy trên các đặc trưng rẻ mạt :v (inexpensive) và chương trình suy luận kinh tế. Selective search, một trong số những phương pháp phổ biến nhất, trộn tham lam superpixel trên đặc trưng thiết kế mức thấp (low-level). Tuy nhiên, khi so sánh với các mạng phát hiện hiệu quả, Selective Search chậm hơn , khoảng 2s mỗi hình thực thi trên một CPU. EdgeBoxes hiện đang cung cấp sự cân bằng tốt nhất giữa chất lượng đề nghị và tốc độ, khoảng 0.2s mỗi ảnh. Tuy nhiên, bước đề nghị khu vực vẫn tiêu thụ càng nhiều thời gian chạy như mạng lưới phát hiện.\n",
    "\n",
    "Người ta có thể lưu ý rằng fast region-based CNN tận dụng lợi thế GPU, trong khi các phương pháp region proposal đã sử dụng lại nghiên cứu thực thi trên CPU, làm cho sự so sánh thời gian chạy như vậy không công bằng. Một cách rõ ràng để tăng tốc tính toán đề nghị là để tái thực hiện nó cho GPU. Điều này có thể một giải pháp kỹ thuật hiệu quả, nhưng thực hiện lại bỏ qua các mạng phát hiện xuống dòng (down-stream) và do đó bỏ lỡ cơ hội quan trọng để chia sẻ tính toán.\n",
    "\n",
    "Trong paper này, chúng tôi chỉ ra sự thay đổi thuật toán - tính toán proposal với deep net - dẫn đến một giải pháp thanh lịch và hiệu quả, tại đây proposal tính toán chi phí gần như ko cần cho tính toán các mạng phát hiện. Để kết thúc điều này, chúng tôi giới thiệu *Region Proposal Networks (RPN)* mà chia sẻ các layer convolution với kiến trúc mạng object detection. Bởi chia sẻ convolution tại các lần test, các chi phí cận biên cho đề xuất tính toán là nhỏ (ví dụ 10ms trên ảnh).\n",
    "\n",
    "Quan sát của chúng tôi là các map đặc trưng conv được sử dụng bởi region-based detectors, giống như Fast R-CNN, có thể sử dụng để sinh các region proposal. On top của các đặc truwng conv này, chúng tôi xây dựng RPN bằng cách thêm 2 layer conv: một cái để mãhoaoas mỗi conv map position thành một vector đặc trưng ngắn (ví dụ 246 chiều) , và cái thứ 2, mỗi conv map position, output một objectness score và regressed bounds cho $k$ region proposal tương ứng với quy mô khác nhau và các khía cạnh tỷ lệ tại địa điểm đó (k = 9 là một giá trị điển hình)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPN của chúng tôi là như vậy, một mạng fully-convolutional network (FCN) và chúng có thể trained end-to-end để là nhiệm vụ sinh các proposal. Để hợp nhất RPN với Fast R-CNN object detection network, chúng tôi đề nghị một chương trình đào tạo đơn giản giữa fine-tuning cho các nhiệm vụ region proposal và rồi fine-tuning cho object detection, trong khi giữa các proposal cố định. Đề án này hội tụ một cách nhanh chóng và tạo ra một mạng lưới thống nhất với các tính năng conv mà chia sẻ giữa hai nhiệm vụ.\n",
    "\n",
    "Chúng tôi tính toán phương pháp của chúng tôi trên tiêu chuẩn PASCAL VOC detection, tại đây RPN với Fast R-CNN phát hiện các sản phẩm chính xác tốt hơn so với ban đầu mạnh mẽ của Selective Search với Fast R-CNNs. Trong khi đó, phương pháp của chúng tôi khước từ gần như tất cả gánh nặng tính toán của SS tại kiểm tra thời gian, các hiệu quả hoạt động thời gian cho đề xuất chỉ là 10 mili giây. Sử dụng các mô hình rất sâu đắt\n",
    "của [19], phương pháp phát hiện của chúng tôi vẫn có một tỷ lệ khung hình của 5fps (bao gồm tất cả các bước) trên một GPU, và\n",
    "do đó là một hệ thống phát hiện đối tượng thực tế cả về tốc độ và độ chính xác (73,2% trên bản đồ\n",
    "VOC PASCAL năm 2007 và 70,4% trên bản đồ năm 2012). Mã có sẵn tại https://github.com/\n",
    "ShaoqingRen / faster_rcnn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Related work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Region proposal Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một Region Proposal Network (RPN) lấy một hình ảnh (kích thước bất kỳ) làm inpit và output là một tập các hình chữ nhật object proposal, với điểm số mỗi objectness (objectness = object classes + background). Chúng tôi mô hình các tiến trình này với một fully-convolutional network, cái mà chúng tôi mô tả trong section này. Bởi vì mục tiêu cuối cùng của chúng tôi là để chia sẻ tính toán với Fast R-CNN, chúng tôi giả sử rằng cả 2 mạng chia sẻ một tập các conv chung. Trong thực nghiệm của chúng tôi, chúng tôi điều tra các mô hình Zeiler và Fergus [23]\n",
    "(ZF), trong đó có 5 lớp conv có thể chia sẻ và Simonyan và Zisserman mô hình [19] (VGG), trong đó\n",
    "có 13 lớp conv có thể chia sẻ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để sinh các region proposal, chúng tôi slide một mạng nhỏ qua các conv feature map output bởi lớp conv chia sẻ cuối cùng. Mạng này là ột fully connected với một nxn không gian window của input conv feature map. Mỗi sliding window là ánh xạ tới một vector có số chiều thấp hơn. Vector này fed qua 2 sibling fully connected layer - một box regression layer (reg) và một box classification layer (cls). Chúng tôi sử dụng n=3 trong paper này, nhớ rằng các tiếp thu hiệu quả lĩnh vực đầu vào trên hình ảnh là lớn. Mạng neural nhỏ này minh hoạ trong hình 1 bên trái. Lưu ý rằng vì mini-mạng hoạt động\n",
    "trong một thời trang trượt cửa sổ, các lớp đầy đủ kết nối được chia sẻ trên tất cả các địa điểm không gian. Điều này\n",
    "kiến trúc được thực hiện một cách tự nhiên với một n × n lớp conv sau đó là hai anh em ruột 1 × 1 conv\n",
    "lớp (cho reg và cls, tương ứng). ReLUs [15] được áp dụng cho đầu ra của n × n lớp conv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation-Invariant Anchors\n",
    "Tại mỗi địa điểm trượt cửa sổ, chúng tôi đồng thời dự đoán đề xuất khu vực k, vì vậy lớp reg có 4k đầu ra mã hóa các tọa độ của hộp k. Lớp cls kết quả đầu ra 2k điểm rằng ước tính khả năng đối tượng / không-đối tượng cho mỗi proposal. Đề nghị k được tham số liên quan đến hộp tham khảo k, gọi là *anchors*. Mỗi *anchor* tập trung tại cửa sổ trượt trong câu hỏi, và là\n",
    "liên kết với một tỷ lệ quy mô và lĩnh vực. Chúng tôi sử dụng 3 quy mô và 3 hệ số co, năng suất k = 9 anchor tại mỗi vị trí trượt. Đối với một bản đồ tính năng conv của một kích thước W × H (thường ~2,400), sẽ có WHk anchor. Một tính chất quan trọng của cách tiếp cận của chúng tôi là nó là dịch bất biến (*translation invariant*), cả trong điều khoản của anchor và các hàm mà đề nghị tính toán tương đối so với anchor.\n",
    "\n",
    "Như một sự so sánh, phương pháp MultiBox [20] sử dụng k-means để tạo ra 800 anchor, mà không phải là dịch bất biến. Nếu một dịch một đối tượng trong một hình ảnh, đề nghị nên dịch và cùng chức năng nên có thể dự đoán đề xuất trong cả hai địa điểm. Hơn nữa, bởi vì\n",
    "MultiBox neo không phải là dịch bất biến, nó đòi hỏi một lớp đầu ra (4 + 1) × 800 chiều,\n",
    "trong khi phương pháp của chúng tôi đòi hỏi một lớp đầu ra (4 + 2) × 9 chiều. lớp đề nghị của chúng tôi có một trật tự\n",
    "độ lớn thông số ít (27 triệu cho MultiBox sử dụng GoogLeNet [20] vs 2,4 triệu cho\n",
    "RPN sử dụng VGG-16), và do đó có ít nguy cơ overfitting trên tập dữ liệu nhỏ, như VOC PASCAL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A los Function for Learning Region Proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Convolutional Features for Region Proposal and Object Detection\n",
    "Như vậy đến nay, chúng tôi đã mô tả làm thế nào để đào tạo một mạng cho thế hệ đề nghị khu vực, mà không xem xét việc phát hiện đối tượng khu vực dựa trên CNN rằng sẽ sử dụng những đề nghị. Để phát hiện\n",
    "mạng, chúng tôi áp dụng nhanh R-CNN [5] 4 và bây giờ mô tả một thuật toán mà nghe tin CONV lớp mà\n",
    "chia sẻ giữa các RPN và nhanh R-CNN.\n",
    "\n",
    "Cả hai RPN và nhanh R-CNN, được đào tạo một cách độc lập, sẽ sửa đổi các lớp conv của họ theo những cách khác nhau.\n",
    "Do đó chúng tôi cần phát triển một kỹ thuật cho phép chia sẻ các lớp conv giữa hai mạng, chứ không phải là học hai mạng riêng biệt. Lưu ý rằng đây không phải là dễ dàng như việc chỉ đơn giản là xác định\n",
    "một mạng duy nhất trong đó bao gồm cả hai RPN và nhanh R-CNN, và sau đó tối ưu hóa nó cùng với lan truyền ngược. Nguyên nhân là do đào tạo nhanh R-CNN phụ thuộc vào đề nghị đối tượng cố định và nó là không rõ ràng tiên nếu học nhanh R-CNN đồng thời thay đổi cơ chế đề nghị\n",
    "sẽ hội tụ. Trong khi tối ưu hóa doanh này là một câu hỏi thú vị cho công việc trong tương lai, chúng tôi phát triển một\n",
    "4 bước thuật toán đào tạo thực tiễn Để tìm hiểu các tính năng chia sẻ qua tối ưu hóa xen kẽ. Trong bước đầu tiên, chúng tôi đào tạo RPN như đã mô tả ở trên. Mạng lưới này được khởi tạo với một mô hình ImageNetpre-đào tạo và tinh chỉnh end-to-end cho các nhiệm vụ đề nghị khu vực. Trong bước thứ hai, chúng tôi\n",
    "đào tạo một mạng lưới phát hiện riêng biệt theo nhanh R-CNN sử dụng đề nghị tạo ra bởi các bước-1 RPN.\n",
    "mạng phát hiện này cũng được khởi tạo bởi mô hình ImageNet-pre-đào tạo. Tại thời điểm này hai\n",
    "mạng không chia sẻ lớp conv. Trong bước thứ ba, chúng tôi sử dụng mạng máy dò để khởi RPN\n",
    "đào tạo, nhưng chúng tôi sửa chữa các lớp conv chia sẻ và chỉ tinh chỉnh các lớp duy nhất để RPN. Bây giờ hai\n",
    "mạng chia sẻ lớp conv. Cuối cùng, giữ cho lớp conv chia sẻ cố định, chúng ta tinh chỉnh các lớp fc\n",
    "của Fast R-CNN. Như vậy, cả hai mạng chia sẻ các lớp conv giống nhau và tạo thành một mạng thống nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "Chúng tôi đào tạo và thử nghiệm cả hai mạng phát hiện đối tượng và đề xuất khu vực trên các hình ảnh tỷ lệ đơn [7,\n",
    "5]. Chúng tôi chia tỷ lệ lại các hình ảnh sao cho cạnh ngắn hơn của chúng là s = ​​600 pixel [5]. Tính năng đa quy mô\n",
    "trích xuất có thể cải thiện độ chính xác nhưng không thể hiện sự đánh đổi tốc độ chính xác tốt [5]. Chúng tôi cũng\n",
    "lưu ý rằng đối với lưới ZF và VGG, tổng sải chân trên lớp đối lưu cuối cùng là 16 pixel trên tỷ lệ lại\n",
    "hình ảnh và do đó là ∼10 pixel trên hình ảnh PASCAL điển hình (∼500 × 375). Ngay cả một bước tiến lớn như vậy\n",
    "cung cấp kết quả tốt, mặc dù độ chính xác có thể được cải thiện hơn nữa với một bước tiến nhỏ hơn.\n",
    "Đối với neo, chúng tôi sử dụng 3 tỷ lệ với diện tích hộp là 1282, 2562 và 5122 pixel và 3 tỷ lệ khung hình là\n",
    "1: 1, 1: 2 và 2: 1. Chúng tôi lưu ý rằng thuật toán của chúng tôi cho phép sử dụng các hộp neo lớn hơn\n",
    "lĩnh vực tiếp nhận cơ bản khi dự đoán các đề xuất lớn. Những dự đoán như vậy không phải là không thể\n",
    "người ta vẫn có thể suy luận đại khái về phạm vi của một vật thể nếu chỉ nhìn thấy phần giữa của vật thể. Với\n",
    "thiết kế này, giải pháp của chúng tôi không cần các tính năng đa quy mô hoặc cửa sổ trượt đa quy mô để dự đoán\n",
    "khu vực rộng lớn, tiết kiệm đáng kể thời gian chạy. Hình 1 (bên phải) cho thấy khả năng của phương pháp của chúng tôi\n",
    "cho một loạt các tỷ lệ và tỷ lệ khung hình. Bảng dưới đây cho thấy đề xuất trung bình đã học\n",
    "kích thước cho mỗi neo sử dụng mạng ZF (số cho s = 600).\n",
    "\n",
    "Các hộp neo vượt qua ranh giới hình ảnh cần phải được xử lý cẩn thận. Trong quá trình đào tạo, chúng tôi\n",
    "bỏ qua tất cả các neo xuyên biên giới để chúng không góp phần vào sự mất mát. Đối với 1000 × 600 điển hình\n",
    "hình ảnh, sẽ có tổng số neo khoảng 20k (≈ 60 × 40 × 9). Với các neo xuyên biên giới\n",
    "bỏ qua, có khoảng 6k neo cho mỗi hình ảnh để đào tạo. Nếu các ngoại lệ vượt biên không\n",
    "Bỏ qua trong đào tạo, họ giới thiệu các thuật ngữ lỗi lớn, khó sửa trong mục tiêu và đào tạo\n",
    "không hội tụ. Tuy nhiên, trong quá trình thử nghiệm, chúng tôi vẫn áp dụng RPN tích chập hoàn toàn cho toàn bộ\n",
    "hình ảnh. Điều này có thể tạo ra các hộp đề xuất xuyên biên giới, mà chúng tôi đưa vào ranh giới hình ảnh.\n",
    "Một số đề xuất RPN rất chồng chéo với nhau. Để giảm sự dư thừa, chúng tôi áp dụng triệt tiêu không tối đa (NMS) trên các khu vực đề xuất dựa trên điểm số cls của họ. Chúng tôi sửa chữa IoU\n",
    "ngưỡng cho NMS ở mức 0,7, cho chúng ta khoảng 2k vùng đề xuất cho mỗi hình ảnh. Như chúng tôi sẽ chỉ ra\n",
    "NMS không gây hại cho độ chính xác phát hiện cuối cùng, nhưng làm giảm đáng kể số lượng đề xuất. Sau NMS, chúng tôi sử dụng các khu vực đề xuất được xếp hạng N hàng đầu để phát hiện. Sau đây, chúng tôi\n",
    "huấn luyện R-CNN nhanh bằng cách sử dụng các đề xuất 2k RPN, nhưng đánh giá số lượng đề xuất khác nhau tại thời điểm thử nghiệm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Computer vision with Python 3 Translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hơn 1 tháng sau khi paper Fast R-CNN được xuất bản, Girshick cộng tác với Ren và Sun trong một paper 2015, Faster R-CNN: Towards Real-Time object Detection with Region Proposal Networks.\n",
    "\n",
    "Trong công việc này, Girshick et al đã tạo thêm một thành phần cho kiến trúc R-CNN, một **Region proposal Network (RPN)**. Như tên gọi của module này, mục tiêu của RPN là xoá bỏ yêu cầu chạy Selective Search trước khi suy luận và thay vào đó, hãy đưa ra đề xuất khu vực trực tiếp vào kiến trúc R-CNN.\n",
    "\n",
    "Hình 15.5 cung cấp mô phỏng về kiến trúc được cập nhật này. Tại đây, chúng ôi có thể xem một hình ảnh input trình bày cho mạng và các tính năng của nó được chiết xuất qua pre-đào tạo CNN. Các đặc trưng này, song song, gửi đến 2 component khác của Faster R-CNN.\n",
    "\n",
    "Component đầu tiên, RPN, được sử dụng để xác định nơi trong một hình ảnh một đối tượng tiềm năng có thể. Tại thời điểm này chúng tôi không biết những gì đối tượng là, chỉ cần có khả năng là một đối tượng tại một vị trí nhất định trong hình ảnh.\n",
    "\n",
    "Các đề xuất bounding ROI hộp được dựa trên khu vực quan tâm (ROI) Bằng việc kết hợp mô-đun của mạng cùng với các tính năng trích xuất từ các bước trước. ROI Pooling được sử dụng để trích xuất cửa sổ kích thước cố định các tính năng mà sau đó được truyền vào hai lớp đầy đủ kết nối (một cho nhãn lớp và một cho các tọa độ khung giới hạn) để có được địa phương hóa thức của chúng tôi.\n",
    "\n",
    "Chúng tôi sẽ thảo luận về RPN chi tiết bên trong mục 15.2.4, nhưng trong bản chất, bây giờ chúng tôi đang đi đến nơi **anchor** cách nhau thống nhất trên toàn bộ hình ảnh ở quy mô khác nhau và tỉ lệ màn hình. các RPN sau đó sẽ xem xét các anchor và đầu ra một tập hợp các đề xuất như là nơi nó “nghĩ” một đối tượng tồn tại.\n",
    "\n",
    "Điều quan trọng cần lưu ý ở đây là RPN không thực sự ghi nhãn ROI; thay vào đó, nó là máy tính “Điểm objectness” của mình và hỏi: “Liệu cái nhìn khu vực này như một đối tượng của một số loại?” cá nhân tôi như suy nghĩ của RPN và điểm objectness như một bộ phân loại nhị phân của các loại nơi RPN là ghi nhãn mỗi ROI là “nền” hay “tiền cảnh”. Nếu RPN nghĩ ROI là “tiền cảnh” thì ROI là giá trị xem xét thêm ROI Pooling và nhãn thức + bounding hộp lớp đầy đủ kết nối."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The base Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong đường ống phát hiện đối tượng truyền thống, chúng tôi sử dụng một trong hai (1) một sự kết hợp của cửa sổ trượt (sliding window) + kim tự tháp hình ảnh hoặc (2) Selective Search - giống thuật toán sinh proposal cho các classier của chúng tôi. Từ đó, mục tiêu của chúng tôi là develop một object detector end-to-end sử dụng deep learning mà bao gồm các mudule proposal, chúng tôi cần định nghĩa một phương pháp sẽ sinh các proposal ROIS (region of Iterest) của chúng tôi.\n",
    "\n",
    "Việc tách lõi giữa phân loại và phát hiện đối tượng là dự đoán của bounding-box hoặc toạ độ (x,y) xung quanh một đối tượng. Như vậy, chúng tôi phải yêu cầu mạng của chúng tôi trả về một tuple bao gồm các toạ độ bouding box của một đối tượng cụ thể. Nhưng có một vấn đề với cách tiếp cận này, đó là:\n",
    "1. Làm thế nào chúng tối xử lý một mạng dự đoán giá trị bên ngoài ranh giới của hình ảnh?\n",
    "2. Làm thế nào chúng tôi mã hoá hạn chế như $x_{min} < x_{max}$ và $y_{min} < y_{max}$?\n",
    "\n",
    "Nghe có vẻ đây là một vấn đề không thể giải quyết gần. Tuy nhiên , các giải pháp đề xuất bởi Girchick et al, gọi là **anchors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thay vì cố gắng dự đoán các toạ độ (x,y) của bounding box, chúng tôi có thể thay thế học để dự đoán các offset từ các box tham khảo, đó là $\\Delta_{x-center}, \\Delta_{y-center}, \\Delta_{width}, \\Delta_{height}$. Các giá trị Delta này cho phép chúng tôi để có được một sự phù hợp tốt hơn để hộp tài liệu tham khảo của chúng tôi mà không cần phải dự đoán liệu thực tế (x; y) -coordinates, cho phép chúng tôi bỏ qua những vấn đề có khả năng không thể\n",
    "của mã hóa bounding box “quy tắc” vào hệ thống mạng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vậy, các bounding box reference đến từ đâu? Chúng tôi cần sinh các anchor mà không cần sử dụng thuật toán Selective Search. Để thực hiện quá trình này, chúng tôi đầu tiên cần đến các điểm đều mẫu qua một hình ảnh đầu vào (hình 15.7 trái). Ở đây chúng ta có thể thấy một đầu vào hình ảnh có 600x400 pixel - chúng tôi đã dán nhãn từng điểm tại một số nguyên thường xuyên lấy mẫu (tại một khoảng thời gian mười sáu pixel) với một vòng tròn màu xanh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bước tiếp theo là tạo một tập các anchor tại mỗi điểm mẫu. Như bài báo Faster R-CNN, chúng tôi sẽ sinh 9 anchors (được cố định bouding box) với kích thước khác nhau và tỷ lệ khía cạnh xung quanh một điểm lấy mẫu nhất định.\n",
    "\n",
    "Màu sắc của các hộp bounding là scale/size của chúng tôi, đó là: 64x64, 128x128 và 256x256. Với mỗi scale chúng tôi có các tỉ lệ, 1:1 , 1:2 và 2:1. Mỗi sự kết hợp của scale và ratio trả về tổng cộng 9 anchor. Sự kết hợp của sản lượng tỷ lệ quy mô và lĩnh vực chúng tôi bảo hiểm đáng kể so với mọi quy mô đối tượng càng tốt và quy mô trong hình ảnh đầu vào (hình 15,7, phải)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuy nhiên, có một vấn đề ở đây khi chúng tôi phá vỡ tổng số neo tạo:\n",
    "* Nếu chúng tôi sử dụng bước xải 16 pixel (mặc định của Faster R-CNN) trên 600x800 ảnh, chúng tôi sẽ có tổng cộng 1989 vị trí.\n",
    "* Với 9 anchor xung quanh mỗi 1989 vị trí, chúng tôi sẽ có 1989x9=18901 bouding box cho mạng CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu CNN của chúng tôi phân loại mỗi 17.901 hộp bounding mạng của chúng tôi sẽ chỉ hơi nhanh hơn so với cách thấu đáo Looping trên mỗi sự kết hợp của trượt cửa sổ và kim tự tháp hình ảnh. May mắn thay, với Region Proposal Network (RPN) chúng tôi có thể giảm số cửa sổ đề xuất xuống, để lại 1 số lượng dễ quản lý hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region Proposal Network (RPN)\n",
    "Nếu mục tiêu của sinh anchor là để đạt được phủ tốt qua tất cả các vị trí scale và size của đối tượng trong ảnh, mục tiêu của RPN là tỉa bớt số bouding box để thu được kích thước dễ quản lý hơn.\n",
    "\n",
    "Module RPN là đơn giản nhưng mạnh mẽ, bao gồm 2 output. Đỉnh của RPN chấp nhận một đầu vào, cái mà ánh xạ đặc trưng conv của chúng tôi từ section 15.2.2. Chúng tôi áp dụng một 3x3 conv, học 512 filter.\n",
    "\n",
    "Các filter được fed vào 2 đường song song. Output đầu tiên (trái) của RPN là điểm cho biết RPN nghĩ ROI là *foreground* (giá trị xem xét thêm) hoặc *background* (loại bỏ). Hignh 15.8 cung cấp một mô phỏng của nhãn \"objecness\" của một input ROI.\n",
    "\n",
    "Một lần nữa, RPN không thực sự ghi nhãn ROI - nó chỉ cố gắng để xác định xem ROI là một trong hai background hoặc foreground. Việc ghi nhãn của ROI sẽ diễn ra trong kiến trúc sau đó (section 15.2.6). Số chiều của output này là 2xK  với K là tổng số anchor, một output cho xác xuất foreground và cái thứ 2 cho xác suất background.\n",
    "\n",
    "Output thứ 2 (phải) là bounding box regressor của chúng tôi sử dụng để điều chỉnh các anchor tốt hơn fit với đối tượng mà nó bao bọc. Điều chỉnh anchor một lần nữa được thực hiện thông qua 1 × 1 convolution, nhưng lần này output là 4xK. Output 4xK như chúng tôi dự đoán cho 4 giá trị offset $\\Delta_{x-center}, \\Delta_{y-center}, \\Delta_{width}, \\Delta_{height}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với điều kiện là khả foreground của chúng tôi là đủ lớn, chúng tôi sau đó áp dụng:\n",
    "* Non-maxima supperssion to suppres overlapping\n",
    "* Proposal selection\n",
    "\n",
    "Sẽ tự nhiên có nhiều địa điểm chồng chéo neo theo mục 15.2.3 trên -\n",
    "phi maxima ức chế giúp giảm số lượng các địa điểm để truyền lại cho ROI Pooling mô-đun. Chúng tôi tiếp tục giảm số địa điểm để vượt qua vào module ROI tổng hợp qua lựa chọn đề nghị Ở đây chúng ta chỉ mất đề nghị N hàng đầu và loại bỏ phần còn lại.\n",
    "\n",
    "Trong ấn phẩm nhanh hơn R-CNN ban đầu, Girshick et al. bộ N = 2; 000, nhưng chúng tôi có thể lấy đi với N nhỏ hơn nhiều, chẳng hạn như N = 10; 50; 100; 200 và vẫn được dự đoán tốt. Bước tiếp theo trong các đường ống sẽ được phổ biến rộng rãi ROI và đồng bằng châu thổ đến khu vực yêu thích (ROI) Pooling mô-đun (Phần 15.2.5), nhưng chúng ta hãy đầu tiên thảo luận về cách chúng ta có thể đào tạo RPN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the  RPN\n",
    "Trong quá trình training, chúng tôi lấy các anchor của chúng tôi và đặc hcúng qua 2 buckets khác nhau:\n",
    "* Bucket #1 - Foreground: Tất cả các anchor mà có iou 0.5 với một ground-truth object bounding box.\n",
    "* Bucket #2 - Background: Tất cả các anchor có <0.1 iou với một ground-truth object.\n",
    "\n",
    "Dựa trên những xô chúng tôi ngẫu nhiên mẫu giữa hai để duy trì một tỷ lệ tương đương giữa nền và foreground.\n",
    "\n",
    "Từ đó, chúng tôi cần xem xét loss function của chúng tôi. Cụ thể, module RPN có 2 loss function liên quan với nó. Đầu tiên là loss fucntion cho phân lớp dự đoán foreground vs background (binary cross entropy làm tốt tại đây).\n",
    "\n",
    "Loss function thứ 2 cho bounding box regression. Hàm loss function này chỉ hoạt động trên foreground anchor như background anchor would have no sense of a bounding box (and we should have already detected “background” and discarded it)\n",
    "\n",
    "Với bounding box regression loss function, Girshick et al, sử dụng **smooth L1 loss** Vì nó là không thực tế cho chúng ta đến 100% dự đoán chính xác trên ground-truth tọa độ của một hộp bounding. Smooth L1 loss cho phép bounding box đó \"đủ gần\" tới ground-truth box tương ứng là cơ bản đúng và do đó làm giảm tác động của sự mất mát."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
