{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tài liệu tham khảo:\n",
    "- Rich feature hirarchies for accurate object detection and sematic segmentaion (Paper)\n",
    "- https://d2l.ai/chapter_computer-vision/rcnn.html\n",
    "- https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html#r-cnn\n",
    "- https://thorpham.github.io/blog/\n",
    "- https://github.com/yangxue0827/RCNN\n",
    "- https://helpex.vn/article/tu-r-cnn-den-r-cnn-nhanh-hon-su-phat-trien-cua-cong-nghe-phat-hien-doi-tuong-5c66458aae03f601287658bf\n",
    "- RCNN Tensorflow: https://github.com/Liu-Yicheng/R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "Hiệu suất object detection, được đo trên bộ dữ liệu PASCAL VOC, đã giữ nguyên trong những năm cuối gần đây. Các phương pháp tốt nhất là những hệ thống quần thể phức tạp mà thường kết hợp nhiều đặc trưng ảnh mức độ thấp với bối cảnh mức độ cao. Trong paper này, chúng tôi đưa ra một thuật toán detection đơn giản và có khả năng mở rộng, cải thiện độ chính xác trung bình `mean average precision` (mAP) hơn 30% so với phương pháp tốt nhất trước đó tại VOC 2012 - đạt được một mAP 53.3%. Cách tiếp cận của chúng tôi kết hợp 2 key chính:\n",
    "\n",
    "(1) Áp dụng các mạng CNN cấp cao để bottom-up region proposals (đề xuất khu vực) để bản địa hoá và phân khúc đối tượng.\n",
    "\n",
    "(2) Khi dữ liệu training đã gán nhãn là khán hiếm, supervised pre-training cho nhiệm vụ hỗ trợ, followed by domain-specific fine-tuning, mang lại một hiệu suất tốt hơn.\n",
    "\n",
    "Từ đó chúng tôi tổ hợp các region proposals với CNN, chúng tôi gọi phương pháp của chúng tôi là R-CNN: `Region with CNN features`. Chúng tôi so sánh R-CNN với OverFeat, một detector gần đây đề xuất các proposed bằng cửa số trượt trên nền kiến trúc CNN. Chúng tôi thấy R-CNN nhanh hơn OverFeat bằng việc pát hiện 200 class ILSVRC2013. Sources cho hệ thống đã hoàn thành có sẵn tại: http://www.rossgirshick.info/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Các tính năng quan trọng. Thập kỷ qua cho thấy nhiều sự tiến bộ của thị giác máy tính dựa trên đáng kể ở SIFT và HOG. Nhưng nếu chúng tôi nhìn hiệu xuất trên các tác vụ thị giác kinh điển, object detection PASCAL VOC, nó thường được thừa nhận là đã phát triển chậm trong khoảng 2010-2012, với sự tăng trưởng nhỏ thu được bằng cách xây dựng hệ thống quần thể và sử dụng các biến thể nhỏ của phương pháp thành công.\n",
    "\n",
    "SIFT và HOG là các biểu đồ định hướng blockwise, một biểu chúng chúng tôi có thể kết hợp với khoảng phức tạp tế bào trong V1, khu vực vỏ não đầu tiên trong con đường thị giác linh trưởng. Nhưng chúng ta cũng biết thừa nhận xảy ra nhiều\n",
    "giai đoạn hạ lưu, quá trình nhiều giai đoạn cho các tính năng tính toán rằng\n",
    "Thậm chí còn nhiều thông tin hơn để được công nhận thị giác\n",
    "\n",
    "**CHƯA DỊCH**\n",
    "\n",
    "Để thay thế, chúng tôi giải quyết vấn đề CNN thường bởi mô hình `recoginition using regions`, nó đã thành công trong cả object detection và sematic segmentaion. Lúc thử nghiệm phương pháp của chúng tôi sinh khoảng 2000 region proposal độc lập cho ảnh đầu vào, trích xuất ra một vector đặc trưng chiều dài cố định từ mỗi vùng sử dụng một CNN, và rồi phần lớp mỗi vùng với SVM. Chúng tôi sử dụng công nghệ đơn giản (affine image warping) để tính toán một kích cỡ cố định cho input CNN từ mỗi region proposal, bất kể shape của region như thế nào. Hình 1 biểu diễn tổng quan về phương pháp của của chúng và một số nổi bật của kết quả. Do hệ thống tổ hợp các region proposal với CNN, chúng tôi gọi phương pháp R-CNN: Regions with CNN features.\n",
    "\n",
    "Trong phiên bản đã cập nhật của paper này, chúng tôi cung cấp head-to-head so sánh R-CNN và OverFeat trên 200 class ILSVRC2013. OverFeat sử dụng một sliding-window CNN cho detection và cho đến giờ nó là phương pháp đạt hiệu xuất tốt nhất trên ILSVRC2013. Chúng tôi thấy rằng R-CNN cải thiện đáng kể so với OverFeat, mới mAP 31.4% so với 24.3%.\n",
    "\n",
    "Thách thức thứ 2 phải đối mặt trong việc dectection là dữ liệu được gán nhãn khan hiếm và số lượng có sẵn là không đủ để training mọt mạng CNN lớn. Giải pháp thông thường trong vấn đề này là sử dụng một `unsupervises pre-training`, đi theo supervised fine-tuning. Sự đóng góp nguyên tắc thứ 2 của nghiên cứu paper này cho thấy rằng *supervises pre-training* trên dataset phụ lớn (ILSVRC), tiếp theo domainspecific tinh chỉnh trên một tập dữ liệu nhỏ (PASCAL), là một mô hình hiệu quả để CNN high-capacity học khi dữ liệu khan hiếm. Trong thực nghiệm của chúng tôi, fine-tuning cho detection cải thiện hiệu suất mAP lên 8%. Sau khi fine-tuning, hệ thống của chúng tôi đạt mAP 54% trên VOC 2012 so với 33% ch highly-tuned, HOG-based deformable part model .We also point readers to contemporaneous work by Donahue et al. [12], who\n",
    "show that Krizhevsky’s CNN can be used (without finetuning) as a blackbox feature extractor, yielding excellent\n",
    "performance on several recognition tasks including scene\n",
    "classification, fine-grained sub-categorization, and domain\n",
    "adaptation.\n",
    "\n",
    "**CHƯA DỊCH**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Object detection with R-CNN\n",
    "Hệ thống object detection của chúng tôi bao gồm 3 modules. Đầu tính sinh các region proposals độc lập. Các đề xuất xác định tập các candidate detections cho bộ phát hiện của chúng tôi. Module thứ 2 là một mạng CNN lớn trích xuất một vector đặc trưng độ dài cố định từ mỗi region. Module thứ 3 là một tập các SVM tuyến tính đặc thù. Trong section này, chúng tôi biểu diễn thiết kế quyết định cho mỗi module, mô tả thí nghiệm, thông tin tham số và học như thế nào, và show kết quả detection on PASCAL VOC 2010-12 và trên ILSVRC2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Module design\n",
    "**Region proposals**: Một loạt các paper gần đây đưa ra các phương pháp để sinh các region proposal độc lập.\n",
    "\n",
    "Ví dụ: objectness, selective search, category-independent object proposals, constrained parametric min-cuts (CPMC), multi-scale combinatorial grouping, and Ciresan et al, họ phát hiện các mitotic cells bằng cách áp dụng CNN để regulary-spaced square crops, là một trường hợp đặc biệt của region proposals. Trong khi R-CNN là bất khả thi với phương pháp region proposal cụ thể, chúng tôi sử dụng selective search để cho phép so sánh kiểm soát trước công việc detection.\n",
    "\n",
    "**Feature extraction**: Chúng tôi trích xuất một vector đặc trưng 4096 chiều từ mỗi region proposal sử dụng Caffe thực thi CNN mô tả bởi Krizhevsky et al. Các đặc trưng được tính toán bởi lan truyền tiến một mean-subtracted 227x227 RGB image qua 5 layer convolution và 2 layer fully connected. Chúng tôi tham khảo 24, 25 để chi tiết kiến trúc mạng hơn.\n",
    "\n",
    "Để tính toán đặc trưng cho một region proposal, chúng tôi đầu tiên phải chuyển dữ liệu ảnh trong region thành một form để tương tích với CNN (kiến trúc của nó yêu cầu input cố định 227x227 pixel. Trong số nhiều biến đổi có thể có của các vùng tuỳ ý của chúng tôi, chúng tôi lựa chọn cho đơn giản nhất. Không phục thuộc vào kích thước hoặc tỷ lệ của các vùng đề xuất, chúng tôi warp tất cả pixel trong một bouding box chặt chẽ xung quanh nó với kích thước yêu cầu. Prior to warping, we dilate the\n",
    "tight bounding box so that at the warped size there are exactly p pixels of warped image context around the original\n",
    "box (we use p = 16). Figure 2 shows a random sampling\n",
    "of warped training regions. Alternatives to warping are discussed in Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test-time detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Results in PASCAL VOC 2010-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Result on ILSVRC2013 detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization, ablation, and modes of error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ILSVRC2013 detection dataset\n",
    "## Sematic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "Trong những năm gần đây, hiệu suất của object detection trì trệ. Hệ thống hiệu suất tốt nhất là phức tạp kết hợp các cụm đặc trưng ảnh mức thấp với các ngữ cảnh mức cao từ object detectors và scene classifer. Paper này biểu diễn một thuật toán object detection đơn giản và có khả năng mở rộng cải thiện 30% so với kết quả tốt  nhất trước đó trên PASCAL VOC 2012.\n",
    "\n",
    "**CHƯA DỊCH**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Appendix\n",
    "## 7.1 Object proposal trangormations\n",
    "Mạng CNN đã sử dụng trong công việc này yêu cầu input cố định 227x227 pixels. Với detection, chúng tôi xem xét object proposals đó là hình ảnh tuỳ ý hình chữ nhật. Chúng tôi đã đánh giá 2 cách tiếp cận cho chuyển đổi object proposal thành đầu vào CNN hợp lệ.\n",
    "\n",
    "Phương pháp đầu tiên `tightest square with context` (vuông chặt chẽ với bối cảnh) bao quanh mỗi object proposal bên trong ô vuông chặt chẽ và rồi scales (istropically) hình ảnh bao gồm ô vuông này thành input CNN. Hình 7 cột B cho thấy chuyển đổi này. Một biến thể trên phương pháp này không bao gồm các nội dung hình ảnh (bối cảnh) xung quanh object proposal gốc. Hình 7 cột C cho thấy chuyển đổi này. Phương pháp thứ 2 `warp`quy mô dị hướng (anisotropically scales) mỗi object proposal thành input CNN. Hình 7 cột D cho thấy chuyển đổi warp. \n",
    "\n",
    "Với mỗi một trong các chuyển đổi, chúng tôi cũng xem xét bao gồm bối cảnh hình ảnh bổ sung quanh object proposal gốc. Lượng context padding (đệm ngữ cảnh) (p) được định nghĩa là viền xung quanh object proposal gốc trong khung toạ độ input đã chuyển đổi. Hình 7 cho thấy p=0 pixels trong top row của mỗi mẫu và p=16 pixel trong bottom row. Trong tất cả các phương pháp, nếu hình chữ nhật nguồn mở rộng vượt ra ngoài hình ảnh, các dữ liệu thiếu sẽ được thay thế bởi image mean (sau đó được trừ đi trước khi thành input ảnh vào mạng CNN). Một tập hợp thí điểm các thí nghiệm cho thấy mà warping với đệm bối cảnh (p=16 pixel) vượt trội hơn (3-5 điểm mAP). Rõ ràng nhiều lựa chọn thay thế có thể xảy ra, trong đó có sử dụng\n",
    "nhân rộng thay vì đệm trung bình. đánh giá đầy đủ\n",
    "các giải pháp thay thế là trái như công việc tương lai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Bounding-box regression\n",
    "Chúng tôi sử dụng một regression bounding-box đơn giản để cải thiện hiệu suất. Sau khi ghi điểm cho mỗi vùng selective search với một class riêng detection SVM, chúng toi dự đoán một bouding box mới cho detection sử dụng một trình regressor bouding-box class-specific. Điều này tương tự trong linh hồn để hồi quuy bouding-box được sử dụng trong mô hình phần biến dạng. Sự khác biệt chính là 2 phương pháp tiếp cận là mà ở đây chúng tôi thoái từ các đặc trừng chúng tôi tính toán bởi CNN, chứ không phải từ các đặc trưng hình học được tính toán trên infered DPM part locations.\n",
    "\n",
    "Input của thuật toán training của chúng tôi là một tập của N cặp ${(P^i, G^i)}$, với $P^i=(P^i_x, P^i_y, P^i_w, P^i_h)$ quy định toạ độ pixel của trung tâm của bouding box đề xuất $P^i$ cùng với chiều dài và chiều rộng của $P^i$. Do đó nếu, chúng tôi bỏ các superscript i trừ khi nó là cần thiết. Mỗi bouding box ground-truth G là xác định: $G=(G_x, G_y, G_w, G_h)$. Mục tiêu của chúng tôi là học một chuyển đổi mà ánh xạ một box đề xuất P thành một box ground-truth G.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng tôi tham số hoá chuyển đổi trong 4 hàm: $d_x(P), d_y(P), d_w(P), d_h(P)$. \n",
    "Hai phần đầu xác định bản dịch tỷ lệ của trung tâm hộp giới hạn P, trong khi 2 cái sau chỉ định các bản dịch không gian log của chiều rộng và chiều cao của hộp giới hạn P. Sau khi học các hàm này, chúng tôi có thể chuyển đổi một vùng đề xuất input P thành một box ground-truth dự đoán $\\hat{G}$ bởi áp dụng chuyển đổi:\n",
    "$$\\begin{align}\n",
    "\\hat{G}_x & = P_w d_x(P)+P_x \\\\\n",
    "\\hat{G}_y & = P_h d_y(P)+P_y \\\\\n",
    "\\hat{G}_w & = P_w exp(d_w(P)) \\\\\n",
    "\\hat{G}_h & = P_h exp(d_h(P)) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d2l.ai\n",
    "Các mô hình R-CNN trước tiên chọn một số vùng được đề xuất từ ​​một hình ảnh (ví dụ: hộp neo là một loại phương thức lựa chọn) và sau đó gắn nhãn danh mục và hộp giới hạn của chúng (ví dụ: offset). Sau đó, họ sử dụng CNN để thực hiện tính toán chuyển tiếp để trích xuất các tính năng từ mỗi khu vực được đề xuất. Sau đó, chúng tôi sử dụng các tính năng của từng khu vực được đề xuất để dự đoán danh mục và hộp giới hạn của chúng. Hình 13.8.1 cho thấy một mô hình R-CNN.\n",
    "\n",
    "Cụ thể, R-CNN bao gồm bốn phần chính:\n",
    "\n",
    "Tìm kiếm chọn lọc được thực hiện trên hình ảnh đầu vào để chọn nhiều khu vực được đề xuất chất lượng cao [Uijlings et al., 2013]\n",
    "- Các khu vực được đề xuất này thường được chọn trên nhiều tỷ lệ và có hình dạng và kích thước khác nhau. Hộp giới hạn loại và mặt đất của từng khu vực được đề xuất được dán nhãn.\n",
    "- Một CNN được đào tạo trước được chọn và đặt, ở dạng rút gọn, trước lớp đầu ra. Nó biến đổi từng khu vực được đề xuất thành các kích thước đầu vào theo yêu cầu của mạng và sử dụng tính toán chuyển tiếp để xuất các tính năng được trích xuất từ ​​các khu vực được đề xuất.\n",
    "- Các tính năng và thể loại được gắn nhãn của từng khu vực được đề xuất được kết hợp làm ví dụ để đào tạo nhiều máy vectơ hỗ trợ để phân loại đối tượng. Ở đây, mỗi máy vectơ hỗ trợ được sử dụng để xác định xem một ví dụ có thuộc về một loại nhất định hay không.\n",
    "- Các tính năng và hộp giới hạn được gắn nhãn của từng khu vực được đề xuất được kết hợp làm ví dụ để huấn luyện mô hình hồi quy tuyến tính cho dự đoán hộp giới hạn mặt đất.\n",
    "\n",
    "Mặc dù các mô hình R-CNN sử dụng các CNN được đào tạo trước để trích xuất hiệu quả các tính năng hình ảnh, nhưng nhược điểm chính là tốc độ chậm. Như bạn có thể tưởng tượng, chúng ta có thể chọn hàng ngàn vùng được đề xuất từ ​​một hình ảnh duy nhất, yêu cầu hàng ngàn tính toán chuyển tiếp từ CNN để thực hiện phát hiện đối tượng. Tải máy tính khổng lồ này có nghĩa là R-CNN không được sử dụng rộng rãi trong các ứng dụng thực tế."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-CNN\n",
    "R-CNN là viết tắt của `\"Region-based Convolutional Network\"`. Ý tưởng chính trong 2 bước. Đầu tiên, sử dụng **selective search**, xác định các vùng bouding-box đối tượng (\"region of interest\" or \"RoI\"). Và rồi nó trích xuất đặc trừng CNN từ mỗi vùng để thực hiện phân lớp.\n",
    "\n",
    "## Model workflow\n",
    "Cách R-CNN làm việc có thể tóm tắt như sua:\n",
    "1. Pre-train một mạng CNN trên bài toán phân lớp ảnh; ví dụ, VGG hoặc ResNEt đã train tren tập dataset ImageNet. Bài toán phân lớp phân N class.\n",
    "2. Đề xuất các vùng có đối tượng bằng thuật toán selective search (khoảng 2k candidates trên 1 ảnh). Các vùng có thể bao gồm đối tượng mục tiếu và chúng có kích cỡ khác nhau.\n",
    "3. Region candidates được **warped** để đưa về kích thước cố định mà mạng CNN yêu cầu.\n",
    "4. Tiếp tục fine-tuning mạng CNN trên các warped proposal region cho K+1 lớp; Thêm 1 một biểu thị background (Không là đối tượng nào). Trong giai đoạn fine-tuning, chúng tôi nên sử dụng một learning rate nhỏ hơn và mini-batch oversapmples the positive cases bởi vì hầu hết các proposed region chỉ là background.\n",
    "5. Với mỗi vùng ảnh, lan truyền tiến qua CNN sinh một vector đặc trưng. Vectơ đặc trưng này sau đó được sử dụng bởi một SVM nhị phân được đào tạo cho từng lớp một cách độc lập. Các mẫu positive là các proposed region với IoU ngưỡng overlap >= 0.3 và các mẫu negative là các trường hợp còn lại.\n",
    "6. Để giảm các lỗi nội địa hóa, một mô hình hồi quy được đào tạo để sửa cửa sổ phát hiện dự đoán trên phần bù hiệu chỉnh hộp giới hạn bằng các tính năng CNN.\n",
    "\n",
    "## Bouding box Regression\n",
    "Cho một bouding box dự đoán toạ độ $p=(p_x, p_y, p_w, p_h)$ (toạ độ trung tâm, chiều rộng, chiều cao) và tương ứng là ground truth box toạ độ $g=(g_x, g_y, g_w, g_h)$. Một trình hồi quy được cấu hình để tìm hiểu chuyển đổi bất biến tỷ lệ giữa hai trung tâm và chuyển đổi quy mô log giữa chiều rộng và chiều cao. Tất cả các hàm chuyển đổi lấy $p$ làm đầu vào:\n",
    "\n",
    "**CÔNG THỨC VÀ HÌNH ẢNH**\n",
    "\n",
    "Một lợi ích rõ ràng của việc áp dụng chuyển đổi như vậy là tất cả các hàm hiệu chỉnh hộp giới hạn, $d_i(p)$ với $i \\in \\{x,y,w,h\\}$, có thể lấy một vài giá trị giữa $[-\\inf, \\inf]$. Mục tiêu là học:\n",
    "$$\\begin{align}\n",
    "t_x = (g_x-p_x)/p_w \\\\\n",
    "t_y = (g_y-p_y)/p_h \\\\\n",
    "t_w = log(g_w/p_w) \\\\\n",
    "t_h = log(g_h/p_h) \\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một mô hình hồi quy chuẩn có thể giải quyết vấn đề cực tiểu hoá mất mát SSE với regularization:\n",
    "$$\\mathcal{L}_\\text{reg} = \\sum_{i \\in \\{x, y, w, h\\}} (t_i - d_i(\\mathbf{p}))^2 + \\lambda \\|\\mathbf{w}\\|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thuật ngữ chính quy (regularization) là rất quan trọng ở đây và bài báo RCNN đã chọn tốt nhất $\\lambda$ bằng cách xác thực chéo (cross validation). Một điều đáng chú ý là không phải tất cả các hộp giới hạn dự đoán đều có các hộp sự thật mặt đất tương ứng. Ví dụ, nếu không có sự trùng lặp, việc chạy hồi quy bbox không có nghĩa gì.  Ở đây, chỉ có một hộp dự đoán có hộp sự thật mặt đất gần đó với ít nhất 0,6 IoU được giữ lại để đào tạo mô hình hồi quy bbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Tricks\n",
    "Một số thủ thuật thường được sử dụng trong R-CNN và các mô hình khác.\n",
    "## Non-maximum suppression\n",
    "Có khả năng mô hình có thể tìm thấy nhiều hộp giới hạn cho cùng một đối tượng. Ức chế không tối đa (Non-maximim supperession) giúp tránh phát hiện lặp lại cùng một trường hợp. Sau khi chúng tôi nhận được một bộ các hộp giới hạn phù hợp cho cùng một loại đối tượng: Sắp xếp tất cả các hộp giới hạn theo điểm tin cậy. Hủy hộp có điểm tin cậy thấp. Trong khi có bất kỳ hộp giới hạn còn lại, hãy lặp lại như sau: Tham lam chọn cái có số điểm cao nhất.  Bỏ qua các hộp còn lại có IoU cao (tức là> 0,5) với hộp đã chọn trước đó.\n",
    "\n",
    "## Hard Negative Mining (Khai thác tiêu cực cứng)\n",
    "Chúng tôi xem xét bouding box không có đối tượng như negative examples. Không phải tất cả các negative examples là khó xác định như nhau. Ví dụ, nếu nó giữ một khoảng trống background, nó giống như một \"easy negative\", nhưng nếu box bao gồm các nhiễu hoặc một phần đối tượng, nó là khó để phân biệt và là \"hard negative\"\n",
    "\n",
    "Các mẫu hard negative là dễ bị phân loại sai. Chúng tôi có thể tìm thấy rõ ràng những mẫu dương tính giả đó trong các vòng huấn luyện và đưa chúng vào dữ liệu huấn luyện để cải thiện trình phân loại.\n",
    "\n",
    "## Speed Bottleneck (Nút cổ chai tốc độ)\n",
    "Nhìn qua các bước học tập R-CNN, bạn có thể dễ dàng phát hiện ra rằng việc đào tạo một mô hình R-CNN rất tốn kém và chậm, vì các bước sau đây liên quan đến rất nhiều công việc:\n",
    "1. Chạy tìm kiếm chọn lọc để đề xuất 2000 ứng cử viên khu vực cho mọi hình ảnh;\n",
    "2. Tạo vector tính năng CNN cho mọi vùng hình ảnh (N hình ảnh * 2000).\n",
    "3. Toàn bộ quá trình bao gồm ba mô hình riêng biệt mà không có nhiều tính toán được chia sẻ: mạng lưới thần kinh tích chập để phân loại hình ảnh và trích xuất tính năng;  trình phân loại SVM hàng đầu để xác định các đối tượng đích;  và mô hình hồi quy để thắt chặt các hộp giới hạn vùng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code bouding box regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "number_data = 5000\n",
    "img_size = 8\n",
    "min_size_obj = 1\n",
    "max_size_obj = 4\n",
    "number_obj = 1\n",
    "\n",
    "# x là dataset image, y là label với 4 tham số (x,y,w,h)\n",
    "bboxes = np.zeros((5000, 1, 4)) # 5k bouding box có size từ 1 đến 4\n",
    "image = np.zeros((5000, img_size, img_size)) # tạo 5k image 8x8\n",
    "# background là while\n",
    "for i in range(5000):\n",
    "    for obj in range(number_obj):\n",
    "        w,h = np.random.randint(min_size_obj, max_size_obj, size=2)\n",
    "        x = np.random.randint(0, img_size-w)\n",
    "        y = np.random.randint(0, img_size-h)\n",
    "        bboxes[i, obj, :] = (x,y,w,h)\n",
    "        image[i,y:y+h, x:x+w] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axis(\"off\")\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.imshow(image[i],cmap=\"Greys\",interpolation='none', origin='lower', extent=[0, img_size, 0, img_size])\n",
    "    for bbox in bboxes[i]:\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], ec='r', fc='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cái chùng ta cần là predict đường viền màu đỏ. Image sẽ có chiều là (5000, 8, 8), bouding box có chiều là (5000, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta sẽ dùng một mạng neural network đơn giản để training với library keras. Ở đây người ta gọi Bounding-box regression trong khi dùng neural network training, rất nhiều người lầm tưởng là dùng simple regression. Hãy mở rộng khái niệm regression ra một tí, nó là bài toán predict khi output là biến liên tục. Vì bounding box ở đây (x,y,w,h) là bốn biến liên tục nên ta gọi là bài toán regression.\n",
    "\n",
    "Đầu tiên chúng ta sẽ reshape các biến trước khi đưa vào model. Cũng có thể normalizer trước khi training để thuật toán hội tụ nhanh hơn. Nhưng do ảnh kích thước nhỏ và là binary nên không cần thiết . Sau đó chia dữ liệu thành training và testing với test_size = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 8, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = image.reshape((5000, -1))\n",
    "y = bboxes.reshape((5000,-1))\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 300)               19500     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 50,004\n",
      "Trainable params: 50,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300,input_dim =64))\n",
    "model.add(Dense(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(4))\n",
    "model.compile(optimizer=\"adadelta\",loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a dùng 2 layers : layer 1 là 300 node,layer 2 là 100 node với activation là relu.Cuối cùng là một layer dropout với tỉ lệ 20%. Optimizer bằng `adadelta` và loss là `mean square error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "3500/3500 [==============================] - 1s 196us/step - loss: 3.5575\n",
      "Epoch 2/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 1.2603\n",
      "Epoch 3/50\n",
      "3500/3500 [==============================] - 0s 20us/step - loss: 0.8301\n",
      "Epoch 4/50\n",
      "3500/3500 [==============================] - 0s 31us/step - loss: 0.6023\n",
      "Epoch 5/50\n",
      "3500/3500 [==============================] - 0s 18us/step - loss: 0.5085\n",
      "Epoch 6/50\n",
      "3500/3500 [==============================] - 0s 19us/step - loss: 0.4390\n",
      "Epoch 7/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.3818\n",
      "Epoch 8/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.3426\n",
      "Epoch 9/50\n",
      "3500/3500 [==============================] - 0s 22us/step - loss: 0.3079\n",
      "Epoch 10/50\n",
      "3500/3500 [==============================] - 0s 19us/step - loss: 0.2696\n",
      "Epoch 11/50\n",
      "3500/3500 [==============================] - 0s 22us/step - loss: 0.2585\n",
      "Epoch 12/50\n",
      "3500/3500 [==============================] - 0s 22us/step - loss: 0.2200\n",
      "Epoch 13/50\n",
      "3500/3500 [==============================] - 0s 30us/step - loss: 0.2151\n",
      "Epoch 14/50\n",
      "3500/3500 [==============================] - 0s 23us/step - loss: 0.2087\n",
      "Epoch 15/50\n",
      "3500/3500 [==============================] - 0s 23us/step - loss: 0.2078\n",
      "Epoch 16/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1919\n",
      "Epoch 17/50\n",
      "3500/3500 [==============================] - 0s 19us/step - loss: 0.1844\n",
      "Epoch 18/50\n",
      "3500/3500 [==============================] - 0s 19us/step - loss: 0.1541\n",
      "Epoch 19/50\n",
      "3500/3500 [==============================] - 0s 18us/step - loss: 0.1546\n",
      "Epoch 20/50\n",
      "3500/3500 [==============================] - 0s 18us/step - loss: 0.1632\n",
      "Epoch 21/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1490\n",
      "Epoch 22/50\n",
      "3500/3500 [==============================] - 0s 20us/step - loss: 0.1363\n",
      "Epoch 23/50\n",
      "3500/3500 [==============================] - 0s 19us/step - loss: 0.1556\n",
      "Epoch 24/50\n",
      "3500/3500 [==============================] - 0s 18us/step - loss: 0.1383\n",
      "Epoch 25/50\n",
      "3500/3500 [==============================] - 0s 20us/step - loss: 0.1482\n",
      "Epoch 26/50\n",
      "3500/3500 [==============================] - 0s 20us/step - loss: 0.1200\n",
      "Epoch 27/50\n",
      "3500/3500 [==============================] - 0s 22us/step - loss: 0.1333\n",
      "Epoch 28/50\n",
      "3500/3500 [==============================] - 0s 23us/step - loss: 0.1258\n",
      "Epoch 29/50\n",
      "3500/3500 [==============================] - 0s 23us/step - loss: 0.1372\n",
      "Epoch 30/50\n",
      "3500/3500 [==============================] - 0s 20us/step - loss: 0.1345\n",
      "Epoch 31/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1199\n",
      "Epoch 32/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1160\n",
      "Epoch 33/50\n",
      "3500/3500 [==============================] - 0s 22us/step - loss: 0.1258\n",
      "Epoch 34/50\n",
      "3500/3500 [==============================] - 0s 22us/step - loss: 0.1282\n",
      "Epoch 35/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1146\n",
      "Epoch 36/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1083\n",
      "Epoch 37/50\n",
      "3500/3500 [==============================] - 0s 22us/step - loss: 0.1180\n",
      "Epoch 38/50\n",
      "3500/3500 [==============================] - 0s 27us/step - loss: 0.1195\n",
      "Epoch 39/50\n",
      "3500/3500 [==============================] - 0s 23us/step - loss: 0.1119\n",
      "Epoch 40/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1065\n",
      "Epoch 41/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1086\n",
      "Epoch 42/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1230\n",
      "Epoch 43/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1095\n",
      "Epoch 44/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1205\n",
      "Epoch 45/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1066\n",
      "Epoch 46/50\n",
      "3500/3500 [==============================] - 0s 20us/step - loss: 0.1089\n",
      "Epoch 47/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1020\n",
      "Epoch 48/50\n",
      "3500/3500 [==============================] - 0s 19us/step - loss: 0.1048\n",
      "Epoch 49/50\n",
      "3500/3500 [==============================] - 0s 20us/step - loss: 0.1003\n",
      "Epoch 50/50\n",
      "3500/3500 [==============================] - 0s 21us/step - loss: 0.1082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x223ec469288>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.7541435 ,  3.7081826 ,  2.7213347 ,  1.8879557 ],\n",
       "       [ 4.630879  ,  3.7446082 ,  1.9242449 ,  1.9807534 ],\n",
       "       [ 1.1250424 , -0.03564757,  3.0373385 ,  2.9428952 ],\n",
       "       ...,\n",
       "       [ 3.983115  ,  2.0205817 ,  2.962473  ,  2.9578934 ],\n",
       "       [ 0.92938167,  2.5686743 ,  0.83708715,  2.61414   ],\n",
       "       [-0.09937781,  4.2658863 ,  2.6514635 ,  1.8555152 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhìn vào hình vẽ đầu tiền ta có nhận xét là : model predict tốt là khi đường viền màu đỏ và bounding box nó càng sát nhau . Như vậy ta có thể dùng cái này để đánh giá model. Ta đã quen với khái niệm IOU là Intersection over Union. Có nghĩa là ta sẽ đánh giá model bằng tỉ lệ area overlap với area union giữa thực tế và predict. Sau đó tính mean là sẽ ra được tỉ lệ IOU của model . IOU càng cao có nghĩa model predict tốt và ngược lại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlaping_area(detection_1,detection_2):\n",
    "    \n",
    "  x_1 = detection_1[0]\n",
    "  y_1 = detection_1[1]\n",
    "  x_w_1 = detection_1[0] + detection_1[2]\n",
    "  y_h_1 = detection_1[1] + detection_1[3]\n",
    "    \n",
    "  x_2 = detection_2[0]\n",
    "  y_2 = detection_2[1]\n",
    "  x_w_2 = detection_2[0] + detection_2[2]\n",
    "  y_h_2 = detection_2[1] + detection_2[3]\n",
    "  # tính overlap theo ox,oy .Nếu ko giao nhau trả về 0\n",
    "  overlap_x = max(0,min(x_w_1,x_w_2) - max(x_1,x_2))\n",
    "  overlap_y = max(0,min(y_h_1,y_h_2) - max(y_1,y_2))\n",
    "  # tính area overlap\n",
    "  overlap_area = overlap_x*overlap_y\n",
    "  # tính total area hợp của 2 detection\n",
    "  total_area = detection_1[2]*detection_1[3] + detection_2[2]*detection_2[3] - overlap_area\n",
    "    \n",
    "  return np.round(overlap_area/float(total_area),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evalution bằng IOU\n",
    "X_test_image = X_test.reshape((-1,8,8))\n",
    "bbox_y = y_predict.reshape((-1,1,4))\n",
    "bbx_y_true = y_test.reshape((-1,1,4))\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(4):\n",
    "    iou = overlaping_area(bbox_y[i].flatten(),bbx_y_true[i].flatten())\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.imshow(X_test_image[i],cmap=\"Greys\",interpolation='none', origin='lower', extent=[0, img_size, 0, img_size])\n",
    "    for bbox in bbox_y[i]:\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], ec='r', fc='none'))\n",
    "        plt.title(\"IOU = \"+str(iou),color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6870906666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IOU = []\n",
    "for i in range(len(X_test)):\n",
    "    iou = overlaping_area(bbox_y[i].flatten(),bbx_y_true[i].flatten())\n",
    "    IOU.append(iou)\n",
    "np.mean(IOU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta tính được mean cua IOU 0.68 tức 68% tương đối tốt, chúng ta có thể cải thiện model bằng một số cách như : normalizer data trước khi training, thay đổi số node trên mỗi layer hoặc thay đổi active fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
